{"created": 1747280969.480225, "duration": 0.19092798233032227, "exitcode": 1, "root": "/home/justinchiu_cohere_com/librarybench/projects/command_line_task_manager/command_line_task_manager_researcher", "environment": {}, "summary": {"failed": 2, "passed": 2, "total": 4, "collected": 4}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/performance/test_experiment_tracking_performance.py", "type": "Module"}]}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py", "outcome": "passed", "result": [{"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_experiment_creation_performance", "type": "Function", "lineno": 114}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_best_run_query_performance", "type": "Function", "lineno": 138}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_comparison_performance", "type": "Function", "lineno": 174}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_large_experiment_performance", "type": "Function", "lineno": 228}]}], "tests": [{"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_experiment_creation_performance", "lineno": 114, "outcome": "failed", "keywords": ["test_experiment_creation_performance", "test_experiment_tracking_performance.py", "performance", "tests", "command_line_task_manager_researcher", ""], "setup": {"duration": 0.0004858400207012892, "outcome": "passed"}, "call": {"duration": 0.004335236037150025, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/command_line_task_manager/command_line_task_manager_researcher/tests/performance/test_experiment_tracking_performance.py", "lineno": 136, "message": "assert False\n +  where False = all(<generator object test_experiment_creation_performance.<locals>.<genexpr> at 0x7fef27112a80>)"}, "traceback": [{"path": "tests/performance/test_experiment_tracking_performance.py", "lineno": 136, "message": "AssertionError"}], "longrepr": "experiment_service = <researchtrack.experiment_tracking.service.ExperimentService object at 0x7fef27092870>\n\n    def test_experiment_creation_performance(experiment_service):\n        \"\"\"Test the performance of creating an experiment with many runs.\"\"\"\n        start_time = time.time()\n    \n        experiment, runs = create_experiment_with_runs(\n            experiment_service,\n            num_runs=20,\n            params_per_run=10,\n            metrics_per_run=10\n        )\n    \n        end_time = time.time()\n        creation_time = end_time - start_time\n    \n        # Creating 20 runs with 10 parameters and 10 metrics each should be fast\n        assert creation_time < 2.0, f\"Experiment creation took {creation_time:.2f}s, expected < 2.0s\"\n    \n        # Verify experiment structure\n        experiment = experiment_service.get_experiment(experiment.id)\n        assert len(experiment.runs) == 20\n        assert all(len(run.parameters) == 10 for run in experiment.runs)\n>       assert all(len(run.metrics) == 10 for run in experiment.runs)\nE       assert False\nE        +  where False = all(<generator object test_experiment_creation_performance.<locals>.<genexpr> at 0x7fef27112a80>)\n\ntests/performance/test_experiment_tracking_performance.py:136: AssertionError"}, "teardown": {"duration": 0.0001472020521759987, "outcome": "passed"}}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_best_run_query_performance", "lineno": 138, "outcome": "passed", "keywords": ["test_best_run_query_performance", "test_experiment_tracking_performance.py", "performance", "tests", "command_line_task_manager_researcher", ""], "setup": {"duration": 0.00014626490883529186, "outcome": "passed"}, "call": {"duration": 0.012297931010834873, "outcome": "passed", "stdout": "Best run query time for 50 runs: 0.000013s\n"}, "teardown": {"duration": 0.00012160802725702524, "outcome": "passed"}}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_comparison_performance", "lineno": 174, "outcome": "failed", "keywords": ["test_comparison_performance", "test_experiment_tracking_performance.py", "performance", "tests", "command_line_task_manager_researcher", ""], "setup": {"duration": 0.00013501697685569525, "outcome": "passed"}, "call": {"duration": 0.002770974999293685, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/command_line_task_manager/command_line_task_manager_researcher/tests/performance/test_experiment_tracking_performance.py", "lineno": 224, "message": "AssertionError: assert 10 == 2\n +  where 10 = len({'Performance Test Experiment with 10 runs:Run 1': {'accuracy': 0.85, 'f1': 0.75, 'loss': 0.7999999999999999}, 'Performance Test Experiment with 10 runs:Run 10': {'accuracy': 0.94, 'f1': 0.84, 'loss': 0.8899999999999999}, 'Performance Test Experiment with 10 runs:Run 2': {'accuracy': 0.86, 'f1': 0.76, 'loss': 0.8099999999999999}, 'Performance Test Experiment with 10 runs:Run 3': {'accuracy': 0.87, 'f1': 0.77, 'loss': 0.82}, ...})"}, "traceback": [{"path": "tests/performance/test_experiment_tracking_performance.py", "lineno": 224, "message": "AssertionError"}], "longrepr": "experiment_service = <researchtrack.experiment_tracking.service.ExperimentService object at 0x7fef27107fb0>\n\n    def test_comparison_performance(experiment_service):\n        \"\"\"Test the performance of creating and querying experiment comparisons.\"\"\"\n        # Create multiple experiments with runs\n        experiment1, runs1 = create_experiment_with_runs(\n            experiment_service, num_runs=10, params_per_run=5, metrics_per_run=5\n        )\n    \n        experiment2, runs2 = create_experiment_with_runs(\n            experiment_service, num_runs=10, params_per_run=5, metrics_per_run=5\n        )\n    \n        # Add common metrics to all runs for comparison\n        common_metrics = [\"accuracy\", \"loss\", \"f1\"]\n        for i, (run_list, base_val) in enumerate([(runs1, 0.8), (runs2, 0.85)]):\n            for j, run in enumerate(run_list):\n                for k, metric in enumerate(common_metrics):\n                    experiment_service.add_run_metric(\n                        run_id=run.id,\n                        name=metric,\n                        type=MetricType.CUSTOM,\n                        value=base_val + (j * 0.01) - (k * 0.05)\n                    )\n    \n        # Measure comparison creation time\n        start_time = time.time()\n    \n        comparison = experiment_service.create_comparison(\n            name=\"Performance Test Comparison\",\n            description=\"Comparison created for performance testing\",\n            experiment_ids=[experiment1.id, experiment2.id],\n            metrics=common_metrics\n        )\n    \n        creation_time = time.time() - start_time\n    \n        # Comparison creation should be fast\n        assert creation_time < 0.1, f\"Comparison creation took {creation_time:.2f}s, expected < 0.1s\"\n    \n        # Measure comparison data retrieval time\n        start_time = time.time()\n    \n        comparison_data = experiment_service.get_comparison_data(comparison.id)\n    \n        retrieval_time = time.time() - start_time\n    \n        # Comparison data retrieval should be efficient\n        assert retrieval_time < 0.2, f\"Comparison data retrieval took {retrieval_time:.2f}s, expected < 0.2s\"\n    \n        # Verify comparison data\n>       assert len(comparison_data) == 2  # Two experiment \"best\" entries\nE       AssertionError: assert 10 == 2\nE        +  where 10 = len({'Performance Test Experiment with 10 runs:Run 1': {'accuracy': 0.85, 'f1': 0.75, 'loss': 0.7999999999999999}, 'Performance Test Experiment with 10 runs:Run 10': {'accuracy': 0.94, 'f1': 0.84, 'loss': 0.8899999999999999}, 'Performance Test Experiment with 10 runs:Run 2': {'accuracy': 0.86, 'f1': 0.76, 'loss': 0.8099999999999999}, 'Performance Test Experiment with 10 runs:Run 3': {'accuracy': 0.87, 'f1': 0.77, 'loss': 0.82}, ...})\n\ntests/performance/test_experiment_tracking_performance.py:224: AssertionError"}, "teardown": {"duration": 0.00013034907169640064, "outcome": "passed"}}, {"nodeid": "tests/performance/test_experiment_tracking_performance.py::test_large_experiment_performance", "lineno": 228, "outcome": "passed", "keywords": ["test_large_experiment_performance", "test_experiment_tracking_performance.py", "performance", "tests", "command_line_task_manager_researcher", ""], "setup": {"duration": 0.00012416194658726454, "outcome": "passed"}, "call": {"duration": 0.051334065035916865, "outcome": "passed", "stdout": "Large experiment creation time: 0.05s\nLarge experiment retrieval time: 0.000001s\nLarge experiment update time: 0.000031s\nTotal runs: 100\nTotal parameters: 2000\nTotal metrics: 2000\n"}, "teardown": {"duration": 0.00011553091462701559, "outcome": "passed"}}]}