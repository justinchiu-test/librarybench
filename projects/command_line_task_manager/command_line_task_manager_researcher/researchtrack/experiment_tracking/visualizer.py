from typing import Dict, List, Optional, Set, Union, Any
from uuid import UUID
import json

from .models import Experiment, ExperimentRun, Parameter, Metric


class ExperimentVisualizer:
    """Utility for creating visualizations of experiment results."""

    def format_parameter_table(self, run: ExperimentRun) -> str:
        """
        Format the parameters of an experiment run as a markdown table.

        Args:
            run: The experiment run

        Returns:
            str: Markdown table of parameters
        """
        if not run.parameters:
            return "No parameters defined."

        # Build table
        table = [
            "| Parameter | Type | Value | Description |",
            "| --- | --- | --- | --- |",
        ]

        for param in run.parameters:
            description = param.description or ""
            table.append(f"| {param.name} | {param.type.value} | {param.value} | {description} |")

        return "\n".join(table)

    def format_metric_table(self, run: ExperimentRun) -> str:
        """
        Format the metrics of an experiment run as a markdown table.

        Args:
            run: The experiment run

        Returns:
            str: Markdown table of metrics
        """
        if not run.metrics:
            return "No metrics recorded."

        # Build table
        table = [
            "| Metric | Type | Value | Timestamp | Description |",
            "| --- | --- | --- | --- | --- |",
        ]

        for name, metric in run.metrics.items():
            description = metric.description or ""
            timestamp = metric.timestamp.strftime("%Y-%m-%d %H:%M:%S")
            table.append(f"| {name} | {metric.type.value} | {metric.value} | {timestamp} | {description} |")

        return "\n".join(table)

    def format_artifact_table(self, run: ExperimentRun) -> str:
        """
        Format the artifacts of an experiment run as a markdown table.

        Args:
            run: The experiment run

        Returns:
            str: Markdown table of artifacts
        """
        if not run.artifacts:
            return "No artifacts recorded."

        # Build table
        table = [
            "| Artifact | Path |",
            "| --- | --- |",
        ]

        for name, path in run.artifacts.items():
            table.append(f"| {name} | {path} |")

        return "\n".join(table)

    def format_run_summary(self, run: ExperimentRun, experiment_name: str) -> str:
        """
        Format a complete summary of an experiment run.

        Args:
            run: The experiment run
            experiment_name: The name of the experiment

        Returns:
            str: Markdown summary of the run
        """
        summary = [
            f"# Run {run.run_number} of Experiment: {experiment_name}",
            "",
            f"**Status**: {run.status.value}",
        ]

        if run.start_time:
            summary.append(f"**Started**: {run.start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if run.end_time:
            summary.append(f"**Ended**: {run.end_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if run.duration() is not None:
            summary.append(f"**Duration**: {run.duration():.2f} seconds")

        summary.append("")

        # Notes
        if run.notes:
            summary.append("## Notes")
            summary.append("")
            summary.append(run.notes)
            summary.append("")

        # Parameters
        summary.append("## Parameters")
        summary.append("")
        summary.append(self.format_parameter_table(run))
        summary.append("")

        # Metrics
        summary.append("## Metrics")
        summary.append("")
        summary.append(self.format_metric_table(run))
        summary.append("")

        # Artifacts
        summary.append("## Artifacts")
        summary.append("")
        summary.append(self.format_artifact_table(run))

        return "\n".join(summary)

    def format_experiment_summary(self, experiment: Experiment) -> str:
        """
        Format a complete summary of an experiment.

        Args:
            experiment: The experiment

        Returns:
            str: Markdown summary of the experiment
        """
        summary = [
            f"# Experiment: {experiment.name}",
            "",
        ]

        if experiment.description:
            summary.append(experiment.description)
            summary.append("")

        # Metadata
        summary.append(f"**Created**: {experiment.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
        summary.append(f"**Last Updated**: {experiment.updated_at.strftime('%Y-%m-%d %H:%M:%S')}")

        if experiment.task_id:
            summary.append(f"**Task ID**: {experiment.task_id}")

        if experiment.dataset_id:
            summary.append(f"**Dataset ID**: {experiment.dataset_id}")

        if experiment.environment_id:
            summary.append(f"**Environment ID**: {experiment.environment_id}")

        if experiment.tags:
            summary.append(f"**Tags**: {', '.join(experiment.tags)}")

        summary.append("")

        # Runs summary
        summary.append(f"**Runs ({len(experiment.runs)})**")
        summary.append("")

        if not experiment.runs:
            summary.append("No runs recorded for this experiment.")
            return "\n".join(summary)

        # Runs table
        summary.extend([
            "| Run # | Status | Start Time | Duration | Key Metrics |",
            "| --- | --- | --- | --- | --- |",
        ])

        for run in experiment.runs:
            status = run.status.value
            start_time = run.start_time.strftime('%Y-%m-%d %H:%M:%S') if run.start_time else "-"
            duration = f"{run.duration():.2f}s" if run.duration() is not None else "-"

            # Format key metrics
            metrics_str = ""
            if run.metrics and run.status.value == "completed":
                metrics = []
                for name, metric in run.metrics.items():
                    metrics.append(f"{name}: {metric.value:.4f}")
                metrics_str = ", ".join(metrics)

            summary.append(f"| {run.run_number} | {status} | {start_time} | {duration} | {metrics_str} |")

        return "\n".join(summary)

    def format_comparison_table(self, comparison_data: Dict[str, Dict[str, float]]) -> str:
        """
        Format a comparison table of multiple runs/experiments.

        Args:
            comparison_data: Dictionary mapping run/experiment names to metric values

        Returns:
            str: Markdown comparison table
        """
        if not comparison_data:
            return "No comparison data available."

        # Collect all metric names
        all_metrics = set()
        for run_data in comparison_data.values():
            all_metrics.update(run_data.keys())

        metrics = sorted(all_metrics)

        # Build table header
        header = ["Run/Experiment"]
        header.extend(metrics)

        table = [
            "| " + " | ".join(header) + " |",
            "| " + " | ".join(["---"] * len(header)) + " |",
        ]

        # Add rows
        for run_name, run_data in comparison_data.items():
            row = [run_name]

            for metric in metrics:
                if metric in run_data:
                    row.append(f"{run_data[metric]:.2f}")
                else:
                    row.append("-")

            table.append("| " + " | ".join(row) + " |")

        return "\n".join(table)

    @staticmethod
    def generate_run_summary_table(
        runs: List[ExperimentRun], metric_names: Optional[List[str]] = None
    ) -> str:
        """
        Generate a markdown table summarizing experiment runs.

        Args:
            runs: List of experiment runs
            metric_names: List of metrics to include in the summary

        Returns:
            str: Markdown table
        """
        if not runs:
            return "No runs to display."

        # Collect all metrics if not specified
        if not metric_names:
            all_metrics = set()
            for run in runs:
                all_metrics.update(run.metrics.keys())
            metric_names = sorted(list(all_metrics))

        # Collect all parameter names
        all_params = set()
        for run in runs:
            all_params.update(param.name for param in run.parameters)
        param_names = sorted(list(all_params))

        # Build header
        header = ["Run #"]
        header.extend(param_names)
        header.extend(metric_names)

        # Build separator
        separator = ["---"]
        separator.extend(["---" for _ in range(len(param_names) + len(metric_names))])

        # Build rows
        rows = []
        for run in runs:
            row = [str(run.run_number)]

            # Add parameters
            param_dict = {param.name: param.value for param in run.parameters}
            for param_name in param_names:
                row.append(str(param_dict.get(param_name, "")))

            # Add metrics
            for metric_name in metric_names:
                metric = run.metrics.get(metric_name)
                if metric:
                    row.append(f"{metric.value:.4f}")
                else:
                    row.append("")

            rows.append(row)

        # Build table
        table = []
        table.append("| " + " | ".join(header) + " |")
        table.append("| " + " | ".join(separator) + " |")

        for row in rows:
            table.append("| " + " | ".join(row) + " |")

        return "\n".join(table)

    @staticmethod
    def generate_parameter_importance(
        runs: List[ExperimentRun], metric_name: str, higher_is_better: bool = True
    ) -> str:
        """
        Generate a text report on parameter importance based on experiment runs.

        Args:
            runs: List of experiment runs
            metric_name: The name of the metric to analyze
            higher_is_better: Whether higher values are better

        Returns:
            str: Markdown report on parameter importance
        """
        if not runs:
            return "No runs to analyze."

        # Filter runs with the specified metric
        runs_with_metric = [run for run in runs if metric_name in run.metrics]
        if not runs_with_metric:
            return f"No runs with metric '{metric_name}' found."

        # Sort runs by metric value
        sorted_runs = sorted(
            runs_with_metric,
            key=lambda r: r.metrics[metric_name].value,
            reverse=higher_is_better,
        )

        best_run = sorted_runs[0]
        worst_run = sorted_runs[-1]

        # Collect parameter names
        all_params = set()
        for run in runs_with_metric:
            all_params.update(param.name for param in run.parameters)
        param_names = list(all_params)

        # Analyze parameters
        result = [
            f"## Parameter Importance Analysis for Metric: {metric_name}",
            "",
            f"**Best value:** {best_run.metrics[metric_name].value:.4f} (Run #{best_run.run_number})",
            f"**Worst value:** {worst_run.metrics[metric_name].value:.4f} (Run #{worst_run.run_number})",
            "",
            "### Parameter Values for Best Run",
            "",
        ]

        # Best run parameters
        best_params = {param.name: param.value for param in best_run.parameters}
        for name in param_names:
            if name in best_params:
                result.append(f"- **{name}:** {best_params[name]}")

        result.append("")
        result.append("### Parameter Comparison")
        result.append("")

        # Analyze parameter correlation with performance
        for param_name in param_names:
            # Group runs by parameter value
            param_values = {}
            for run in runs_with_metric:
                param_value = next(
                    (p.value for p in run.parameters if p.name == param_name), None
                )
                if param_value is not None:
                    if param_value not in param_values:
                        param_values[param_value] = []
                    param_values[param_value].append(
                        run.metrics[metric_name].value
                    )

            # Calculate average metric value for each parameter value
            param_metrics = {}
            for value, metrics in param_values.items():
                param_metrics[value] = sum(metrics) / len(metrics)

            # Sort parameter values by metric
            sorted_values = sorted(
                param_metrics.items(),
                key=lambda item: item[1],
                reverse=higher_is_better,
            )

            # Add analysis
            result.append(f"#### Parameter: {param_name}")
            result.append("")
            result.append("| Value | Average Metric |")
            result.append("| --- | --- |")

            for value, metric in sorted_values:
                result.append(f"| {value} | {metric:.4f} |")

            result.append("")

        return "\n".join(result)

    @staticmethod
    def generate_experiment_summary(experiment: Experiment) -> str:
        """
        Generate a markdown summary of an experiment.

        Args:
            experiment: The experiment to summarize

        Returns:
            str: Markdown summary
        """
        if not experiment:
            return "No experiment to summarize."

        result = [
            f"# Experiment: {experiment.name}",
            "",
        ]

        if experiment.description:
            result.extend([experiment.description, ""])

        # Add metadata
        result.extend([
            f"- **Created:** {experiment.created_at.strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **Updated:** {experiment.updated_at.strftime('%Y-%m-%d %H:%M:%S')}",
        ])

        if experiment.task_id:
            result.append(f"- **Associated Task:** {experiment.task_id}")

        if experiment.dataset_id:
            result.append(f"- **Associated Dataset:** {experiment.dataset_id}")

        if experiment.environment_id:
            result.append(f"- **Associated Environment:** {experiment.environment_id}")

        if experiment.tags:
            result.append(f"- **Tags:** {', '.join(experiment.tags)}")

        result.append("")

        # Add runs summary
        runs = experiment.runs
        if not runs:
            result.append("No runs recorded for this experiment.")
            return "\n".join(result)

        result.append(f"## Runs Summary ({len(runs)} runs)")
        result.append("")

        # Count runs by status
        status_counts = {}
        for run in runs:
            status = run.status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        for status, count in status_counts.items():
            result.append(f"- **{status}:** {count}")

        result.append("")

        # Create runs table
        completed_runs = [run for run in runs if run.status.value == "completed"]
        if completed_runs:
            result.append("## Completed Runs")
            result.append("")
            result.append(ExperimentVisualizer.generate_run_summary_table(completed_runs))
            result.append("")

            # Find all metrics
            all_metrics = set()
            for run in completed_runs:
                all_metrics.update(run.metrics.keys())

            # Generate parameter importance for each metric
            if all_metrics:
                result.append("## Metrics Analysis")
                result.append("")

                for metric in sorted(all_metrics):
                    result.append(
                        ExperimentVisualizer.generate_parameter_importance(
                            completed_runs, metric
                        )
                    )
                    result.append("")

        return "\n".join(result)

    @staticmethod
    def generate_comparison_report(
        comparison_data: Dict[str, Any]
    ) -> str:
        """
        Generate a markdown report for a comparison of experiments or runs.

        Args:
            comparison_data: Comparison data as returned by ExperimentService.get_comparison_data

        Returns:
            str: Markdown report
        """
        if not comparison_data:
            return "No comparison data to report."

        comparison_info = comparison_data.get("comparison", {})
        runs = comparison_data.get("runs", [])
        metrics = comparison_data.get("metrics", [])

        result = [
            f"# Experiment Comparison: {comparison_info.get('name', 'Untitled')}",
            "",
        ]

        if comparison_info.get("description"):
            result.extend([comparison_info["description"], ""])

        if not runs:
            result.append("No runs to compare.")
            return "\n".join(result)

        # Create runs comparison table
        result.append("## Runs Comparison")
        result.append("")

        # Filter runs to only include the metrics we care about
        filtered_runs = []
        for run in runs:
            filtered_run = ExperimentRun(
                id=UUID(run["id"]),
                experiment_id=UUID(run["experiment_id"]),
                run_number=run["run_number"],
                parameters=[
                    Parameter(
                        name=param["name"],
                        type=param["type"],
                        value=param["value"],
                        description=param.get("description"),
                    )
                    for param in run["parameters"]
                ],
            )

            # Add metrics
            for name, metric_data in run["metrics"].items():
                if not metrics or name in metrics:
                    filtered_run.metrics[name] = Metric(
                        name=name,
                        type=metric_data["type"],
                        value=metric_data["value"],
                        description=metric_data.get("description"),
                    )

            filtered_runs.append(filtered_run)

        result.append(ExperimentVisualizer.generate_run_summary_table(filtered_runs, metrics))
        result.append("")

        # Generate parameter importance for each metric
        if metrics:
            result.append("## Metrics Analysis")
            result.append("")

            for metric in metrics:
                result.append(
                    ExperimentVisualizer.generate_parameter_importance(
                        filtered_runs, metric
                    )
                )
                result.append("")

        return "\n".join(result)

    @staticmethod
    def export_runs_to_json(
        runs: List[ExperimentRun], file_path: str, include_metrics: bool = True
    ) -> None:
        """
        Export experiment runs to a JSON file.

        Args:
            runs: List of experiment runs
            file_path: Path to the output JSON file
            include_metrics: Whether to include metrics in the export
        """
        data = []

        for run in runs:
            run_data = {
                "id": str(run.id),
                "experiment_id": str(run.experiment_id),
                "run_number": run.run_number,
                "status": run.status.value,
                "parameters": [
                    {
                        "name": param.name,
                        "type": param.type.value,
                        "value": param.value,
                        "description": param.description,
                    }
                    for param in run.parameters
                ],
            }

            if run.start_time:
                run_data["start_time"] = run.start_time.isoformat()

            if run.end_time:
                run_data["end_time"] = run.end_time.isoformat()

            if include_metrics:
                run_data["metrics"] = {
                    name: {
                        "value": metric.value,
                        "type": metric.type.value,
                        "timestamp": metric.timestamp.isoformat(),
                        "description": metric.description,
                    }
                    for name, metric in run.metrics.items()
                }

            if run.artifacts:
                run_data["artifacts"] = run.artifacts

            if run.notes:
                run_data["notes"] = run.notes

            data.append(run_data)

        with open(file_path, "w") as f:
            json.dump(data, f, indent=2)