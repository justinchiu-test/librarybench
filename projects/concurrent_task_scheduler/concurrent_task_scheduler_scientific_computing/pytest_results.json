{"created": 1747285155.532241, "duration": 17.563925743103027, "exitcode": 1, "root": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing", "environment": {}, "summary": {"passed": 190, "xfailed": 4, "xpassed": 1, "failed": 36, "error": 8, "total": 239, "collected": 239}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests", "type": "Package"}]}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_stage", "type": "Function", "lineno": 54}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency", "type": "Function", "lineno": 59}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency_with_cycle", "type": "Function", "lineno": 73}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_remove_dependency", "type": "Function", "lineno": 79}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_dependencies", "type": "Function", "lineno": 85}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_root_and_leaf_stages", "type": "Function", "lineno": 94}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_ready_stages", "type": "Function", "lineno": 103}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_are_all_dependencies_satisfied", "type": "Function", "lineno": 124}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_update_stage_status", "type": "Function", "lineno": 136}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_critical_path", "type": "Function", "lineno": 146}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_is_stage_blocked", "type": "Function", "lineno": 157}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_validate", "type": "Function", "lineno": 167}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_serialize_deserialize", "type": "Function", "lineno": 184}]}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_create_dependency_graph", "type": "Function", "lineno": 267}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dependency", "type": "Function", "lineno": 285}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_remove_dependency", "type": "Function", "lineno": 308}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_transition_rule", "type": "Function", "lineno": 326}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_update_stage_status", "type": "Function", "lineno": 350}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_ready_stages", "type": "Function", "lineno": 374}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_is_stage_ready", "type": "Function", "lineno": 392}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_blocking_stages", "type": "Function", "lineno": 409}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_critical_path", "type": "Function", "lineno": 425}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_bypass_dependency", "type": "Function", "lineno": 439}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dynamic_dependency", "type": "Function", "lineno": 464}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_execution_plan", "type": "Function", "lineno": 490}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_validate_simulation", "type": "Function", "lineno": 506}]}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_register_template", "type": "Function", "lineno": 540}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_instance", "type": "Function", "lineno": 555}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_linear_workflow", "type": "Function", "lineno": 595}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_parallel_workflow", "type": "Function", "lineno": 636}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_next_stages", "type": "Function", "lineno": 673}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_update_instance", "type": "Function", "lineno": 726}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_workflow_status", "type": "Function", "lineno": 781}]}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph", "type": "Class"}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker", "type": "Class"}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager", "type": "Class"}]}, {"nodeid": "tests/dependency_tracking", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py", "type": "Module"}]}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_checkpoint_manager_init", "type": "Function", "lineno": 105}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_register_policy", "type": "Function", "lineno": 115}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_default_policy", "type": "Function", "lineno": 133}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_create_manager_for_simulation", "type": "Function", "lineno": 148}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_update_policy", "type": "Function", "lineno": 182}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_should_create_checkpoint", "type": "Function", "lineno": 217}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_create_checkpoint", "type": "Function", "lineno": 244}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_validate_checkpoint", "type": "Function", "lineno": 284}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_latest_checkpoint", "type": "Function", "lineno": 330}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_all_checkpoints", "type": "Function", "lineno": 360}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_restore_from_checkpoint", "type": "Function", "lineno": 383}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_checkpoint_coordinator", "type": "Function", "lineno": 418}]}, {"nodeid": "tests/failure_resilience/test_failure_detector.py", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_failure_detector.py::test_failure_detector_init", "type": "Function", "lineno": 110}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_record_heartbeat", "type": "Function", "lineno": 121}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_check_node_health", "type": "Function", "lineno": 130}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_check_simulation_health", "type": "Function", "lineno": 163}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_update_simulation_progress", "type": "Function", "lineno": 185}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_update_simulation_status", "type": "Function", "lineno": 197}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_detect_node_failures", "type": "Function", "lineno": 209}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_detect_simulation_failures", "type": "Function", "lineno": 224}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_report_failure", "type": "Function", "lineno": 246}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_active_failures", "type": "Function", "lineno": 283}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_resolve_failure", "type": "Function", "lineno": 329}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_reliability_metrics", "type": "Function", "lineno": 358}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_failure_type_determination", "type": "Function", "lineno": 386}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_recovery_manager_init", "type": "Function", "lineno": 399}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_recovery_strategy", "type": "Function", "lineno": 408}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_initiate_recovery", "type": "Function", "lineno": 445}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_complete_recovery", "type": "Function", "lineno": 491}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_active_recoveries", "type": "Function", "lineno": 528}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_cancel_recovery", "type": "Function", "lineno": 567}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_process_failures", "type": "Function", "lineno": 602}]}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_record_heartbeat", "type": "Function", "lineno": 108}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_node_health", "type": "Function", "lineno": 115}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_simulation_health", "type": "Function", "lineno": 180}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_update_simulation_progress", "type": "Function", "lineno": 209}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_node_failures", "type": "Function", "lineno": 221}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_simulation_failures", "type": "Function", "lineno": 255}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_report_failure", "type": "Function", "lineno": 299}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_active_failures", "type": "Function", "lineno": 319}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_resolve_failure", "type": "Function", "lineno": 361}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_node_reliability_score", "type": "Function", "lineno": 387}]}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_get_recovery_strategy", "type": "Function", "lineno": 421}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_initiate_recovery", "type": "Function", "lineno": 451}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_complete_recovery", "type": "Function", "lineno": 483}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_cancel_recovery", "type": "Function", "lineno": 528}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_process_failures", "type": "Function", "lineno": 558}]}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_set_resilience_level", "type": "Function", "lineno": 681}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_record_event", "type": "Function", "lineno": 699}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_schedule_checkpoint", "type": "Function", "lineno": 721}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_node_status_change", "type": "Function", "lineno": 741}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_simulation_status_change", "type": "Function", "lineno": 772}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_stage_status_change", "type": "Function", "lineno": 811}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_failure_detection", "type": "Function", "lineno": 843}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_complete_recovery", "type": "Function", "lineno": 873}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_detect_and_handle_failures", "type": "Function", "lineno": 910}]}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector", "type": "Class"}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager", "type": "Class"}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator", "type": "Class"}]}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_resilience_coordinator_init", "type": "Function", "lineno": 152}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_set_resilience_level", "type": "Function", "lineno": 163}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_record_event", "type": "Function", "lineno": 181}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_schedule_checkpoint", "type": "Function", "lineno": 203}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_process_scheduled_checkpoints", "type": "Function", "lineno": 219}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_node_status_change", "type": "Function", "lineno": 238}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_simulation_status_change", "type": "Function", "lineno": 290}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_stage_status_change", "type": "Function", "lineno": 344}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_failure_detection", "type": "Function", "lineno": 397}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_complete_recovery", "type": "Function", "lineno": 429}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_active_recoveries", "type": "Function", "lineno": 486}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_checkpoint_schedule", "type": "Function", "lineno": 521}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_resilience_metrics", "type": "Function", "lineno": 536}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_detect_and_handle_failures", "type": "Function", "lineno": 554}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_process_checkpoints", "type": "Function", "lineno": 577}]}, {"nodeid": "tests/failure_resilience", "outcome": "passed", "result": [{"nodeid": "tests/failure_resilience/test_checkpoint_manager.py", "type": "Module"}, {"nodeid": "tests/failure_resilience/test_failure_detector.py", "type": "Module"}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py", "type": "Module"}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py", "type": "Module"}]}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager", "outcome": "passed", "result": [{"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_simulation", "type": "Function", "lineno": 174}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_with_max_concurrent_limit", "type": "Function", "lineno": 190}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_processing_queue", "type": "Function", "lineno": 214}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_pause_and_resume_simulation", "type": "Function", "lineno": 234}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_simulation_stage_transitions", "type": "Function", "lineno": 255}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_handle_node_failure", "type": "Function", "lineno": 284}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_checkpoint_creation", "type": "Function", "lineno": 300}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_maintenance_handling", "type": "Function", "lineno": 316}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_policies", "type": "Function", "lineno": 354}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_long_term_reservation", "type": "Function", "lineno": 410}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_node_allocation_optimization", "type": "Function", "lineno": 430}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_dynamic_priority_adjustment", "type": "Function", "lineno": 499}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_protection", "type": "Function", "lineno": 530}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_recovery_from_failure", "type": "Function", "lineno": 547}]}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler", "outcome": "passed", "result": [{"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_reserve_resources", "type": "Function", "lineno": 581}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_activate_reservation", "type": "Function", "lineno": 636}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_active_reservations", "type": "Function", "lineno": 668}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_maintenance_windows", "type": "Function", "lineno": 712}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_preemption_history", "type": "Function", "lineno": 767}]}, {"nodeid": "tests/job_management/test_long_running_job_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager", "type": "Class"}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler", "type": "Class"}]}, {"nodeid": "tests/job_management", "outcome": "passed", "result": [{"nodeid": "tests/job_management/test_long_running_job_manager.py", "type": "Module"}]}, {"nodeid": "tests/resource_forecasting/test_data_collector.py", "outcome": "passed", "result": [{"nodeid": "tests/resource_forecasting/test_data_collector.py::test_resource_data_collector_init", "type": "Function", "lineno": 96}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_record_data_point", "type": "Function", "lineno": 110}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_simulation_data", "type": "Function", "lineno": 148}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_node_data", "type": "Function", "lineno": 175}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_get_resource_history", "type": "Function", "lineno": 212}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_aggregate_data_points", "type": "Function", "lineno": 277}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_batch_data", "type": "Function", "lineno": 335}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_resource_usage_analyzer", "type": "Function", "lineno": 371}]}, {"nodeid": "tests/resource_forecasting/test_forecaster.py", "outcome": "passed", "result": [{"nodeid": "tests/resource_forecasting/test_forecaster.py::test_forecaster_init", "type": "Function", "lineno": 156}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_extract_time_features", "type": "Function", "lineno": 165}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_train_model", "type": "Function", "lineno": 195}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_forecast_resource_usage", "type": "Function", "lineno": 250}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_update_forecast_with_actuals", "type": "Function", "lineno": 325}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_create_resource_projection", "type": "Function", "lineno": 365}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_detect_anomalies", "type": "Function", "lineno": 413}]}, {"nodeid": "tests/resource_forecasting/test_optimizer.py", "outcome": "passed", "result": [{"nodeid": "tests/resource_forecasting/test_optimizer.py::test_optimizer_init", "type": "Function", "lineno": 151}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_optimize_simulation_resources", "type": "Function", "lineno": 168}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_generate_capacity_plan", "type": "Function", "lineno": 232}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_resource_allocation_recommendation", "type": "Function", "lineno": 267}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_capacity_planning_recommendation", "type": "Function", "lineno": 318}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_resource_cost", "type": "Function", "lineno": 368}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_mock_allocation_and_capacity", "type": "Function", "lineno": 375}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_justification_generation", "type": "Function", "lineno": 402}]}, {"nodeid": "tests/resource_forecasting/test_reporter.py", "outcome": "passed", "result": [{"nodeid": "tests/resource_forecasting/test_reporter.py::test_reporter_init", "type": "Function", "lineno": 145}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_utilization_report", "type": "Function", "lineno": 154}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_forecast_report", "type": "Function", "lineno": 252}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_grant_report", "type": "Function", "lineno": 332}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_recommendation_report", "type": "Function", "lineno": 422}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_get_report", "type": "Function", "lineno": 483}]}, {"nodeid": "tests/resource_forecasting", "outcome": "passed", "result": [{"nodeid": "tests/resource_forecasting/test_data_collector.py", "type": "Module"}, {"nodeid": "tests/resource_forecasting/test_forecaster.py", "type": "Module"}, {"nodeid": "tests/resource_forecasting/test_optimizer.py", "type": "Module"}, {"nodeid": "tests/resource_forecasting/test_reporter.py", "type": "Module"}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_empty_resource_allocation", "type": "Function", "lineno": 302}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_very_high_priority_scenario", "type": "Function", "lineno": 371}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_unusual_resource_types", "type": "Function", "lineno": 470}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_paused_scenario_handling", "type": "Function", "lineno": 545}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "type": "Function", "lineno": 624}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "type": "Function", "lineno": 624}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_sudden_priority_change", "type": "Function", "lineno": 722}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_queue_for_recalculation", "type": "Function", "lineno": 899}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_recompute_all_priorities", "type": "Function", "lineno": 971}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis::test_priority_trend_tracking", "type": "Function", "lineno": 1117}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory::test_resource_allocation_tracking", "type": "Function", "lineno": 1203}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_invalid_priority_override", "type": "Function", "lineno": 1333}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_empty_scenario_list", "type": "Function", "lineno": 1388}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_single_scenario", "type": "Function", "lineno": 1399}]}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement", "type": "Class"}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies", "type": "Class"}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents", "type": "Class"}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis", "type": "Class"}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory", "type": "Class"}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling", "type": "Class"}]}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_initialization", "type": "Function", "lineno": 211}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_direct", "type": "Function", "lineno": 226}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_weighted", "type": "Function", "lineno": 259}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_relative", "type": "Function", "lineno": 281}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_rank_based", "type": "Function", "lineno": 315}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_get_comparison_history", "type": "Function", "lineno": 338}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_multiple_scenarios", "type": "Function", "lineno": 374}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_group_scenarios_by_similarity", "type": "Function", "lineno": 392}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_identify_complementary_scenarios", "type": "Function", "lineno": 431}]}, {"nodeid": "tests/scenario_management/test_comparator.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator", "type": "Class"}]}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_initialization", "type": "Function", "lineno": 84}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_scenario", "type": "Function", "lineno": 101}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_with_custom_weights", "type": "Function", "lineno": 125}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_empty_scenario", "type": "Function", "lineno": 157}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_manual_rating_integration", "type": "Function", "lineno": 179}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_history_tracking", "type": "Function", "lineno": 206}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_compare_evaluations", "type": "Function", "lineno": 226}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_criteria_functions", "type": "Function", "lineno": 255}]}, {"nodeid": "tests/scenario_management/test_evaluator.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator", "type": "Class"}]}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_initialization", "type": "Function", "lineno": 230}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "type": "Function", "lineno": 255}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_evaluate_scenario_priority", "type": "Function", "lineno": 283}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "type": "Function", "lineno": 304}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "type": "Function", "lineno": 351}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "type": "Function", "lineno": 387}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "type": "Function", "lineno": 448}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "type": "Function", "lineno": 485}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "type": "Function", "lineno": 508}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "type": "Function", "lineno": 549}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "type": "Function", "lineno": 585}]}, {"nodeid": "tests/scenario_management/test_priority_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager", "type": "Class"}]}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_workflow", "type": "Function", "lineno": 226}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_scenario_metric_updates_propagation", "type": "Function", "lineno": 327}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_manager_adaptation", "type": "Function", "lineno": 371}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "type": "Function", "lineno": 429}]}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration", "type": "Class"}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric::test_initialization", "type": "Function", "lineno": 24}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric::test_normalized_score", "type": "Function", "lineno": 58}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective::test_initialization", "type": "Function", "lineno": 138}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective::test_is_relevant_to_scenario", "type": "Function", "lineno": 156}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_initialization", "type": "Function", "lineno": 189}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_calculate_priority_score", "type": "Function", "lineno": 210}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_update_priority", "type": "Function", "lineno": 258}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_get_simulation_status_counts", "type": "Function", "lineno": 288}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_get_derived_priority", "type": "Function", "lineno": 329}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_total_progress", "type": "Function", "lineno": 353}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult::test_initialization", "type": "Function", "lineno": 384}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult::test_clear_winner", "type": "Function", "lineno": 403}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_initialization", "type": "Function", "lineno": 457}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_should_adjust_priority", "type": "Function", "lineno": 478}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_initialization", "type": "Function", "lineno": 503}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_get_absolute_allocation", "type": "Function", "lineno": 521}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_is_valid", "type": "Function", "lineno": 541}]}, {"nodeid": "tests/scenario_management/test_scenario_model.py", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric", "type": "Class"}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective", "type": "Class"}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario", "type": "Class"}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult", "type": "Class"}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult", "type": "Class"}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation", "type": "Class"}]}, {"nodeid": "tests/scenario_management", "outcome": "passed", "result": [{"nodeid": "tests/scenario_management/test_advanced_priority_management.py", "type": "Module"}, {"nodeid": "tests/scenario_management/test_comparator.py", "type": "Module"}, {"nodeid": "tests/scenario_management/test_evaluator.py", "type": "Module"}, {"nodeid": "tests/scenario_management/test_priority_manager.py", "type": "Module"}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py", "type": "Module"}, {"nodeid": "tests/scenario_management/test_scenario_model.py", "type": "Module"}]}, {"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow", "outcome": "passed", "result": [{"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow::test_scenario_lifecycle", "type": "Function", "lineno": 277}, {"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow::test_multi_scenario_prioritization", "type": "Function", "lineno": 391}]}, {"nodeid": "tests/test_full_system_integration.py::TestFailureResilienceWithForecasting", "outcome": "passed", "result": [{"nodeid": "tests/test_full_system_integration.py::TestFailureResilienceWithForecasting::test_forecasting_affects_checkpoint_frequency", "type": "Function", "lineno": 562}]}, {"nodeid": "tests/test_full_system_integration.py::TestResourceOptimizationWithPriorities", "outcome": "passed", "result": [{"nodeid": "tests/test_full_system_integration.py::TestResourceOptimizationWithPriorities::test_priority_affects_resource_allocation", "type": "Function", "lineno": 627}]}, {"nodeid": "tests/test_full_system_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow", "type": "Class"}, {"nodeid": "tests/test_full_system_integration.py::TestFailureResilienceWithForecasting", "type": "Class"}, {"nodeid": "tests/test_full_system_integration.py::TestResourceOptimizationWithPriorities", "type": "Class"}]}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager", "outcome": "passed", "result": [{"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "type": "TestCaseFunction", "lineno": 234}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_force_update_ignores_threshold", "type": "TestCaseFunction", "lineno": 208}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "type": "TestCaseFunction", "lineno": 476}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "type": "TestCaseFunction", "lineno": 506}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "type": "TestCaseFunction", "lineno": 535}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_initialization", "type": "TestCaseFunction", "lineno": 109}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "type": "TestCaseFunction", "lineno": 395}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "type": "TestCaseFunction", "lineno": 117}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "type": "TestCaseFunction", "lineno": 291}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_reallocation_strategy_threshold_based", "type": "TestCaseFunction", "lineno": 347}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "type": "TestCaseFunction", "lineno": 425}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_small_priority_change_below_threshold", "type": "TestCaseFunction", "lineno": 176}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "type": "TestCaseFunction", "lineno": 138}]}, {"nodeid": "tests/test_priority_manager.py", "outcome": "passed", "result": [{"nodeid": "tests/test_priority_manager.py::TestPriorityManager", "type": "UnitTestCase"}]}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration", "outcome": "passed", "result": [{"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_complementary_scenario_detection", "type": "TestCaseFunction", "lineno": 272}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_priority_workflow", "type": "TestCaseFunction", "lineno": 136}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "type": "TestCaseFunction", "lineno": 234}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_adaptation_to_metric_changes", "type": "TestCaseFunction", "lineno": 196}]}, {"nodeid": "tests/test_scenario_management_integration.py", "outcome": "passed", "result": [{"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration", "type": "UnitTestCase"}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/dependency_tracking", "type": "Package"}, {"nodeid": "tests/failure_resilience", "type": "Package"}, {"nodeid": "tests/job_management", "type": "Package"}, {"nodeid": "tests/resource_forecasting", "type": "Dir"}, {"nodeid": "tests/scenario_management", "type": "Package"}, {"nodeid": "tests/test_full_system_integration.py", "type": "Module"}, {"nodeid": "tests/test_priority_manager.py", "type": "Module"}, {"nodeid": "tests/test_scenario_management_integration.py", "type": "Module"}]}], "tests": [{"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_stage", "lineno": 54, "outcome": "passed", "keywords": ["test_add_stage", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.001145854010246694, "outcome": "passed"}, "call": {"duration": 0.0001469251001253724, "outcome": "passed"}, "teardown": {"duration": 0.00011595501564443111, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency", "lineno": 59, "outcome": "passed", "keywords": ["test_add_dependency", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002261260524392128, "outcome": "passed"}, "call": {"duration": 0.0001325670164078474, "outcome": "passed"}, "teardown": {"duration": 9.956408757716417e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_add_dependency_with_cycle", "lineno": 73, "outcome": "passed", "keywords": ["test_add_dependency_with_cycle", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00020408991258591413, "outcome": "passed"}, "call": {"duration": 0.00011280400212854147, "outcome": "passed"}, "teardown": {"duration": 8.80509614944458e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_remove_dependency", "lineno": 79, "outcome": "passed", "keywords": ["test_remove_dependency", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019190798047930002, "outcome": "passed"}, "call": {"duration": 0.00010481395293027163, "outcome": "passed"}, "teardown": {"duration": 9.205110836774111e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_dependencies", "lineno": 85, "outcome": "passed", "keywords": ["test_get_dependencies", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00018253503367304802, "outcome": "passed"}, "call": {"duration": 0.0001045420067384839, "outcome": "passed"}, "teardown": {"duration": 8.530495688319206e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_root_and_leaf_stages", "lineno": 94, "outcome": "passed", "keywords": ["test_get_root_and_leaf_stages", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001762639731168747, "outcome": "passed"}, "call": {"duration": 0.00011131493374705315, "outcome": "passed"}, "teardown": {"duration": 8.85039335116744e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_ready_stages", "lineno": 103, "outcome": "passed", "keywords": ["test_get_ready_stages", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001918919151648879, "outcome": "passed"}, "call": {"duration": 0.00011847494170069695, "outcome": "passed"}, "teardown": {"duration": 8.550600614398718e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_are_all_dependencies_satisfied", "lineno": 124, "outcome": "passed", "keywords": ["test_are_all_dependencies_satisfied", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00031078991014510393, "outcome": "passed"}, "call": {"duration": 0.00011678901500999928, "outcome": "passed"}, "teardown": {"duration": 9.004701860249043e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_update_stage_status", "lineno": 136, "outcome": "passed", "keywords": ["test_update_stage_status", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019049993716180325, "outcome": "passed"}, "call": {"duration": 9.684206452220678e-05, "outcome": "passed"}, "teardown": {"duration": 9.207706898450851e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_get_critical_path", "lineno": 146, "outcome": "passed", "keywords": ["test_get_critical_path", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017863092944025993, "outcome": "passed"}, "call": {"duration": 0.0004135729977861047, "outcome": "passed"}, "teardown": {"duration": 0.00011350004933774471, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_is_stage_blocked", "lineno": 157, "outcome": "passed", "keywords": ["test_is_stage_blocked", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019846600480377674, "outcome": "passed"}, "call": {"duration": 9.760190732777119e-05, "outcome": "passed"}, "teardown": {"duration": 8.819007780402899e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_validate", "lineno": 167, "outcome": "passed", "keywords": ["test_validate", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00020392693113535643, "outcome": "passed"}, "call": {"duration": 0.00012562598567456007, "outcome": "passed"}, "teardown": {"duration": 9.131606202572584e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyGraph::test_serialize_deserialize", "lineno": 184, "outcome": "passed", "keywords": ["test_serialize_deserialize", "TestDependencyGraph", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019114697352051735, "outcome": "passed"}, "call": {"duration": 0.00020241900347173214, "outcome": "passed"}, "teardown": {"duration": 9.830098133534193e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_create_dependency_graph", "lineno": 267, "outcome": "passed", "keywords": ["test_create_dependency_graph", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00020050001330673695, "outcome": "passed"}, "call": {"duration": 0.0001494180178269744, "outcome": "passed"}, "teardown": {"duration": 9.258103091269732e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dependency", "lineno": 285, "outcome": "passed", "keywords": ["test_add_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017524196300655603, "outcome": "passed"}, "call": {"duration": 0.0001666559837758541, "outcome": "passed"}, "teardown": {"duration": 9.062502067536116e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_remove_dependency", "lineno": 308, "outcome": "passed", "keywords": ["test_remove_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001718519488349557, "outcome": "passed"}, "call": {"duration": 0.00014740799088031054, "outcome": "passed"}, "teardown": {"duration": 8.576398249715567e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_transition_rule", "lineno": 326, "outcome": "passed", "keywords": ["test_add_transition_rule", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002495549852028489, "outcome": "passed"}, "call": {"duration": 0.0001896880567073822, "outcome": "passed"}, "teardown": {"duration": 9.337207302451134e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_update_stage_status", "lineno": 350, "outcome": "passed", "keywords": ["test_update_stage_status", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016544898971915245, "outcome": "passed"}, "call": {"duration": 0.0002045850269496441, "outcome": "passed"}, "teardown": {"duration": 9.198801126331091e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_ready_stages", "lineno": 374, "outcome": "passed", "keywords": ["test_get_ready_stages", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016149599105119705, "outcome": "passed"}, "call": {"duration": 0.00017692300025373697, "outcome": "passed"}, "teardown": {"duration": 9.572494309395552e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_is_stage_ready", "lineno": 392, "outcome": "passed", "keywords": ["test_is_stage_ready", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001856969902291894, "outcome": "passed"}, "call": {"duration": 0.00017249700613319874, "outcome": "passed"}, "teardown": {"duration": 8.691998664289713e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_blocking_stages", "lineno": 409, "outcome": "passed", "keywords": ["test_get_blocking_stages", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00015924801118671894, "outcome": "passed"}, "call": {"duration": 0.00017438095528632402, "outcome": "passed"}, "teardown": {"duration": 8.807610720396042e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_critical_path", "lineno": 425, "outcome": "passed", "keywords": ["test_get_critical_path", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016103091184049845, "outcome": "passed"}, "call": {"duration": 0.0001603430137038231, "outcome": "passed"}, "teardown": {"duration": 8.52260272949934e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_bypass_dependency", "lineno": 439, "outcome": "passed", "keywords": ["test_bypass_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017331703566014767, "outcome": "passed"}, "call": {"duration": 0.0001577290240675211, "outcome": "passed"}, "teardown": {"duration": 8.770392742007971e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_add_dynamic_dependency", "lineno": 464, "outcome": "passed", "keywords": ["test_add_dynamic_dependency", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001591669861227274, "outcome": "passed"}, "call": {"duration": 0.0001626000739634037, "outcome": "passed"}, "teardown": {"duration": 8.797994814813137e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_get_execution_plan", "lineno": 490, "outcome": "passed", "keywords": ["test_get_execution_plan", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023886398412287235, "outcome": "passed"}, "call": {"duration": 0.00015146005898714066, "outcome": "passed"}, "teardown": {"duration": 9.123899508267641e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestDependencyTracker::test_validate_simulation", "lineno": 506, "outcome": "passed", "keywords": ["test_validate_simulation", "TestDependencyTracker", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016435596626251936, "outcome": "passed"}, "call": {"duration": 0.00010634493082761765, "outcome": "passed"}, "teardown": {"duration": 0.0001019149785861373, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_register_template", "lineno": 540, "outcome": "passed", "keywords": ["test_register_template", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001461799256503582, "outcome": "passed"}, "call": {"duration": 0.00016930303536355495, "outcome": "passed"}, "teardown": {"duration": 8.642207831144333e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_instance", "lineno": 555, "outcome": "passed", "keywords": ["test_create_instance", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013024802319705486, "outcome": "passed"}, "call": {"duration": 0.0003184779779985547, "outcome": "passed"}, "teardown": {"duration": 0.00010168401058763266, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_linear_workflow", "lineno": 595, "outcome": "passed", "keywords": ["test_create_linear_workflow", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014556990936398506, "outcome": "passed"}, "call": {"duration": 0.00027775298804044724, "outcome": "passed"}, "teardown": {"duration": 9.356997907161713e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_create_parallel_workflow", "lineno": 636, "outcome": "passed", "keywords": ["test_create_parallel_workflow", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013226910959929228, "outcome": "passed"}, "call": {"duration": 0.0003277159994468093, "outcome": "passed"}, "teardown": {"duration": 0.00010636902879923582, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_next_stages", "lineno": 673, "outcome": "passed", "keywords": ["test_get_next_stages", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014382798690348864, "outcome": "passed"}, "call": {"duration": 0.000403024023398757, "outcome": "passed"}, "teardown": {"duration": 0.00010191509500145912, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_update_instance", "lineno": 726, "outcome": "passed", "keywords": ["test_update_instance", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014482298865914345, "outcome": "passed"}, "call": {"duration": 0.00033262697979807854, "outcome": "passed"}, "teardown": {"duration": 9.64929349720478e-05, "outcome": "passed"}}, {"nodeid": "tests/dependency_tracking/test_dependency_tracker.py::TestWorkflowManager::test_get_workflow_status", "lineno": 781, "outcome": "passed", "keywords": ["test_get_workflow_status", "TestWorkflowManager", "test_dependency_tracker.py", "dependency_tracking", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014950602781027555, "outcome": "passed"}, "call": {"duration": 0.00031843106262385845, "outcome": "passed"}, "teardown": {"duration": 0.000103021040558815, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_checkpoint_manager_init", "lineno": 105, "outcome": "passed", "keywords": ["test_checkpoint_manager_init", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0004923200467601418, "outcome": "passed"}, "call": {"duration": 0.00010035198647528887, "outcome": "passed"}, "teardown": {"duration": 0.00019772397354245186, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_register_policy", "lineno": 115, "outcome": "passed", "keywords": ["test_register_policy", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00025015498977154493, "outcome": "passed"}, "call": {"duration": 0.00012327800504863262, "outcome": "passed"}, "teardown": {"duration": 0.00016477296594530344, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_default_policy", "lineno": 133, "outcome": "passed", "keywords": ["test_get_default_policy", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022433395497500896, "outcome": "passed"}, "call": {"duration": 0.00011523603461682796, "outcome": "passed"}, "teardown": {"duration": 0.00014780904166400433, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_create_manager_for_simulation", "lineno": 148, "outcome": "passed", "keywords": ["test_create_manager_for_simulation", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002925019944086671, "outcome": "passed"}, "call": {"duration": 0.0001973779872059822, "outcome": "passed"}, "teardown": {"duration": 0.0002165300538763404, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_update_policy", "lineno": 182, "outcome": "passed", "keywords": ["test_update_policy", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002884670393541455, "outcome": "passed"}, "call": {"duration": 0.00018312595784664154, "outcome": "passed"}, "teardown": {"duration": 0.0001864529913291335, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_should_create_checkpoint", "lineno": 217, "outcome": "passed", "keywords": ["test_should_create_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00027628405950963497, "outcome": "passed"}, "call": {"duration": 0.00017459504306316376, "outcome": "passed"}, "teardown": {"duration": 0.00018800992984324694, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_create_checkpoint", "lineno": 244, "outcome": "passed", "keywords": ["test_create_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002842819085344672, "outcome": "passed"}, "call": {"duration": 0.010226438054814935, "outcome": "passed"}, "teardown": {"duration": 0.0005580850411206484, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_validate_checkpoint", "lineno": 284, "outcome": "xfailed", "keywords": ["test_validate_checkpoint", "xfail", "pytestmark", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003142119385302067, "outcome": "passed"}, "call": {"duration": 0.011621034936979413, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/failure_resilience/test_checkpoint_manager.py", "lineno": 316, "message": "AssertionError: assert <ValidationRe...ALID: 'valid'> == <ValidationRe... 'incomplete'>\n  \n  - incomplete\n  + valid"}, "traceback": [{"path": "tests/failure_resilience/test_checkpoint_manager.py", "lineno": 316, "message": "AssertionError"}], "log": [{"name": "concurrent_task_scheduler.failure_resilience.checkpoint_manager", "msg": "Checkpoint nonexistent not found for simulation sim_climate_test", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/failure_resilience/checkpoint_manager.py", "filename": "checkpoint_manager.py", "module": "checkpoint_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 351, "funcName": "validate_checkpoint", "created": 1747285139.1547732, "msecs": 154.0, "relativeCreated": 1223.2356071472168, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}], "longrepr": "checkpoint_manager = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointManager object at 0x7f459c24fbc0>\nsample_simulation = Simulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': SimulationS...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Checkpoint validation not fully implemented yet\")\n    def test_validate_checkpoint(checkpoint_manager, sample_simulation):\n        \"\"\"Test validating a checkpoint.\"\"\"\n        # Create checkpoint first\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint = result.value\n    \n        # Validate checkpoint\n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint.id, sample_simulation.id)\n        assert validation_result == ValidationResult.VALID\n    \n        # Test validation caching\n        # This call should use the cached result\n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint.id, sample_simulation.id)\n        assert validation_result == ValidationResult.VALID\n    \n        # Test with non-existent checkpoint\n        validation_result = checkpoint_manager.validate_checkpoint(\"nonexistent\", sample_simulation.id)\n        assert validation_result == ValidationResult.MISSING\n    \n        # Test with incomplete checkpoint\n        # Create a checkpoint, then delete one of the required files\n        result = checkpoint_manager.create_checkpoint(sample_simulation, description=\"Incomplete test\")\n        assert result.success\n        checkpoint2 = result.value\n    \n        # Delete the data file\n        os.remove(os.path.join(checkpoint2.path, \"data.bin\"))\n    \n        validation_result = checkpoint_manager.validate_checkpoint(checkpoint2.id, sample_simulation.id)\n>       assert validation_result == ValidationResult.INCOMPLETE\nE       AssertionError: assert <ValidationRe...ALID: 'valid'> == <ValidationRe... 'incomplete'>\nE         \nE         - incomplete\nE         + valid\n\ntests/failure_resilience/test_checkpoint_manager.py:316: AssertionError"}, "teardown": {"duration": 0.0002975320676341653, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_latest_checkpoint", "lineno": 330, "outcome": "passed", "keywords": ["test_get_latest_checkpoint", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003054429544135928, "outcome": "passed"}, "call": {"duration": 0.013892209972254932, "outcome": "passed"}, "teardown": {"duration": 0.0005346080288290977, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_get_all_checkpoints", "lineno": 360, "outcome": "passed", "keywords": ["test_get_all_checkpoints", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00031329505145549774, "outcome": "passed"}, "call": {"duration": 0.013670801068656147, "outcome": "passed"}, "teardown": {"duration": 0.000522937043569982, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_restore_from_checkpoint", "lineno": 383, "outcome": "xfailed", "keywords": ["test_restore_from_checkpoint", "xfail", "pytestmark", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00030056294053792953, "outcome": "passed"}, "call": {"duration": 0.0112806890392676, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/failure_resilience/test_checkpoint_manager.py", "lineno": 415, "message": "assert not True\n +  where True = Result(success=True, value=True, error=None).success"}, "traceback": [{"path": "tests/failure_resilience/test_checkpoint_manager.py", "lineno": 415, "message": "AssertionError"}], "longrepr": "checkpoint_manager = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointManager object at 0x7f459c09ab40>\nsample_simulation = Simulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': SimulationS...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Checkpoint validation and restoration not fully implemented yet\")\n    def test_restore_from_checkpoint(checkpoint_manager, sample_simulation):\n        \"\"\"Test restoring a simulation from a checkpoint.\"\"\"\n        # Create checkpoint first\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint = result.value\n    \n        # Restore\n        restore_result = checkpoint_manager.restore_from_checkpoint(checkpoint.id, sample_simulation.id)\n        assert restore_result.success\n    \n        # Verify restore count was incremented\n        checkpoint = checkpoint_manager.checkpoints[sample_simulation.id][checkpoint.id]\n        assert checkpoint.restore_count == 1\n        assert checkpoint.last_restore_time is not None\n    \n        # Test with non-existent checkpoint\n        restore_result = checkpoint_manager.restore_from_checkpoint(\"nonexistent\", sample_simulation.id)\n        assert not restore_result.success\n    \n        # Test with invalid checkpoint\n        # Create a checkpoint, then delete one of the required files\n        result = checkpoint_manager.create_checkpoint(sample_simulation)\n        assert result.success\n        checkpoint2 = result.value\n    \n        # Delete the data file\n        os.remove(os.path.join(checkpoint2.path, \"data.bin\"))\n    \n        restore_result = checkpoint_manager.restore_from_checkpoint(checkpoint2.id, sample_simulation.id)\n>       assert not restore_result.success\nE       assert not True\nE        +  where True = Result(success=True, value=True, error=None).success\n\ntests/failure_resilience/test_checkpoint_manager.py:415: AssertionError"}, "teardown": {"duration": 0.0002968630287796259, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_checkpoint_manager.py::test_checkpoint_coordinator", "lineno": 418, "outcome": "passed", "keywords": ["test_checkpoint_coordinator", "test_checkpoint_manager.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003338779788464308, "outcome": "passed"}, "call": {"duration": 0.004633028991520405, "outcome": "passed"}, "teardown": {"duration": 0.00035377906169742346, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_failure_detector_init", "lineno": 110, "outcome": "passed", "keywords": ["test_failure_detector_init", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014819204807281494, "outcome": "passed"}, "call": {"duration": 9.143096394836903e-05, "outcome": "passed"}, "teardown": {"duration": 8.493696805089712e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_record_heartbeat", "lineno": 121, "outcome": "passed", "keywords": ["test_record_heartbeat", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001083890674635768, "outcome": "passed"}, "call": {"duration": 9.436695836484432e-05, "outcome": "passed"}, "teardown": {"duration": 8.188700303435326e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_check_node_health", "lineno": 130, "outcome": "passed", "keywords": ["test_check_node_health", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014465802814811468, "outcome": "passed"}, "call": {"duration": 0.00014020397793501616, "outcome": "passed"}, "teardown": {"duration": 9.098602458834648e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_check_simulation_health", "lineno": 163, "outcome": "passed", "keywords": ["test_check_simulation_health", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017516501247882843, "outcome": "passed"}, "call": {"duration": 0.00010379299055784941, "outcome": "passed"}, "teardown": {"duration": 0.00011760799679905176, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_update_simulation_progress", "lineno": 185, "outcome": "passed", "keywords": ["test_update_simulation_progress", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010724097955971956, "outcome": "passed"}, "call": {"duration": 8.531403727829456e-05, "outcome": "passed"}, "teardown": {"duration": 8.760602213442326e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_update_simulation_status", "lineno": 197, "outcome": "passed", "keywords": ["test_update_simulation_status", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010522198863327503, "outcome": "passed"}, "call": {"duration": 8.442800026386976e-05, "outcome": "passed"}, "teardown": {"duration": 7.88660254329443e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_detect_node_failures", "lineno": 209, "outcome": "passed", "keywords": ["test_detect_node_failures", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013482302892953157, "outcome": "passed"}, "call": {"duration": 0.00018247903790324926, "outcome": "passed"}, "teardown": {"duration": 9.284599218517542e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_detect_simulation_failures", "lineno": 224, "outcome": "passed", "keywords": ["test_detect_simulation_failures", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016150902956724167, "outcome": "passed"}, "call": {"duration": 0.00012904696632176638, "outcome": "passed"}, "teardown": {"duration": 8.882407564669847e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_report_failure", "lineno": 246, "outcome": "passed", "keywords": ["test_report_failure", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010669103357940912, "outcome": "passed"}, "call": {"duration": 0.00011794804595410824, "outcome": "passed"}, "teardown": {"duration": 8.201191667467356e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_active_failures", "lineno": 283, "outcome": "passed", "keywords": ["test_get_active_failures", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 9.996793232858181e-05, "outcome": "passed"}, "call": {"duration": 0.0001285059843212366, "outcome": "passed"}, "teardown": {"duration": 9.40720783546567e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_resolve_failure", "lineno": 329, "outcome": "passed", "keywords": ["test_resolve_failure", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001077490160241723, "outcome": "passed"}, "call": {"duration": 0.00013960094656795263, "outcome": "passed"}, "teardown": {"duration": 8.418108336627483e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_reliability_metrics", "lineno": 358, "outcome": "passed", "keywords": ["test_reliability_metrics", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013791199307888746, "outcome": "passed"}, "call": {"duration": 0.00016321404837071896, "outcome": "passed"}, "teardown": {"duration": 8.674198761582375e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_failure_type_determination", "lineno": 386, "outcome": "passed", "keywords": ["test_failure_type_determination", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010377203579992056, "outcome": "passed"}, "call": {"duration": 8.976506069302559e-05, "outcome": "passed"}, "teardown": {"duration": 7.85220181569457e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_recovery_manager_init", "lineno": 399, "outcome": "passed", "keywords": ["test_recovery_manager_init", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013762398157268763, "outcome": "passed"}, "call": {"duration": 8.524209260940552e-05, "outcome": "passed"}, "teardown": {"duration": 8.947495371103287e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_recovery_strategy", "lineno": 408, "outcome": "passed", "keywords": ["test_get_recovery_strategy", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012501899618655443, "outcome": "passed"}, "call": {"duration": 9.729003068059683e-05, "outcome": "passed"}, "teardown": {"duration": 9.455997496843338e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_initiate_recovery", "lineno": 445, "outcome": "passed", "keywords": ["test_initiate_recovery", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012248707935214043, "outcome": "passed"}, "call": {"duration": 0.00016060599591583014, "outcome": "passed"}, "teardown": {"duration": 9.044795297086239e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_complete_recovery", "lineno": 491, "outcome": "passed", "keywords": ["test_complete_recovery", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013546296395361423, "outcome": "passed"}, "call": {"duration": 0.00015317101497203112, "outcome": "passed"}, "teardown": {"duration": 9.508500806987286e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_get_active_recoveries", "lineno": 528, "outcome": "passed", "keywords": ["test_get_active_recoveries", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013356399722397327, "outcome": "passed"}, "call": {"duration": 0.00014891894534230232, "outcome": "passed"}, "teardown": {"duration": 0.00010853796266019344, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_cancel_recovery", "lineno": 567, "outcome": "passed", "keywords": ["test_cancel_recovery", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012773193884640932, "outcome": "passed"}, "call": {"duration": 0.00013583700638264418, "outcome": "passed"}, "teardown": {"duration": 9.22270119190216e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_detector.py::test_process_failures", "lineno": 602, "outcome": "passed", "keywords": ["test_process_failures", "test_failure_detector.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001327419886365533, "outcome": "passed"}, "call": {"duration": 0.00024590606335550547, "outcome": "passed"}, "teardown": {"duration": 9.931600652635098e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_record_heartbeat", "lineno": 108, "outcome": "passed", "keywords": ["test_record_heartbeat", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012622796930372715, "outcome": "passed"}, "call": {"duration": 8.911301847547293e-05, "outcome": "passed"}, "teardown": {"duration": 8.251005783677101e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_node_health", "lineno": 115, "outcome": "passed", "keywords": ["test_check_node_health", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013740500435233116, "outcome": "passed"}, "call": {"duration": 0.00013590999878942966, "outcome": "passed"}, "teardown": {"duration": 9.427592158317566e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_check_simulation_health", "lineno": 180, "outcome": "passed", "keywords": ["test_check_simulation_health", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00018369092140346766, "outcome": "passed"}, "call": {"duration": 0.0001226110616698861, "outcome": "passed"}, "teardown": {"duration": 0.000100955949164927, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_update_simulation_progress", "lineno": 209, "outcome": "passed", "keywords": ["test_update_simulation_progress", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010658998508006334, "outcome": "passed"}, "call": {"duration": 8.799100760370493e-05, "outcome": "passed"}, "teardown": {"duration": 8.892698679119349e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_node_failures", "lineno": 221, "outcome": "passed", "keywords": ["test_detect_node_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013495096936821938, "outcome": "passed"}, "call": {"duration": 0.00014290702529251575, "outcome": "passed"}, "teardown": {"duration": 9.023305028676987e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_detect_simulation_failures", "lineno": 255, "outcome": "passed", "keywords": ["test_detect_simulation_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0008410470327362418, "outcome": "passed"}, "call": {"duration": 0.0002006130525842309, "outcome": "passed"}, "teardown": {"duration": 0.00010475597810000181, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_report_failure", "lineno": 299, "outcome": "passed", "keywords": ["test_report_failure", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012493494432419538, "outcome": "passed"}, "call": {"duration": 0.00011618598364293575, "outcome": "passed"}, "teardown": {"duration": 9.072700049728155e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_active_failures", "lineno": 319, "outcome": "passed", "keywords": ["test_get_active_failures", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012100499588996172, "outcome": "passed"}, "call": {"duration": 0.00013316294644027948, "outcome": "passed"}, "teardown": {"duration": 9.762600529938936e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_resolve_failure", "lineno": 361, "outcome": "passed", "keywords": ["test_resolve_failure", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00010880094487220049, "outcome": "passed"}, "call": {"duration": 0.00012513704132288694, "outcome": "passed"}, "teardown": {"duration": 8.955399971455336e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureDetector::test_get_node_reliability_score", "lineno": 387, "outcome": "passed", "keywords": ["test_get_node_reliability_score", "TestFailureDetector", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001396059524267912, "outcome": "passed"}, "call": {"duration": 0.0001027729595080018, "outcome": "passed"}, "teardown": {"duration": 0.00011787295807152987, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_get_recovery_strategy", "lineno": 421, "outcome": "passed", "keywords": ["test_get_recovery_strategy", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001481689978390932, "outcome": "passed"}, "call": {"duration": 0.00011214695405215025, "outcome": "passed"}, "teardown": {"duration": 9.241001680493355e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_initiate_recovery", "lineno": 451, "outcome": "passed", "keywords": ["test_initiate_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013286701869219542, "outcome": "passed"}, "call": {"duration": 0.00013926695100963116, "outcome": "passed"}, "teardown": {"duration": 0.00010164710693061352, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_complete_recovery", "lineno": 483, "outcome": "passed", "keywords": ["test_complete_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001328829675912857, "outcome": "passed"}, "call": {"duration": 0.00015096901915967464, "outcome": "passed"}, "teardown": {"duration": 9.728001896291971e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_cancel_recovery", "lineno": 528, "outcome": "passed", "keywords": ["test_cancel_recovery", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014254101552069187, "outcome": "passed"}, "call": {"duration": 0.000145943951793015, "outcome": "passed"}, "teardown": {"duration": 9.433797094970942e-05, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestFailureRecoveryManager::test_process_failures", "lineno": 558, "outcome": "passed", "keywords": ["test_process_failures", "TestFailureRecoveryManager", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00015106599312275648, "outcome": "passed"}, "call": {"duration": 0.0001711950171738863, "outcome": "passed"}, "teardown": {"duration": 0.00010144698899239302, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_set_resilience_level", "lineno": 681, "outcome": "passed", "keywords": ["test_set_resilience_level", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003021908923983574, "outcome": "passed"}, "call": {"duration": 0.00010701199062168598, "outcome": "passed"}, "teardown": {"duration": 0.0001629090402275324, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_record_event", "lineno": 699, "outcome": "passed", "keywords": ["test_record_event", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002822049427777529, "outcome": "passed"}, "call": {"duration": 0.00011880695819854736, "outcome": "passed"}, "teardown": {"duration": 0.00015778804663568735, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_schedule_checkpoint", "lineno": 721, "outcome": "passed", "keywords": ["test_schedule_checkpoint", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003258739598095417, "outcome": "passed"}, "call": {"duration": 0.00010734191164374352, "outcome": "passed"}, "teardown": {"duration": 0.00016377505380660295, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_node_status_change", "lineno": 741, "outcome": "passed", "keywords": ["test_handle_node_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003299060044810176, "outcome": "passed"}, "call": {"duration": 0.00021400104742497206, "outcome": "passed"}, "teardown": {"duration": 0.00016844598576426506, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_simulation_status_change", "lineno": 772, "outcome": "passed", "keywords": ["test_handle_simulation_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00032403995282948017, "outcome": "passed"}, "call": {"duration": 0.00017478002700954676, "outcome": "passed"}, "teardown": {"duration": 0.0001665659947320819, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_stage_status_change", "lineno": 811, "outcome": "passed", "keywords": ["test_handle_stage_status_change", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003340350231155753, "outcome": "passed"}, "call": {"duration": 0.00016550696454942226, "outcome": "passed"}, "teardown": {"duration": 0.00016371894162148237, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_handle_failure_detection", "lineno": 843, "outcome": "passed", "keywords": ["test_handle_failure_detection", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00028091296553611755, "outcome": "passed"}, "call": {"duration": 0.00015266507398337126, "outcome": "passed"}, "teardown": {"duration": 0.0001651920611038804, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_complete_recovery", "lineno": 873, "outcome": "passed", "keywords": ["test_complete_recovery", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002907009329646826, "outcome": "passed"}, "call": {"duration": 0.00014528795145452023, "outcome": "passed"}, "teardown": {"duration": 0.00017718097660690546, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_failure_resilience.py::TestResilienceCoordinator::test_detect_and_handle_failures", "lineno": 910, "outcome": "xfailed", "keywords": ["test_detect_and_handle_failures", "xfail", "pytestmark", "TestResilienceCoordinator", "test_failure_resilience.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003550059627741575, "outcome": "passed"}, "call": {"duration": 0.0005355509929358959, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/failure_resilience/test_failure_resilience.py", "lineno": 968, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/failure_resilience/test_failure_resilience.py", "lineno": 968, "message": "AssertionError"}], "longrepr": "self = <tests.failure_resilience.test_failure_resilience.TestResilienceCoordinator object at 0x7f4645671190>\nresilience_coordinator = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f46456704d0>\nmock_node = ComputeNode(id='node-123', name='compute-123', node_type=<NodeType.COMPUTE: 'compute'>, status=<NodeStatus.ONLINE: 'on...igned_simulations=[], last_failure_time=None, maintenance_window=None, location='data_center_1', reliability_score=1.0)\nmock_simulation = MockSimulation(id='sim-123', name='Test Simulation', description='A test simulation', stages={'stage-1': SimulationSta...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Test is inconsistent due to implementation details\")\n    def test_detect_and_handle_failures(self, resilience_coordinator, mock_node, mock_simulation):\n        \"\"\"Test end-to-end failure detection and handling.\"\"\"\n        # Create problematic node and simulation\n        problem_node = ComputeNode(\n            id=\"problem-node\",\n            name=\"problem\",\n            status=NodeStatus.ONLINE,\n            current_load={\"cpu\": 0.99, \"memory\": 0.5, \"storage\": 0.5},  # High CPU load\n            cpu_cores=32,\n            memory_gb=128,\n            storage_gb=1000,\n            node_type=NodeType.COMPUTE,\n            gpu_count=0,\n            network_bandwidth_gbps=10,\n            location=\"data_center_1\"\n        )\n    \n        nodes = {\n            \"node-123\": mock_node,\n            \"problem-node\": problem_node,\n        }\n    \n        # Record heartbeats\n        for node_id in nodes:\n            resilience_coordinator.failure_detector.record_heartbeat(node_id)\n    \n        # Create problematic simulation\n        stalled_sim = StalledSimulation(\n            id=\"stalled-sim\",\n            name=\"Stalled Simulation\",\n            status=SimulationStatus.RUNNING,\n            stages={}\n        )\n    \n        # Set up simulation to appear stalled\n        resilience_coordinator.failure_detector.simulation_health_history[\"stalled-sim\"] = {\n            \"last_progress_update\": datetime.now() - timedelta(minutes=120),  # No progress for 2 hours\n        }\n    \n        simulations = {\n            \"sim-123\": mock_simulation,\n            \"stalled-sim\": stalled_sim,\n        }\n    \n        # Set low threshold for testing\n        resilience_coordinator.failure_detector.thresholds[\"simulation_progress_stall_minutes\"] = 60\n    \n        # Detect and handle failures\n        with patch.object(resilience_coordinator.recovery_manager, 'initiate_recovery', return_value=Result.ok(RecoveryStrategy.RESTART)):\n            handled_failures = resilience_coordinator.detect_and_handle_failures(nodes, simulations)\n    \n        # Should detect and handle at least one failure\n        assert len(handled_failures) > 0\n    \n        # The CPU load failure and stalled simulation should be detected\n        node_failures = resilience_coordinator.failure_detector.get_active_failures(node_id=\"problem-node\")\n>       assert len(node_failures) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/failure_resilience/test_failure_resilience.py:968: AssertionError"}, "teardown": {"duration": 0.00021999410819262266, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_resilience_coordinator_init", "lineno": 152, "outcome": "passed", "keywords": ["test_resilience_coordinator_init", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00031147291883826256, "outcome": "passed"}, "call": {"duration": 0.00010362104512751102, "outcome": "passed"}, "teardown": {"duration": 0.00016750802751630545, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_set_resilience_level", "lineno": 163, "outcome": "passed", "keywords": ["test_set_resilience_level", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00027931900694966316, "outcome": "passed"}, "call": {"duration": 9.914499241858721e-05, "outcome": "passed"}, "teardown": {"duration": 0.00016346294432878494, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_record_event", "lineno": 181, "outcome": "passed", "keywords": ["test_record_event", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00027563306502997875, "outcome": "passed"}, "call": {"duration": 0.00011035893112421036, "outcome": "passed"}, "teardown": {"duration": 0.0001590329920873046, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_schedule_checkpoint", "lineno": 203, "outcome": "passed", "keywords": ["test_schedule_checkpoint", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003304269630461931, "outcome": "passed"}, "call": {"duration": 0.00010550394654273987, "outcome": "passed"}, "teardown": {"duration": 0.00016663596034049988, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_process_scheduled_checkpoints", "lineno": 219, "outcome": "passed", "keywords": ["test_process_scheduled_checkpoints", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003392189973965287, "outcome": "passed"}, "call": {"duration": 0.0046036080457270145, "outcome": "passed"}, "teardown": {"duration": 0.0003781360574066639, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_node_status_change", "lineno": 238, "outcome": "passed", "keywords": ["test_handle_node_status_change", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00037981104105710983, "outcome": "passed"}, "call": {"duration": 0.00018762797117233276, "outcome": "passed"}, "teardown": {"duration": 0.00017219095025211573, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_simulation_status_change", "lineno": 290, "outcome": "passed", "keywords": ["test_handle_simulation_status_change", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00033627799712121487, "outcome": "passed"}, "call": {"duration": 0.00018680095672607422, "outcome": "passed"}, "teardown": {"duration": 0.0001719790743663907, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_stage_status_change", "lineno": 344, "outcome": "passed", "keywords": ["test_handle_stage_status_change", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00031538004986941814, "outcome": "passed"}, "call": {"duration": 0.00017272902186959982, "outcome": "passed"}, "teardown": {"duration": 0.00017543509602546692, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_handle_failure_detection", "lineno": 397, "outcome": "passed", "keywords": ["test_handle_failure_detection", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003269679145887494, "outcome": "passed"}, "call": {"duration": 0.00016594596672803164, "outcome": "passed"}, "teardown": {"duration": 0.00017636409029364586, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_complete_recovery", "lineno": 429, "outcome": "passed", "keywords": ["test_complete_recovery", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003337530652061105, "outcome": "passed"}, "call": {"duration": 0.00020049093291163445, "outcome": "passed"}, "teardown": {"duration": 0.00017377606127411127, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_active_recoveries", "lineno": 486, "outcome": "passed", "keywords": ["test_get_active_recoveries", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002854799386113882, "outcome": "passed"}, "call": {"duration": 0.0001674490049481392, "outcome": "passed"}, "teardown": {"duration": 0.0001628840109333396, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_checkpoint_schedule", "lineno": 521, "outcome": "passed", "keywords": ["test_get_checkpoint_schedule", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003141800407320261, "outcome": "passed"}, "call": {"duration": 0.00010657601524144411, "outcome": "passed"}, "teardown": {"duration": 0.0001793919363990426, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_get_resilience_metrics", "lineno": 536, "outcome": "passed", "keywords": ["test_get_resilience_metrics", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002685670042410493, "outcome": "passed"}, "call": {"duration": 0.00010740896686911583, "outcome": "passed"}, "teardown": {"duration": 0.00016208598390221596, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_detect_and_handle_failures", "lineno": 554, "outcome": "xpassed", "keywords": ["test_detect_and_handle_failures", "xfail", "pytestmark", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003626249963417649, "outcome": "passed"}, "call": {"duration": 0.00016689393669366837, "outcome": "passed"}, "teardown": {"duration": 0.0001862290082499385, "outcome": "passed"}}, {"nodeid": "tests/failure_resilience/test_resilience_coordinator.py::test_process_checkpoints", "lineno": 577, "outcome": "xfailed", "keywords": ["test_process_checkpoints", "xfail", "pytestmark", "test_resilience_coordinator.py", "failure_resilience", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00033185596112161875, "outcome": "passed"}, "call": {"duration": 0.0047202479327097535, "outcome": "skipped", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/failure_resilience/test_resilience_coordinator.py", "lineno": 591, "message": "AssertionError: assert 'sim_climate_test' in {}\n +  where 'sim_climate_test' = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)).id\n +  and   {} = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f459c09b2c0>.scheduled_checkpoints\n +    where <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f459c09b2c0> = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f459c098620>.checkpoint_coordinator"}, "traceback": [{"path": "tests/failure_resilience/test_resilience_coordinator.py", "lineno": 591, "message": "AssertionError"}], "longrepr": "resilience_coordinator = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f459c098620>\nsample_simulation = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    @pytest.mark.xfail(reason=\"Test depends on specific timing for checkpoint scheduling\")\n    def test_process_checkpoints(resilience_coordinator, sample_simulation):\n        \"\"\"Test processing checkpoints for simulations.\"\"\"\n        # Create dictionaries for simulations\n        simulations = {sample_simulation.id: sample_simulation}\n    \n        # Ensure the simulation is schedulable\n        resilience_coordinator.checkpoint_coordinator.last_checkpoint_time[sample_simulation.id] = datetime.now() - timedelta(hours=2)\n    \n        # Process checkpoints\n        processed = resilience_coordinator.process_checkpoints(simulations)\n    \n        # Should schedule checkpoint\n>       assert sample_simulation.id in resilience_coordinator.checkpoint_coordinator.scheduled_checkpoints\nE       AssertionError: assert 'sim_climate_test' in {}\nE        +  where 'sim_climate_test' = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1)).id\nE        +  and   {} = <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f459c09b2c0>.scheduled_checkpoints\nE        +    where <concurrent_task_scheduler.failure_resilience.checkpoint_manager.CheckpointCoordinator object at 0x7f459c09b2c0> = <concurrent_task_scheduler.failure_resilience.resilience_coordinator.ResilienceCoordinator object at 0x7f459c098620>.checkpoint_coordinator\n\ntests/failure_resilience/test_resilience_coordinator.py:591: AssertionError"}, "teardown": {"duration": 0.0004154099151492119, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_simulation", "lineno": 174, "outcome": "passed", "keywords": ["test_submit_simulation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0004602250410243869, "outcome": "passed"}, "call": {"duration": 0.00013123604003340006, "outcome": "passed"}, "teardown": {"duration": 9.369105100631714e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_submit_with_max_concurrent_limit", "lineno": 190, "outcome": "passed", "keywords": ["test_submit_with_max_concurrent_limit", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002270280383527279, "outcome": "passed"}, "call": {"duration": 0.0004119639052078128, "outcome": "passed"}, "teardown": {"duration": 0.00010111997835338116, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_processing_queue", "lineno": 214, "outcome": "passed", "keywords": ["test_processing_queue", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022435502614825964, "outcome": "passed"}, "call": {"duration": 0.0001758710714057088, "outcome": "passed"}, "teardown": {"duration": 9.859399870038033e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_pause_and_resume_simulation", "lineno": 234, "outcome": "passed", "keywords": ["test_pause_and_resume_simulation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00021376495715230703, "outcome": "passed"}, "call": {"duration": 0.00028242403641343117, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Checkpoint manager not available, skipping checkpoint", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 991, "funcName": "_checkpoint_simulation", "created": 1747285139.3632016, "msecs": 363.0, "relativeCreated": 1431.663990020752, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 9.946199133992195e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_simulation_stage_transitions", "lineno": 255, "outcome": "passed", "keywords": ["test_simulation_stage_transitions", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0003173800650984049, "outcome": "passed"}, "call": {"duration": 0.00022074603475630283, "outcome": "passed"}, "teardown": {"duration": 9.396998211741447e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_handle_node_failure", "lineno": 284, "outcome": "passed", "keywords": ["test_handle_node_failure", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00021898606792092323, "outcome": "passed"}, "call": {"duration": 0.0002983369631692767, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Checkpoint manager not available, skipping checkpoint", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 991, "funcName": "_checkpoint_simulation", "created": 1747285139.3651004, "msecs": 365.0, "relativeCreated": 1433.5627555847168, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Simulation sim-test-1 paused due to node node-0 going offline", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 652, "funcName": "_handle_node_status_change", "created": 1747285139.3651588, "msecs": 365.0, "relativeCreated": 1433.6211681365967, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 9.867898188531399e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_checkpoint_creation", "lineno": 300, "outcome": "passed", "keywords": ["test_checkpoint_creation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022591708693653345, "outcome": "passed"}, "call": {"duration": 0.000734908040612936, "outcome": "passed"}, "teardown": {"duration": 0.00010796193964779377, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_maintenance_handling", "lineno": 316, "outcome": "passed", "keywords": ["test_maintenance_handling", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023151992354542017, "outcome": "passed"}, "call": {"duration": 0.00042550498619675636, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Reservation rsv_20250515045859_dcdbdaf0904e15c6 conflicts with maintenance window maint_20250515045858_2ece87ac01c85552", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 368, "funcName": "add_maintenance_window", "created": 1747285139.3674493, "msecs": 367.0, "relativeCreated": 1435.9116554260254, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Reservation for simulation sim-test-1 overlaps with maintenance window maint_20250515045858_2ece87ac01c85552", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 237, "funcName": "reserve_resources", "created": 1747285139.3675306, "msecs": 367.0, "relativeCreated": 1435.992956161499, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}, {"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Checkpoint manager not available, skipping checkpoint", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 991, "funcName": "_checkpoint_simulation", "created": 1747285139.3676014, "msecs": 367.0, "relativeCreated": 1436.0637664794922, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 0.00010564201511442661, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_policies", "lineno": 354, "outcome": "passed", "keywords": ["test_preemption_policies", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023100001271814108, "outcome": "passed"}, "call": {"duration": 0.0004504589596763253, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Checkpoint manager not available, skipping checkpoint", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 991, "funcName": "_checkpoint_simulation", "created": 1747285139.3687322, "msecs": 368.0, "relativeCreated": 1437.194585800171, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 0.00010143499821424484, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_long_term_reservation", "lineno": 410, "outcome": "passed", "keywords": ["test_long_term_reservation", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00030544097535312176, "outcome": "passed"}, "call": {"duration": 0.00024168298114091158, "outcome": "passed"}, "teardown": {"duration": 9.472796227782965e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_node_allocation_optimization", "lineno": 430, "outcome": "passed", "keywords": ["test_node_allocation_optimization", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00020921998657286167, "outcome": "passed"}, "call": {"duration": 0.0003133349819108844, "outcome": "passed"}, "teardown": {"duration": 9.709491860121489e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_dynamic_priority_adjustment", "lineno": 499, "outcome": "passed", "keywords": ["test_dynamic_priority_adjustment", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002205859636887908, "outcome": "passed"}, "call": {"duration": 0.00018625403754413128, "outcome": "passed"}, "teardown": {"duration": 9.190489072352648e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_preemption_protection", "lineno": 530, "outcome": "passed", "keywords": ["test_preemption_protection", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002268699463456869, "outcome": "passed"}, "call": {"duration": 0.00010630697943270206, "outcome": "passed"}, "teardown": {"duration": 8.634699042886496e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestLongRunningJobManager::test_recovery_from_failure", "lineno": 547, "outcome": "passed", "keywords": ["test_recovery_from_failure", "TestLongRunningJobManager", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00030568207148462534, "outcome": "passed"}, "call": {"duration": 0.0009436450200155377, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.job_management.scheduler", "msg": "Simulation sim-test-1 paused due to node node-0 going offline", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/job_management/scheduler.py", "filename": "scheduler.py", "module": "scheduler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 652, "funcName": "_handle_node_status_change", "created": 1747285139.3736567, "msecs": 373.0, "relativeCreated": 1442.1191215515137, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 0.00010958791244775057, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_reserve_resources", "lineno": 581, "outcome": "passed", "keywords": ["test_reserve_resources", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014009000733494759, "outcome": "passed"}, "call": {"duration": 0.0002035590587183833, "outcome": "passed"}, "teardown": {"duration": 9.977002628147602e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_activate_reservation", "lineno": 636, "outcome": "passed", "keywords": ["test_activate_reservation", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.000131865032017231, "outcome": "passed"}, "call": {"duration": 0.00014429900329560041, "outcome": "passed"}, "teardown": {"duration": 8.925201836973429e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_active_reservations", "lineno": 668, "outcome": "passed", "keywords": ["test_active_reservations", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012518896255642176, "outcome": "passed"}, "call": {"duration": 0.00016441999468952417, "outcome": "passed"}, "teardown": {"duration": 9.215192403644323e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_maintenance_windows", "lineno": 712, "outcome": "passed", "keywords": ["test_maintenance_windows", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012057193089276552, "outcome": "passed"}, "call": {"duration": 0.0001791330287232995, "outcome": "passed"}, "teardown": {"duration": 9.028799831867218e-05, "outcome": "passed"}}, {"nodeid": "tests/job_management/test_long_running_job_manager.py::TestJobScheduler::test_preemption_history", "lineno": 767, "outcome": "passed", "keywords": ["test_preemption_history", "TestJobScheduler", "test_long_running_job_manager.py", "job_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001246549654752016, "outcome": "passed"}, "call": {"duration": 0.00015648198314011097, "outcome": "passed"}, "teardown": {"duration": 9.225495159626007e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_resource_data_collector_init", "lineno": 96, "outcome": "passed", "keywords": ["test_resource_data_collector_init", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013972900342196226, "outcome": "passed"}, "call": {"duration": 9.335100185126066e-05, "outcome": "passed"}, "teardown": {"duration": 8.344498928636312e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_record_data_point", "lineno": 110, "outcome": "passed", "keywords": ["test_record_data_point", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012055796105414629, "outcome": "passed"}, "call": {"duration": 0.0004626299487426877, "outcome": "passed"}, "teardown": {"duration": 8.906202856451273e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_simulation_data", "lineno": 148, "outcome": "failed", "keywords": ["test_collect_simulation_data", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017469609156250954, "outcome": "passed"}, "call": {"duration": 0.00046260294038802385, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_data_collector.py", "lineno": 154, "message": "AssertionError: assert {<ResourceTyp...E: 'storage'>} == {<ResourceTyp...E: 'storage'>}\n  \n  Extra items in the right set:\n  <ResourceType.GPU: 'gpu'>\n  Use -v to get more diff"}, "traceback": [{"path": "tests/resource_forecasting/test_data_collector.py", "lineno": 154, "message": "AssertionError"}], "longrepr": "resource_data_collector = <concurrent_task_scheduler.resource_forecasting.data_collector.ResourceDataCollector object at 0x7f459beb9df0>\nsample_simulation = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    def test_collect_simulation_data(resource_data_collector, sample_simulation):\n        \"\"\"Test collecting simulation data.\"\"\"\n        data_points = resource_data_collector.collect_simulation_data(sample_simulation)\n    \n        # Check if data points were returned for all resource types\n>       assert set(data_points.keys()) == set(ResourceType)\nE       AssertionError: assert {<ResourceTyp...E: 'storage'>} == {<ResourceTyp...E: 'storage'>}\nE         \nE         Extra items in the right set:\nE         <ResourceType.GPU: 'gpu'>\nE         Use -v to get more diff\n\ntests/resource_forecasting/test_data_collector.py:154: AssertionError"}, "teardown": {"duration": 0.00013681699056178331, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_node_data", "lineno": 175, "outcome": "failed", "keywords": ["test_collect_node_data", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013300706632435322, "outcome": "passed"}, "call": {"duration": 0.00037540902849286795, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_data_collector.py", "lineno": 193, "message": "AssertionError: assert {<ResourceTyp...E: 'storage'>} == {<ResourceTyp...E: 'storage'>}\n  \n  Extra items in the right set:\n  <ResourceType.GPU: 'gpu'>\n  Use -v to get more diff"}, "traceback": [{"path": "tests/resource_forecasting/test_data_collector.py", "lineno": 193, "message": "AssertionError"}], "longrepr": "resource_data_collector = <concurrent_task_scheduler.resource_forecasting.data_collector.ResourceDataCollector object at 0x7f459bebb260>\n\n    def test_collect_node_data(resource_data_collector):\n        \"\"\"Test collecting node data.\"\"\"\n        node_id = \"node001\"\n        node_metrics = {\n            \"cpu_usage\": 0.8,\n            \"memory_usage\": 0.6,\n            \"disk_usage\": 0.5,\n            \"network_usage\": 0.3,\n            \"cpu_usage_capacity\": 1.0,\n            \"memory_usage_capacity\": 1.0,\n            \"disk_usage_capacity\": 1.0,\n            \"network_usage_capacity\": 1.0,\n        }\n    \n        data_points = resource_data_collector.collect_node_data(node_id, node_metrics)\n    \n        # Check if data points were returned for all resource types\n>       assert set(data_points.keys()) == set(ResourceType)\nE       AssertionError: assert {<ResourceTyp...E: 'storage'>} == {<ResourceTyp...E: 'storage'>}\nE         \nE         Extra items in the right set:\nE         <ResourceType.GPU: 'gpu'>\nE         Use -v to get more diff\n\ntests/resource_forecasting/test_data_collector.py:193: AssertionError"}, "teardown": {"duration": 0.00013140495866537094, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_get_resource_history", "lineno": 212, "outcome": "failed", "keywords": ["test_get_resource_history", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019225804135203362, "outcome": "passed"}, "call": {"duration": 0.02765981899574399, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_data_collector.py", "lineno": 247, "message": "AssertionError: assert 100 == (24 * 7)\n +  where 100 = len([UtilizationDataPoint(timestamp=Timestamp('2025-05-11 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...ceType.CPU: 'cpu'>, utilization=0.6772740661031255, capacity=1.0, simulation_id='sim_climate_test', node_id=None), ...])\n +    where [UtilizationDataPoint(timestamp=Timestamp('2025-05-11 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...ceType.CPU: 'cpu'>, utilization=0.6772740661031255, capacity=1.0, simulation_id='sim_climate_test', node_id=None), ...] = ResourceUtilizationHistory(resource_type=<ResourceType.CPU: 'cpu'>, start_date=datetime.datetime(2025, 5, 8, 4, 58, 59...lization=0.6565685424949238, capacity=1.0, simulation_id='sim_climate_test', node_id=None)], aggregation_period='hour').data_points"}, "traceback": [{"path": "tests/resource_forecasting/test_data_collector.py", "lineno": 247, "message": "AssertionError"}], "longrepr": "resource_data_collector = <concurrent_task_scheduler.resource_forecasting.data_collector.ResourceDataCollector object at 0x7f459bebbd40>\nsample_simulation = MockSimulation(id='sim_climate_test', name='Climate Model Test', description=None, stages={'stage_preprocess': Simulat...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    def test_get_resource_history(resource_data_collector, sample_simulation):\n        \"\"\"Test getting resource history.\"\"\"\n        # Record some data points\n        start_date = datetime.now() - timedelta(days=7)\n        end_date = datetime.now()\n    \n        for i in range(24 * 7):  # 7 days of hourly data\n            timestamp = start_date + timedelta(hours=i)\n            # Simulate a daily pattern\n            hour_factor = 0.5 + 0.4 * np.sin(timestamp.hour / 24.0 * 2 * np.pi)\n    \n            resource_data_collector.record_data_point(\n                resource_type=ResourceType.CPU,\n                utilization=0.5 + (0.2 * hour_factor),\n                capacity=1.0,\n                timestamp=timestamp,\n                simulation_id=sample_simulation.id,\n            )\n    \n        # Get resource history\n        result = resource_data_collector.get_resource_history(\n            resource_type=ResourceType.CPU,\n            start_date=start_date,\n            end_date=end_date,\n            simulation_id=sample_simulation.id,\n        )\n    \n        assert result.success\n        history = result.value\n    \n        # Check history properties\n        assert history.resource_type == ResourceType.CPU\n        assert history.start_date == start_date\n        assert history.end_date == end_date\n>       assert len(history.data_points) == 24 * 7\nE       AssertionError: assert 100 == (24 * 7)\nE        +  where 100 = len([UtilizationDataPoint(timestamp=Timestamp('2025-05-11 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...ceType.CPU: 'cpu'>, utilization=0.6772740661031255, capacity=1.0, simulation_id='sim_climate_test', node_id=None), ...])\nE        +    where [UtilizationDataPoint(timestamp=Timestamp('2025-05-11 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...ceType.CPU: 'cpu'>, utilization=0.6772740661031255, capacity=1.0, simulation_id='sim_climate_test', node_id=None), ...] = ResourceUtilizationHistory(resource_type=<ResourceType.CPU: 'cpu'>, start_date=datetime.datetime(2025, 5, 8, 4, 58, 59...lization=0.6565685424949238, capacity=1.0, simulation_id='sim_climate_test', node_id=None)], aggregation_period='hour').data_points\n\ntests/resource_forecasting/test_data_collector.py:247: AssertionError"}, "teardown": {"duration": 0.0001507609849795699, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_aggregate_data_points", "lineno": 277, "outcome": "failed", "keywords": ["test_aggregate_data_points", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014624092727899551, "outcome": "passed"}, "call": {"duration": 0.005685777054168284, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_data_collector.py", "lineno": 303, "message": "AssertionError: assert 8 == 7\n +  where 8 = len([UtilizationDataPoint(timestamp=Timestamp('2025-05-08 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...pe=<ResourceType.CPU: 'cpu'>, utilization=0.5479166666666667, capacity=1.0, simulation_id='sim001', node_id=None), ...])"}, "traceback": [{"path": "tests/resource_forecasting/test_data_collector.py", "lineno": 303, "message": "AssertionError"}], "longrepr": "resource_data_collector = <concurrent_task_scheduler.resource_forecasting.data_collector.ResourceDataCollector object at 0x7f459bebb740>\n\n    def test_aggregate_data_points(resource_data_collector):\n        \"\"\"Test aggregating data points.\"\"\"\n        # Create some test data points\n        data_points = []\n        start_date = datetime.now() - timedelta(days=7)\n    \n        for i in range(24 * 7):  # 7 days of hourly data\n            timestamp = start_date + timedelta(hours=i)\n            data_points.append(\n                UtilizationDataPoint(\n                    timestamp=timestamp,\n                    resource_type=ResourceType.CPU,\n                    utilization=0.5 + (0.1 * (i % 24) / 24),  # Daily pattern\n                    capacity=1.0,\n                    simulation_id=\"sim001\",\n                )\n            )\n    \n        # Aggregate by day using mean\n        aggregated = resource_data_collector._aggregate_data_points(\n            data_points,\n            AggregationMethod.MEAN,\n            AggregationPeriod.DAY,\n        )\n    \n>       assert len(aggregated) == 7  # 7 days\nE       AssertionError: assert 8 == 7\nE        +  where 8 = len([UtilizationDataPoint(timestamp=Timestamp('2025-05-08 00:00:00'), resource_type=<ResourceType.CPU: 'cpu'>, utilization...pe=<ResourceType.CPU: 'cpu'>, utilization=0.5479166666666667, capacity=1.0, simulation_id='sim001', node_id=None), ...])\n\ntests/resource_forecasting/test_data_collector.py:303: AssertionError"}, "teardown": {"duration": 0.00013217097148299217, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_collect_batch_data", "lineno": 335, "outcome": "passed", "keywords": ["test_collect_batch_data", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00019277201499789953, "outcome": "passed"}, "call": {"duration": 0.0002284629736095667, "outcome": "passed"}, "teardown": {"duration": 9.726302232593298e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_data_collector.py::test_resource_usage_analyzer", "lineno": 371, "outcome": "passed", "keywords": ["test_resource_usage_analyzer", "test_data_collector.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00021250604186207056, "outcome": "passed"}, "call": {"duration": 0.08373832702636719, "outcome": "passed"}, "teardown": {"duration": 0.00013305293396115303, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_forecaster_init", "lineno": 156, "outcome": "passed", "keywords": ["test_forecaster_init", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002069289330393076, "outcome": "passed"}, "call": {"duration": 9.949400555342436e-05, "outcome": "passed"}, "teardown": {"duration": 9.334005881100893e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_extract_time_features", "lineno": 165, "outcome": "failed", "keywords": ["test_extract_time_features", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014245696365833282, "outcome": "passed"}, "call": {"duration": 0.00023059803061187267, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_forecaster.py", "lineno": 187, "message": "assert np.float64(2.0) == 1"}, "traceback": [{"path": "tests/resource_forecasting/test_forecaster.py", "lineno": 187, "message": "AssertionError"}], "longrepr": "resource_forecaster = <concurrent_task_scheduler.resource_forecasting.forecaster.ResourceForecaster object at 0x7f459be71700>\n\n    def test_extract_time_features(resource_forecaster):\n        \"\"\"Test extracting time features from timestamps.\"\"\"\n        # Create some test timestamps\n        timestamps = [\n            datetime(2023, 1, 1, 12, 0),  # Sunday noon\n            datetime(2023, 1, 2, 8, 0),   # Monday morning\n            datetime(2023, 1, 3, 18, 0),  # Tuesday evening\n            datetime(2023, 1, 7, 23, 0),  # Saturday night\n        ]\n    \n        features = resource_forecaster._extract_time_features(timestamps)\n    \n        # Check feature shape and basic properties\n        assert features.shape == (4, 10)  # 4 timestamps, 10 features\n    \n        # Check values for the first timestamp (Sunday noon)\n        assert features[0][0] == 12  # Hour\n        assert features[0][1] == 1   # Day\n        assert features[0][2] == 1   # Month\n        assert features[0][3] == 6   # Day of week (0=Monday, 6=Sunday)\n        assert features[0][8] == 1   # Is weekend (Sunday)\n>       assert features[0][9] == 1   # Time of day (noon = afternoon = 1)\nE       assert np.float64(2.0) == 1\n\ntests/resource_forecasting/test_forecaster.py:187: AssertionError"}, "teardown": {"duration": 0.0001524249091744423, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_train_model", "lineno": 195, "outcome": "failed", "keywords": ["test_train_model", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.02342505392152816, "outcome": "passed"}, "call": {"duration": 0.15549345209728926, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_forecaster.py", "lineno": 231, "message": "AssertionError: assert 'Insufficient data' in 'No data found for simulation nonexistent'\n +  where 'No data found for simulation nonexistent' = Result(success=False, value=None, error='No data found for simulation nonexistent').error"}, "traceback": [{"path": "tests/resource_forecasting/test_forecaster.py", "lineno": 231, "message": "AssertionError"}], "longrepr": "resource_forecaster = <concurrent_task_scheduler.resource_forecasting.forecaster.ResourceForecaster object at 0x7f459be72960>\nsimulation_with_history = MockSimulation(id='sim_forecast_test', name='Forecast Test Simulation', description=None, stages={'stage_preprocess': ...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    def test_train_model(resource_forecaster, simulation_with_history):\n        \"\"\"Test training a forecasting model.\"\"\"\n        # Train a linear regression model\n        result = resource_forecaster.train_model(\n            resource_type=ResourceType.CPU,\n            simulation_id=simulation_with_history.id,\n            method=ForecastingMethod.LINEAR_REGRESSION,\n            training_days=30,\n        )\n    \n        assert result.success\n        model_data = result.value\n    \n        # Check model data structure\n        assert \"model\" in model_data\n        assert \"method\" in model_data\n        assert \"scaler\" in model_data\n        assert \"training_start\" in model_data\n        assert \"training_end\" in model_data\n        assert \"data_points\" in model_data\n        assert \"last_updated\" in model_data\n        assert \"resource_type\" in model_data\n    \n        # Check model was stored correctly\n        assert simulation_with_history.id in resource_forecaster.models\n        assert ResourceType.CPU in resource_forecaster.models[simulation_with_history.id]\n    \n        # Test training with insufficient data\n        result = resource_forecaster.train_model(\n            resource_type=ResourceType.CPU,\n            simulation_id=\"nonexistent\",\n            method=ForecastingMethod.LINEAR_REGRESSION,\n        )\n    \n        assert not result.success\n>       assert \"Insufficient data\" in result.error\nE       AssertionError: assert 'Insufficient data' in 'No data found for simulation nonexistent'\nE        +  where 'No data found for simulation nonexistent' = Result(success=False, value=None, error='No data found for simulation nonexistent').error\n\ntests/resource_forecasting/test_forecaster.py:231: AssertionError"}, "teardown": {"duration": 0.0001563230762258172, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_forecast_resource_usage", "lineno": 250, "outcome": "failed", "keywords": ["test_forecast_resource_usage", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.02332216000650078, "outcome": "passed"}, "call": {"duration": 0.24368707090616226, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/forecaster.py", "lineno": 427, "message": "TypeError: 'LinearRegression' object is not subscriptable"}, "traceback": [{"path": "tests/resource_forecasting/test_forecaster.py", "lineno": 287, "message": ""}, {"path": "concurrent_task_scheduler/resource_forecasting/forecaster.py", "lineno": 427, "message": "TypeError"}], "longrepr": "resource_forecaster = <concurrent_task_scheduler.resource_forecasting.forecaster.ResourceForecaster object at 0x7f4599c966f0>\nsimulation_with_history = MockSimulation(id='sim_forecast_test', name='Forecast Test Simulation', description=None, stages={'stage_preprocess': ...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    def test_forecast_resource_usage(resource_forecaster, simulation_with_history):\n        \"\"\"Test forecasting resource usage.\"\"\"\n        # Forecast CPU usage\n        start_date = datetime.now()\n        end_date = start_date + timedelta(days=14)\n    \n        result = resource_forecaster.forecast_resource_usage(\n            resource_type=ResourceType.CPU,\n            start_date=start_date,\n            end_date=end_date,\n            simulation_id=simulation_with_history.id,\n            method=ForecastingMethod.LINEAR_REGRESSION,\n            period=ForecastPeriod.DAILY,\n        )\n    \n        assert result.success\n        forecast = result.value\n    \n        # Check forecast structure\n        assert forecast.start_date == start_date\n        assert forecast.end_date == end_date\n        assert forecast.period == ForecastPeriod.DAILY\n        assert forecast.resource_type == ResourceType.CPU\n        assert len(forecast.forecasted_values) > 0\n        assert len(forecast.confidence_intervals) > 0\n    \n        # Check forecast values\n        for date_str, value in forecast.forecasted_values.items():\n            assert 0.0 <= value <= 1.0  # Should be a utilization value between 0 and 1\n    \n        # Test forecasting with other methods\n        for method in [\n            ForecastingMethod.MOVING_AVERAGE,\n            ForecastingMethod.EXPONENTIAL_SMOOTHING,\n            ForecastingMethod.ENSEMBLE,\n        ]:\n>           result = resource_forecaster.forecast_resource_usage(\n                resource_type=ResourceType.CPU,\n                start_date=start_date,\n                end_date=start_date + timedelta(days=7),\n                simulation_id=simulation_with_history.id,\n                method=method,\n                period=ForecastPeriod.DAILY,\n            )\n\ntests/resource_forecasting/test_forecaster.py:287: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.resource_forecasting.forecaster.ResourceForecaster object at 0x7f4599c966f0>\nresource_type = <ResourceType.CPU: 'cpu'>\nstart_date = datetime.datetime(2025, 5, 15, 4, 58, 59, 734495)\nend_date = datetime.datetime(2025, 5, 22, 4, 58, 59, 734495)\nsimulation_id = 'sim_forecast_test', node_id = None\nmethod = <ForecastingMethod.MOVING_AVERAGE: 'moving_average'>\nperiod = <ForecastPeriod.DAILY: 'daily'>\n\n    def forecast_resource_usage(\n        self,\n        resource_type: ResourceType,\n        start_date: datetime,\n        end_date: datetime,\n        simulation_id: Optional[str] = None,\n        node_id: Optional[str] = None,\n        method: Optional[ForecastingMethod] = None,\n        period: ForecastPeriod = ForecastPeriod.DAILY,\n    ) -> Result[ResourceForecast]:\n        \"\"\"Forecast resource usage for a specified period.\"\"\"\n        # Determine the key for model lookup\n        key = simulation_id or node_id or \"global\"\n    \n        # Check if we have a trained model, if not, train one\n        if key not in self.models or resource_type not in self.models[key]:\n            train_result = self.train_model(\n                resource_type=resource_type,\n                simulation_id=simulation_id,\n                node_id=node_id,\n                method=method,\n            )\n    \n            if not train_result.success:\n                return Result.err(f\"Failed to train model: {train_result.error}\")\n    \n        model_data = self.models[key][resource_type]\n        model = model_data[\"model\"]\n        method = method or model_data[\"method\"]\n        scaler = model_data[\"scaler\"]\n    \n        # Generate dates for the forecast period\n        forecast_dates = []\n        current_date = start_date\n        date_format = \"%Y-%m-%d\"\n    \n        if period == ForecastPeriod.DAILY:\n            while current_date <= end_date:\n                forecast_dates.append(current_date.strftime(date_format))\n                current_date += timedelta(days=1)\n        elif period == ForecastPeriod.WEEKLY:\n            # Start from the beginning of the week\n            current_date = start_date - timedelta(days=start_date.weekday())\n            while current_date <= end_date:\n                forecast_dates.append(current_date.strftime(date_format))\n                current_date += timedelta(days=7)\n        elif period == ForecastPeriod.MONTHLY:\n            # Start from the beginning of the month\n            current_date = start_date.replace(day=1)\n            while current_date <= end_date:\n                forecast_dates.append(current_date.strftime(date_format))\n                # Move to the next month\n                if current_date.month == 12:\n                    current_date = current_date.replace(year=current_date.year + 1, month=1)\n                else:\n                    current_date = current_date.replace(month=current_date.month + 1)\n        else:\n            # Default to daily\n            while current_date <= end_date:\n                forecast_dates.append(current_date.strftime(date_format))\n                current_date += timedelta(days=1)\n    \n        # Generate hourly timestamps for each forecast date\n        all_timestamps = []\n        for date_str in forecast_dates:\n            date_obj = datetime.strptime(date_str, date_format)\n            for hour in range(24):\n                all_timestamps.append(date_obj.replace(hour=hour))\n    \n        # Extract features for each timestamp\n        features = self._extract_time_features(all_timestamps)\n        scaled_features = scaler.transform(features)\n    \n        # Make predictions based on method\n        all_predictions = []\n        confidence_intervals = {}\n    \n        if method == ForecastingMethod.LINEAR_REGRESSION:\n            # Linear model prediction\n            all_predictions = model.predict(scaled_features)\n    \n            # Simple confidence interval (fixed percentage)\n            std_dev = 0.1  # Placeholder\n            for i, ts in enumerate(all_timestamps):\n                ts_str = ts.isoformat()\n                prediction = all_predictions[i]\n                confidence_intervals[ts_str] = (\n                    max(0, prediction - 1.96 * std_dev),\n                    prediction + 1.96 * std_dev\n                )\n    \n        elif method == ForecastingMethod.RANDOM_FOREST:\n            # Random forest prediction\n            all_predictions = model.predict(scaled_features)\n    \n            # Use model's inherent variance for confidence intervals\n            for i, ts in enumerate(all_timestamps):\n                ts_str = ts.isoformat()\n                prediction = all_predictions[i]\n                # This is a simplification - real implementation would use proper variance\n                confidence_intervals[ts_str] = (\n                    max(0, prediction * 0.8),\n                    prediction * 1.2\n                )\n    \n        elif method == ForecastingMethod.MOVING_AVERAGE:\n            # Get recent history for moving average\n            history_result = self.data_collector.get_resource_history(\n                resource_type=resource_type,\n                start_date=start_date - timedelta(days=14),\n                end_date=start_date,\n                simulation_id=simulation_id,\n                node_id=node_id,\n                aggregation_method=AggregationMethod.MEAN,\n                aggregation_period=AggregationPeriod.HOUR,\n            )\n    \n            if not history_result.success:\n                return Result.err(f\"Failed to get history for moving average: {history_result.error}\")\n    \n            history = history_result.value\n            if not history.data_points:\n                return Result.err(\"Insufficient historical data for moving average\")\n    \n            # Calculate moving average\n>           window_size = model[\"window_size\"]\nE           TypeError: 'LinearRegression' object is not subscriptable\n\nconcurrent_task_scheduler/resource_forecasting/forecaster.py:427: TypeError"}, "teardown": {"duration": 0.00019055604934692383, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_update_forecast_with_actuals", "lineno": 325, "outcome": "passed", "keywords": ["test_update_forecast_with_actuals", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.02271682198625058, "outcome": "passed"}, "call": {"duration": 0.1594267600448802, "outcome": "passed"}, "teardown": {"duration": 0.00013987405691295862, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_create_resource_projection", "lineno": 365, "outcome": "passed", "keywords": ["test_create_resource_projection", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.02188980602659285, "outcome": "passed"}, "call": {"duration": 0.8960866840789095, "outcome": "passed"}, "teardown": {"duration": 0.00018352200277149677, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_forecaster.py::test_detect_anomalies", "lineno": 413, "outcome": "failed", "keywords": ["test_detect_anomalies", "test_forecaster.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.023081029998138547, "outcome": "passed"}, "call": {"duration": 0.03902328503318131, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/resource_forecasting/test_forecaster.py", "lineno": 450, "message": "assert np.True_ is True"}, "traceback": [{"path": "tests/resource_forecasting/test_forecaster.py", "lineno": 450, "message": "AssertionError"}], "longrepr": "resource_forecaster = <concurrent_task_scheduler.resource_forecasting.forecaster.ResourceForecaster object at 0x7f45977c0a70>\nsimulation_with_history = MockSimulation(id='sim_forecast_test', name='Forecast Test Simulation', description=None, stages={'stage_preprocess': ...', tags=[], metadata={}, result_path=None, scientific_promise=0.5, estimated_total_duration=datetime.timedelta(days=1))\n\n    def test_detect_anomalies(resource_forecaster, simulation_with_history):\n        \"\"\"Test detecting anomalies in resource usage.\"\"\"\n        # Add some anomalous data points\n        for i in range(3):\n            timestamp = datetime.now() - timedelta(hours=i)\n            resource_forecaster.data_collector.record_data_point(\n                resource_type=ResourceType.CPU,\n                utilization=0.95,  # Anomalously high\n                capacity=1.0,\n                timestamp=timestamp,\n                simulation_id=simulation_with_history.id,\n            )\n    \n        # Detect anomalies\n        result = resource_forecaster.detect_anomalies(\n            resource_type=ResourceType.CPU,\n            simulation_id=simulation_with_history.id,\n            threshold_stdevs=2.0,  # Lower threshold to detect our test anomalies\n            window_days=7,\n        )\n    \n        assert result.success\n        anomalies = result.value\n    \n        # Should have detected at least one anomaly\n        assert len(anomalies) > 0\n    \n        # Check anomaly structure\n        for anomaly in anomalies:\n            assert \"timestamp\" in anomaly\n            assert \"value\" in anomaly\n            assert \"z_score\" in anomaly\n            assert \"is_high\" in anomaly\n    \n            # Our test anomalies should be high\n            if anomaly[\"value\"] >= 0.95:\n>               assert anomaly[\"is_high\"] is True\nE               assert np.True_ is True\n\ntests/resource_forecasting/test_forecaster.py:450: AssertionError"}, "teardown": {"duration": 0.0001974280457943678, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_optimizer_init", "lineno": 151, "outcome": "passed", "keywords": ["test_optimizer_init", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002546800533309579, "outcome": "passed"}, "call": {"duration": 0.00011364102829247713, "outcome": "passed"}, "teardown": {"duration": 0.00010270101483911276, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_optimize_simulation_resources", "lineno": 168, "outcome": "passed", "keywords": ["test_optimize_simulation_resources", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002425189595669508, "outcome": "passed"}, "call": {"duration": 10.288759499089792, "outcome": "passed"}, "teardown": {"duration": 0.0002954259980469942, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_generate_capacity_plan", "lineno": 232, "outcome": "passed", "keywords": ["test_generate_capacity_plan", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0008741150377318263, "outcome": "passed"}, "call": {"duration": 0.0004982110112905502, "outcome": "passed"}, "teardown": {"duration": 0.00012253993190824986, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_resource_allocation_recommendation", "lineno": 267, "outcome": "passed", "keywords": ["test_resource_allocation_recommendation", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.866703137755394e-05, "outcome": "passed"}, "call": {"duration": 0.00015709490980952978, "outcome": "passed"}, "teardown": {"duration": 8.617003913968801e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_capacity_planning_recommendation", "lineno": 318, "outcome": "passed", "keywords": ["test_capacity_planning_recommendation", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.127593901008368e-05, "outcome": "passed"}, "call": {"duration": 0.00011667003855109215, "outcome": "passed"}, "teardown": {"duration": 7.702701259404421e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_resource_cost", "lineno": 368, "outcome": "passed", "keywords": ["test_resource_cost", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00016591197345405817, "outcome": "passed"}, "call": {"duration": 9.004701860249043e-05, "outcome": "passed"}, "teardown": {"duration": 9.698292706161737e-05, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_mock_allocation_and_capacity", "lineno": 375, "outcome": "failed", "keywords": ["test_mock_allocation_and_capacity", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00015262607485055923, "outcome": "passed"}, "call": {"duration": 0.00014653499238193035, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/_pytest/fixtures.py", "lineno": 1169, "message": "Failed: Fixture \"sample_simulation\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly about how to update your code."}, "longrepr": "Fixture \"sample_simulation\" called directly. Fixtures are not meant to be called directly,\nbut are created automatically when test functions request them as parameters.\nSee https://docs.pytest.org/en/stable/explanation/fixtures.html for more information about fixtures, and\nhttps://docs.pytest.org/en/stable/deprecations.html#calling-fixtures-directly about how to update your code."}, "teardown": {"duration": 0.0001291710650548339, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_optimizer.py::test_justification_generation", "lineno": 402, "outcome": "passed", "keywords": ["test_justification_generation", "test_optimizer.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017958390526473522, "outcome": "passed"}, "call": {"duration": 0.0007973570609465241, "outcome": "passed"}, "teardown": {"duration": 0.00012352201156318188, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_reporter_init", "lineno": 145, "outcome": "passed", "keywords": ["test_reporter_init", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.000235480023548007, "outcome": "passed"}, "call": {"duration": 9.323703125119209e-05, "outcome": "passed"}, "teardown": {"duration": 0.00011152902152389288, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_utilization_report", "lineno": 154, "outcome": "passed", "keywords": ["test_generate_utilization_report", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0027513439999893308, "outcome": "passed"}, "call": {"duration": 0.27419467794243246, "outcome": "passed"}, "teardown": {"duration": 0.00018582202028483152, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_forecast_report", "lineno": 252, "outcome": "passed", "keywords": ["test_generate_forecast_report", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.002603003988042474, "outcome": "passed"}, "call": {"duration": 0.15504916291683912, "outcome": "passed"}, "teardown": {"duration": 0.00017104798462241888, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_grant_report", "lineno": 332, "outcome": "passed", "keywords": ["test_generate_grant_report", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0025661169784143567, "outcome": "passed"}, "call": {"duration": 0.4231800999259576, "outcome": "passed"}, "teardown": {"duration": 0.00016935192979872227, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_generate_recommendation_report", "lineno": 422, "outcome": "passed", "keywords": ["test_generate_recommendation_report", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0025435539428144693, "outcome": "passed"}, "call": {"duration": 2.2369681659620255, "outcome": "passed"}, "teardown": {"duration": 0.0003219230566173792, "outcome": "passed"}}, {"nodeid": "tests/resource_forecasting/test_reporter.py::test_get_report", "lineno": 483, "outcome": "passed", "keywords": ["test_get_report", "test_reporter.py", "resource_forecasting", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0028501879423856735, "outcome": "passed"}, "call": {"duration": 0.05204537103418261, "outcome": "passed"}, "teardown": {"duration": 0.00016685202717781067, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_empty_resource_allocation", "lineno": 302, "outcome": "passed", "keywords": ["test_empty_resource_allocation", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.000625013024546206, "outcome": "passed"}, "call": {"duration": 0.0008413090836256742, "outcome": "passed"}, "teardown": {"duration": 0.00011065497528761625, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_very_high_priority_scenario", "lineno": 371, "outcome": "passed", "keywords": ["test_very_high_priority_scenario", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002394040348008275, "outcome": "passed"}, "call": {"duration": 0.0007143790135160089, "outcome": "passed"}, "teardown": {"duration": 9.97820170596242e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_unusual_resource_types", "lineno": 470, "outcome": "passed", "keywords": ["test_unusual_resource_types", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023024494294077158, "outcome": "passed"}, "call": {"duration": 0.0005426889983937144, "outcome": "passed"}, "teardown": {"duration": 9.723496623337269e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestEdgeCasePriorityManagement::test_paused_scenario_handling", "lineno": 545, "outcome": "passed", "keywords": ["test_paused_scenario_handling", "TestEdgeCasePriorityManagement", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002057900419458747, "outcome": "passed"}, "call": {"duration": 0.0006974060088396072, "outcome": "passed"}, "teardown": {"duration": 0.00010626495350152254, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.BALANCED]", "parametrize", "pytestmark", "ResourceReallocationStrategy.BALANCED", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00026239396538585424, "outcome": "passed"}, "call": {"duration": 0.0002408740110695362, "outcome": "passed"}, "teardown": {"duration": 9.687698911875486e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.PROPORTIONAL]", "parametrize", "pytestmark", "ResourceReallocationStrategy.PROPORTIONAL", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022401707246899605, "outcome": "passed"}, "call": {"duration": 0.00015047704800963402, "outcome": "passed"}, "teardown": {"duration": 0.00010078900959342718, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.THRESHOLD_BASED]", "parametrize", "pytestmark", "ResourceReallocationStrategy.THRESHOLD_BASED", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023222004529088736, "outcome": "passed"}, "call": {"duration": 0.0001447990071028471, "outcome": "passed"}, "teardown": {"duration": 9.265600237995386e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.GRADUAL]", "parametrize", "pytestmark", "ResourceReallocationStrategy.GRADUAL", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022734608501195908, "outcome": "passed"}, "call": {"duration": 0.00013644399587064981, "outcome": "passed"}, "teardown": {"duration": 9.139999747276306e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationStrategies::test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "lineno": 624, "outcome": "passed", "keywords": ["test_reallocation_strategy[ResourceReallocationStrategy.AGGRESSIVE]", "parametrize", "pytestmark", "ResourceReallocationStrategy.AGGRESSIVE", "TestResourceAllocationStrategies", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00021295202895998955, "outcome": "passed"}, "call": {"duration": 0.00014224299229681492, "outcome": "passed"}, "teardown": {"duration": 9.81400953605771e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_sudden_priority_change", "lineno": 722, "outcome": "passed", "keywords": ["test_sudden_priority_change", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002078759716823697, "outcome": "passed"}, "call": {"duration": 0.0007308439817279577, "outcome": "passed"}, "teardown": {"duration": 0.00010588299483060837, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_queue_for_recalculation", "lineno": 899, "outcome": "passed", "keywords": ["test_queue_for_recalculation", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022645806893706322, "outcome": "passed"}, "call": {"duration": 0.0005648849764838815, "outcome": "passed"}, "teardown": {"duration": 0.00010374607518315315, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestDynamicPriorityEvents::test_recompute_all_priorities", "lineno": 971, "outcome": "passed", "keywords": ["test_recompute_all_priorities", "TestDynamicPriorityEvents", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002236079890280962, "outcome": "passed"}, "call": {"duration": 0.0014099000254645944, "outcome": "passed"}, "teardown": {"duration": 0.00011839705985039473, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestPriorityTrendAnalysis::test_priority_trend_tracking", "lineno": 1117, "outcome": "passed", "keywords": ["test_priority_trend_tracking", "TestPriorityTrendAnalysis", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002250300021842122, "outcome": "passed"}, "call": {"duration": 0.0001544749829918146, "outcome": "passed"}, "teardown": {"duration": 9.295693598687649e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestResourceAllocationHistory::test_resource_allocation_tracking", "lineno": 1203, "outcome": "passed", "keywords": ["test_resource_allocation_tracking", "TestResourceAllocationHistory", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00020369701087474823, "outcome": "passed"}, "call": {"duration": 0.0013591439928859472, "outcome": "passed"}, "teardown": {"duration": 0.00010737893171608448, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_invalid_priority_override", "lineno": 1333, "outcome": "passed", "keywords": ["test_invalid_priority_override", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022202893160283566, "outcome": "passed"}, "call": {"duration": 0.00014380004722625017, "outcome": "passed"}, "teardown": {"duration": 9.00080194696784e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_empty_scenario_list", "lineno": 1388, "outcome": "passed", "keywords": ["test_empty_scenario_list", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.294498547911644e-05, "outcome": "passed"}, "call": {"duration": 0.0003081930335611105, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios for resource reallocation", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 408, "funcName": "reallocate_resources", "created": 1747285154.6446588, "msecs": 644.0, "relativeCreated": 16713.12117576599, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}, {"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios to compare, skipping adjustment", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 329, "funcName": "compare_and_adjust_priorities", "created": 1747285154.6447961, "msecs": 644.0, "relativeCreated": 16713.258504867554, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 8.130399510264397e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_advanced_priority_management.py::TestErrorHandling::test_single_scenario", "lineno": 1399, "outcome": "passed", "keywords": ["test_single_scenario", "TestErrorHandling", "test_advanced_priority_management.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00023404194507747889, "outcome": "passed"}, "call": {"duration": 0.00022142904344946146, "outcome": "passed", "log": [{"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios to compare, skipping adjustment", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 329, "funcName": "compare_and_adjust_priorities", "created": 1747285154.645577, "msecs": 645.0, "relativeCreated": 16714.03932571411, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}, {"name": "concurrent_task_scheduler.scenario_management.priority_manager", "msg": "Need at least 2 scenarios for resource reallocation", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "filename": "priority_manager.py", "module": "priority_manager", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 408, "funcName": "reallocate_resources", "created": 1747285154.6456435, "msecs": 645.0, "relativeCreated": 16714.10584449768, "thread": 139942540846912, "threadName": "MainThread", "processName": "MainProcess", "process": 1510032, "taskName": null}]}, "teardown": {"duration": 9.821902494877577e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_initialization", "lineno": 211, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0007603520061820745, "outcome": "passed"}, "call": {"duration": 0.00025026604998856783, "outcome": "passed"}, "teardown": {"duration": 0.00010009901598095894, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_direct", "lineno": 226, "outcome": "failed", "keywords": ["test_compare_scenarios_direct", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0007898788899183273, "outcome": "passed"}, "call": {"duration": 0.00046012597158551216, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_comparator.py", "lineno": 251, "message": "AssertionError: assert -0.011538461538461558 > 0\n +  where -0.011538461538461558 = ComparisonResult(scenario_id_a='scenario-a', scenario_id_b='scenario-b', metrics_comparison={'direct': {'efficiency': ..., 5, 15, 4, 59, 14, 648817), recommendation='Both scenarios show similar promise and should be continued in parallel.').overall_difference"}, "traceback": [{"path": "tests/scenario_management/test_comparator.py", "lineno": 251, "message": "AssertionError"}], "longrepr": "self = <tests.scenario_management.test_comparator.TestScenarioComparator object at 0x7f459c1729c0>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-performing scenario', simulations={}, status=<Scenar... 14, 647815), resource_allocation={}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\nmock_evaluator = <MagicMock spec='ScenarioEvaluator' id='139936873314016'>\n\n    def test_compare_scenarios_direct(self, sample_scenarios, mock_evaluator):\n        \"\"\"Test direct comparison between two scenarios.\"\"\"\n        comparator = ScenarioComparator(evaluator=mock_evaluator)\n    \n        # Compare scenario A and B using direct method\n        result = comparator.compare_scenarios(\n            sample_scenarios[0],  # scenario A\n            sample_scenarios[1],  # scenario B\n            method=ComparisonMethod.DIRECT\n        )\n    \n        # Check result structure and success\n        assert result.success\n        comparison = result.value\n    \n        # Check comparison result attributes\n        assert comparison.scenario_id_a == \"scenario-a\"\n        assert comparison.scenario_id_b == \"scenario-b\"\n        assert isinstance(comparison.metrics_comparison, dict)\n        assert isinstance(comparison.overall_difference, float)\n        assert isinstance(comparison.confidence, float)\n        assert isinstance(comparison.recommendation, str)\n    \n        # Direct comparison should show A as better (positive difference)\n>       assert comparison.overall_difference > 0\nE       AssertionError: assert -0.011538461538461558 > 0\nE        +  where -0.011538461538461558 = ComparisonResult(scenario_id_a='scenario-a', scenario_id_b='scenario-b', metrics_comparison={'direct': {'efficiency': ..., 5, 15, 4, 59, 14, 648817), recommendation='Both scenarios show similar promise and should be continued in parallel.').overall_difference\n\ntests/scenario_management/test_comparator.py:251: AssertionError"}, "teardown": {"duration": 0.00014197197742760181, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_weighted", "lineno": 259, "outcome": "passed", "keywords": ["test_compare_scenarios_weighted", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0006784580182284117, "outcome": "passed"}, "call": {"duration": 0.0002337039913982153, "outcome": "passed"}, "teardown": {"duration": 0.00011284300126135349, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_relative", "lineno": 281, "outcome": "passed", "keywords": ["test_compare_scenarios_relative", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0006623519584536552, "outcome": "passed"}, "call": {"duration": 0.00026524404529482126, "outcome": "passed"}, "teardown": {"duration": 0.00010108202695846558, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_scenarios_rank_based", "lineno": 315, "outcome": "passed", "keywords": ["test_compare_scenarios_rank_based", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0007912940345704556, "outcome": "passed"}, "call": {"duration": 0.0002453499473631382, "outcome": "passed"}, "teardown": {"duration": 0.00010319706052541733, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_get_comparison_history", "lineno": 338, "outcome": "passed", "keywords": ["test_get_comparison_history", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0006324190180748701, "outcome": "passed"}, "call": {"duration": 0.00026402808725833893, "outcome": "passed"}, "teardown": {"duration": 0.00010293594095855951, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_compare_multiple_scenarios", "lineno": 374, "outcome": "passed", "keywords": ["test_compare_multiple_scenarios", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0006661319639533758, "outcome": "passed"}, "call": {"duration": 0.0003567860694602132, "outcome": "passed"}, "teardown": {"duration": 0.00010873295832425356, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_group_scenarios_by_similarity", "lineno": 392, "outcome": "passed", "keywords": ["test_group_scenarios_by_similarity", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017280899919569492, "outcome": "passed"}, "call": {"duration": 0.0003187800757586956, "outcome": "passed"}, "teardown": {"duration": 9.461201261729002e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_comparator.py::TestScenarioComparator::test_identify_complementary_scenarios", "lineno": 431, "outcome": "passed", "keywords": ["test_identify_complementary_scenarios", "TestScenarioComparator", "test_comparator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0006631480064243078, "outcome": "passed"}, "call": {"duration": 0.00027448299806565046, "outcome": "passed"}, "teardown": {"duration": 0.00011543603613972664, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_initialization", "lineno": 84, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011110096238553524, "outcome": "passed"}, "call": {"duration": 0.0001282639568671584, "outcome": "passed"}, "teardown": {"duration": 8.238095324486494e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_scenario", "lineno": 101, "outcome": "passed", "keywords": ["test_evaluate_scenario", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017871696036309004, "outcome": "passed"}, "call": {"duration": 0.00021510396618396044, "outcome": "passed"}, "teardown": {"duration": 8.891196921467781e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_with_custom_weights", "lineno": 125, "outcome": "passed", "keywords": ["test_evaluation_with_custom_weights", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001530810259282589, "outcome": "passed"}, "call": {"duration": 0.0002567049814388156, "outcome": "passed"}, "teardown": {"duration": 9.819702245295048e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluate_empty_scenario", "lineno": 157, "outcome": "passed", "keywords": ["test_evaluate_empty_scenario", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.202402386814356e-05, "outcome": "passed"}, "call": {"duration": 0.00015529000665992498, "outcome": "passed"}, "teardown": {"duration": 9.057403076440096e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_manual_rating_integration", "lineno": 179, "outcome": "passed", "keywords": ["test_manual_rating_integration", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001491570146754384, "outcome": "passed"}, "call": {"duration": 0.00020326394587755203, "outcome": "passed"}, "teardown": {"duration": 8.988997433334589e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_history_tracking", "lineno": 206, "outcome": "passed", "keywords": ["test_evaluation_history_tracking", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00015357707161456347, "outcome": "passed"}, "call": {"duration": 0.000223626964725554, "outcome": "passed"}, "teardown": {"duration": 0.00010645994916558266, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_compare_evaluations", "lineno": 226, "outcome": "passed", "keywords": ["test_compare_evaluations", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014820194337517023, "outcome": "passed"}, "call": {"duration": 0.0002069959882646799, "outcome": "passed"}, "teardown": {"duration": 8.874794002622366e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_evaluator.py::TestScenarioEvaluator::test_evaluation_criteria_functions", "lineno": 255, "outcome": "passed", "keywords": ["test_evaluation_criteria_functions", "__wrapped__", "patchings", "TestScenarioEvaluator", "test_evaluator.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014229502994567156, "outcome": "passed"}, "call": {"duration": 0.0007605300052091479, "outcome": "passed"}, "teardown": {"duration": 0.00011055206414312124, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_initialization", "lineno": 230, "outcome": "passed", "keywords": ["test_initialization", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0009605790255591273, "outcome": "passed"}, "call": {"duration": 0.0003651450388133526, "outcome": "passed"}, "teardown": {"duration": 0.0001105449628084898, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "lineno": 255, "outcome": "passed", "keywords": ["test_needs_evaluation", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017180899158120155, "outcome": "passed"}, "call": {"duration": 0.0001565400743857026, "outcome": "passed"}, "teardown": {"duration": 9.376893285661936e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_evaluate_scenario_priority", "lineno": 283, "outcome": "failed", "keywords": ["test_evaluate_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0007276309188455343, "outcome": "passed"}, "call": {"duration": 0.0012913569808006287, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError: max() iterable argument is empty"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 290, "message": ""}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 199, "message": "in evaluate_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a34a0>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 20.0, 'storage': 50.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\nmock_evaluator = <MagicMock spec='ScenarioEvaluator' id='139936945768064'>\n\n    def test_evaluate_scenario_priority(self, sample_scenarios, mock_evaluator):\n        \"\"\"Test evaluation of scenario priority.\"\"\"\n        manager = PriorityManager(evaluator=mock_evaluator)\n    \n        # Test priority evaluation for each scenario\n        for scenario in sample_scenarios:\n>           suggested_priority, reason, details = manager.evaluate_scenario_priority(scenario)\n\ntests/scenario_management/test_priority_manager.py:290: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/scenario_management/priority_manager.py:199: in evaluate_scenario_priority\n    change_reason = self._determine_priority_change_reason(scenario, eval_result)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.scenario_management.priority_manager.PriorityManager object at 0x7f45979537d0>\nscenario = Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<ScenarioS...mpute_nodes': 50.0, 'storage': 200.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\neval_result = <MagicMock name='mock.value' id='139936872602016'>\n\n    def _determine_priority_change_reason(\n        self, scenario: Scenario, eval_result: ScenarioEvaluationResult\n    ) -> PriorityChangeReason:\n        \"\"\"\n        Determine the primary reason for a priority change based on evaluation.\n    \n        Args:\n            scenario: The scenario being evaluated\n            eval_result: The evaluation result\n    \n        Returns:\n            The primary reason for the priority change\n        \"\"\"\n        # Find the criterion with the highest impact on the evaluation\n        if eval_result.metric_scores:\n>           max_criterion = max(eval_result.metric_scores.items(), key=lambda x: x[1])\nE           ValueError: max() iterable argument is empty\n\nconcurrent_task_scheduler/scenario_management/priority_manager.py:227: ValueError"}, "teardown": {"duration": 0.00013984704855829477, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "lineno": 304, "outcome": "failed", "keywords": ["test_update_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.000598446000367403, "outcome": "passed"}, "call": {"duration": 0.000828759977594018, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError: max() iterable argument is empty"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 318, "message": ""}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 267, "message": "in update_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 199, "message": "in evaluate_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a3650>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 20.0, 'storage': 50.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\nmock_evaluator = <MagicMock spec='ScenarioEvaluator' id='139936947342704'>\n\n    def test_update_scenario_priority(self, sample_scenarios, mock_evaluator):\n        \"\"\"Test updating a scenario's priority.\"\"\"\n        manager = PriorityManager(\n            evaluator=mock_evaluator,\n            min_priority_change_threshold=0.05,\n        )\n    \n        # Update each scenario's priority\n        for scenario in sample_scenarios:\n            # Store original priority\n            original_priority = scenario.priority_score\n    \n            # Update priority\n>           change_record = manager.update_scenario_priority(scenario)\n\ntests/scenario_management/test_priority_manager.py:318: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/scenario_management/priority_manager.py:267: in update_scenario_priority\n    new_priority, change_reason, details = self.evaluate_scenario_priority(scenario)\nconcurrent_task_scheduler/scenario_management/priority_manager.py:199: in evaluate_scenario_priority\n    change_reason = self._determine_priority_change_reason(scenario, eval_result)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.scenario_management.priority_manager.PriorityManager object at 0x7f459c0998e0>\nscenario = Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<ScenarioS...mpute_nodes': 50.0, 'storage': 200.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\neval_result = <MagicMock name='mock.value' id='139936869018384'>\n\n    def _determine_priority_change_reason(\n        self, scenario: Scenario, eval_result: ScenarioEvaluationResult\n    ) -> PriorityChangeReason:\n        \"\"\"\n        Determine the primary reason for a priority change based on evaluation.\n    \n        Args:\n            scenario: The scenario being evaluated\n            eval_result: The evaluation result\n    \n        Returns:\n            The primary reason for the priority change\n        \"\"\"\n        # Find the criterion with the highest impact on the evaluation\n        if eval_result.metric_scores:\n>           max_criterion = max(eval_result.metric_scores.items(), key=lambda x: x[1])\nE           ValueError: max() iterable argument is empty\n\nconcurrent_task_scheduler/scenario_management/priority_manager.py:227: ValueError"}, "teardown": {"duration": 0.00013825995847582817, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "lineno": 351, "outcome": "failed", "keywords": ["test_compare_and_adjust_priorities", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0010108610149472952, "outcome": "passed"}, "call": {"duration": 0.0008365949615836143, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError: max() iterable argument is empty"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 364, "message": ""}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 335, "message": "in compare_and_adjust_priorities"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 267, "message": "in update_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 199, "message": "in evaluate_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a3830>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 20.0, 'storage': 50.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\nmock_evaluator = <MagicMock spec='ScenarioEvaluator' id='139936870971824'>\nmock_comparator = <MagicMock spec='ScenarioComparator' id='139936870964432'>\n\n    def test_compare_and_adjust_priorities(self, sample_scenarios, mock_evaluator, mock_comparator):\n        \"\"\"Test comparing and adjusting priorities of multiple scenarios.\"\"\"\n        manager = PriorityManager(\n            evaluator=mock_evaluator,\n            comparator=mock_comparator,\n            min_priority_change_threshold=0.05,\n        )\n    \n        # Store original priorities\n        original_priorities = {s.id: s.priority_score for s in sample_scenarios}\n    \n        # Compare and adjust priorities\n>       changes = manager.compare_and_adjust_priorities(\n            sample_scenarios,\n            comparison_method=ComparisonMethod.WEIGHTED\n        )\n\ntests/scenario_management/test_priority_manager.py:364: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/scenario_management/priority_manager.py:335: in compare_and_adjust_priorities\n    self.update_scenario_priority(scenario)\nconcurrent_task_scheduler/scenario_management/priority_manager.py:267: in update_scenario_priority\n    new_priority, change_reason, details = self.evaluate_scenario_priority(scenario)\nconcurrent_task_scheduler/scenario_management/priority_manager.py:199: in evaluate_scenario_priority\n    change_reason = self._determine_priority_change_reason(scenario, eval_result)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.scenario_management.priority_manager.PriorityManager object at 0x7f4597524fe0>\nscenario = Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<ScenarioS...mpute_nodes': 50.0, 'storage': 200.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\neval_result = <MagicMock name='mock.value' id='139936868226624'>\n\n    def _determine_priority_change_reason(\n        self, scenario: Scenario, eval_result: ScenarioEvaluationResult\n    ) -> PriorityChangeReason:\n        \"\"\"\n        Determine the primary reason for a priority change based on evaluation.\n    \n        Args:\n            scenario: The scenario being evaluated\n            eval_result: The evaluation result\n    \n        Returns:\n            The primary reason for the priority change\n        \"\"\"\n        # Find the criterion with the highest impact on the evaluation\n        if eval_result.metric_scores:\n>           max_criterion = max(eval_result.metric_scores.items(), key=lambda x: x[1])\nE           ValueError: max() iterable argument is empty\n\nconcurrent_task_scheduler/scenario_management/priority_manager.py:227: ValueError"}, "teardown": {"duration": 0.00015084201004356146, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "lineno": 387, "outcome": "failed", "keywords": ["test_reallocate_resources", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017572299111634493, "outcome": "passed"}, "call": {"duration": 0.0002906479639932513, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_priority_manager.py", "lineno": 421, "message": "assert 2.0 < 0.001\n +  where 2.0 = abs((100.0 - 98.0))"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 421, "message": "AssertionError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a3a10>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 16.0, 'storage': 40.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\n\n    def test_reallocate_resources(self, sample_scenarios):\n        \"\"\"Test resource reallocation between scenarios.\"\"\"\n        # Test different strategies\n        for strategy in ResourceReallocationStrategy:\n            manager = PriorityManager(\n                reallocation_strategy=strategy,\n                max_reallocation_per_adjustment=0.2,\n            )\n    \n            # Store original allocations\n            original_allocations = {\n                s.id: s.resource_allocation.copy() for s in sample_scenarios\n            }\n    \n            # Perform resource reallocation\n            allocation_changes = manager.reallocate_resources(sample_scenarios)\n    \n            # Check resource conservation (total should remain approximately the same)\n            original_total_compute = sum(\n                alloc.get(\"compute_nodes\", 0) for alloc in original_allocations.values()\n            )\n            original_total_storage = sum(\n                alloc.get(\"storage\", 0) for alloc in original_allocations.values()\n            )\n    \n            new_total_compute = sum(\n                s.resource_allocation.get(\"compute_nodes\", 0) for s in sample_scenarios\n            )\n            new_total_storage = sum(\n                s.resource_allocation.get(\"storage\", 0) for s in sample_scenarios\n            )\n    \n            # Allow for small floating-point differences\n>           assert abs(original_total_compute - new_total_compute) < 0.001\nE           assert 2.0 < 0.001\nE            +  where 2.0 = abs((100.0 - 98.0))\n\ntests/scenario_management/test_priority_manager.py:421: AssertionError"}, "teardown": {"duration": 0.0001329019432887435, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "lineno": 448, "outcome": "passed", "keywords": ["test_manual_priority_override", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001742680324241519, "outcome": "passed"}, "call": {"duration": 0.00016110599972307682, "outcome": "passed"}, "teardown": {"duration": 9.098299778997898e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "lineno": 485, "outcome": "failed", "keywords": ["test_recompute_all_priorities", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.001011864049360156, "outcome": "passed"}, "call": {"duration": 0.0008372439770027995, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError: max() iterable argument is empty"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 497, "message": ""}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 625, "message": "in recompute_all_priorities"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 267, "message": "in update_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 199, "message": "in evaluate_scenario_priority"}, {"path": "concurrent_task_scheduler/scenario_management/priority_manager.py", "lineno": 227, "message": "ValueError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a3dd0>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 20.0, 'storage': 50.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\nmock_evaluator = <MagicMock spec='ScenarioEvaluator' id='139936945773872'>\nmock_comparator = <MagicMock spec='ScenarioComparator' id='139936945779296'>\n\n    def test_recompute_all_priorities(self, sample_scenarios, mock_evaluator, mock_comparator):\n        \"\"\"Test recomputing priorities for all scenarios.\"\"\"\n        manager = PriorityManager(\n            evaluator=mock_evaluator,\n            comparator=mock_comparator,\n        )\n    \n        # Store original priorities\n        original_priorities = {s.id: s.priority_score for s in sample_scenarios}\n    \n        # Recompute all priorities\n>       changes = manager.recompute_all_priorities(sample_scenarios)\n\ntests/scenario_management/test_priority_manager.py:497: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconcurrent_task_scheduler/scenario_management/priority_manager.py:625: in recompute_all_priorities\n    change = self.update_scenario_priority(scenario, force=True)\nconcurrent_task_scheduler/scenario_management/priority_manager.py:267: in update_scenario_priority\n    new_priority, change_reason, details = self.evaluate_scenario_priority(scenario)\nconcurrent_task_scheduler/scenario_management/priority_manager.py:199: in evaluate_scenario_priority\n    change_reason = self._determine_priority_change_reason(scenario, eval_result)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <concurrent_task_scheduler.scenario_management.priority_manager.PriorityManager object at 0x7f459bf1b2c0>\nscenario = Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<ScenarioS...mpute_nodes': 50.0, 'storage': 200.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\neval_result = <MagicMock name='mock.value' id='139936945498128'>\n\n    def _determine_priority_change_reason(\n        self, scenario: Scenario, eval_result: ScenarioEvaluationResult\n    ) -> PriorityChangeReason:\n        \"\"\"\n        Determine the primary reason for a priority change based on evaluation.\n    \n        Args:\n            scenario: The scenario being evaluated\n            eval_result: The evaluation result\n    \n        Returns:\n            The primary reason for the priority change\n        \"\"\"\n        # Find the criterion with the highest impact on the evaluation\n        if eval_result.metric_scores:\n>           max_criterion = max(eval_result.metric_scores.items(), key=lambda x: x[1])\nE           ValueError: max() iterable argument is empty\n\nconcurrent_task_scheduler/scenario_management/priority_manager.py:227: ValueError"}, "teardown": {"duration": 0.00014294206630438566, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "lineno": 508, "outcome": "passed", "keywords": ["test_get_priority_changes", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00017253297846764326, "outcome": "passed"}, "call": {"duration": 0.00015025190077722073, "outcome": "passed"}, "teardown": {"duration": 9.28359804674983e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "lineno": 549, "outcome": "passed", "keywords": ["test_get_priority_trend", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00015369500033557415, "outcome": "passed"}, "call": {"duration": 0.00015158997848629951, "outcome": "passed"}, "teardown": {"duration": 8.7916967459023e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "lineno": 585, "outcome": "failed", "keywords": ["test_get_resource_allocation_history", "TestPriorityManager", "test_priority_manager.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001584629062563181, "outcome": "passed"}, "call": {"duration": 0.00013155699707567692, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_priority_manager.py", "lineno": 594, "message": "TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given"}, "traceback": [{"path": "tests/scenario_management/test_priority_manager.py", "lineno": 594, "message": "TypeError"}], "longrepr": "self = <tests.scenario_management.test_priority_manager.TestPriorityManager object at 0x7f459c1a34d0>\nsample_scenarios = [Scenario(id='scenario-a', name='Scenario A', description='A high-priority scenario', simulations={}, status=<Scenario...mpute_nodes': 20.0, 'storage': 50.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})]\n\n    def test_get_resource_allocation_history(self, sample_scenarios):\n        \"\"\"Test retrieving resource allocation history.\"\"\"\n        manager = PriorityManager()\n        scenario = sample_scenarios[0]\n    \n        # Create some resource allocations\n        for i in range(3):\n            allocation = {\"compute_nodes\": 50 + i*10, \"storage\": 200 + i*50}\n>           manager.resource_allocation_history[scenario.id].append(ResourceAllocation(allocation))\nE           TypeError: BaseModel.__init__() takes 1 positional argument but 2 were given\n\ntests/scenario_management/test_priority_manager.py:594: TypeError"}, "teardown": {"duration": 0.00013786903582513332, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_workflow", "lineno": 226, "outcome": "error", "keywords": ["test_end_to_end_workflow", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002588910283520818, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "ValidationError"}], "longrepr": "@pytest.fixture\n    def sample_scenarios():\n        \"\"\"Create sample scenarios for integration testing.\"\"\"\n        # Create three scenarios with different characteristics\n        scenarios = []\n    \n        # Scenario 1: High accuracy, good convergence, moderate efficiency\n        scenario1 = Scenario(\n            id=\"high-accuracy-scenario\",\n            name=\"High Accuracy Climate Model\",\n            description=\"A climate model focused on accuracy\",\n            status=ScenarioStatus.ACTIVE,\n            priority_score=0.7,\n            tags=[\"climate\", \"high-resolution\", \"accuracy-focused\"],\n            resource_allocation={\"compute_nodes\": 100, \"storage\": 500, \"memory\": 200},\n        )\n    \n        # Add scientific metrics\n        scenario1.scientific_metrics = {\n            \"accuracy_global_temp\": ScientificMetric(\n                name=\"accuracy_global_temp\",\n                description=\"Accuracy of global temperature prediction\",\n                value=0.92,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=1.2,\n            ),\n            \"convergence_rate\": ScientificMetric(\n                name=\"convergence_rate\",\n                description=\"Convergence rate of simulation\",\n                value=0.85,\n                unit=\"iterations^-1\",\n                is_higher_better=True,\n                weight=1.0,\n            ),\n            \"computational_efficiency\": ScientificMetric(\n                name=\"computational_efficiency\",\n                description=\"Computational efficiency\",\n                value=0.65,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=0.8,\n            ),\n        }\n    \n        # Add simulations\n>       sim1 = Simulation(\n            id=\"sim-accuracy-1\",\n            name=\"Atmospheric Model\",\n            description=\"High-resolution atmospheric simulation\",\n            estimated_duration=timedelta(days=14),\n            priority=SimulationPriority.HIGH,\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_management_integration.py:89: ValidationError"}, "teardown": {"duration": 0.000137686962261796, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_scenario_metric_updates_propagation", "lineno": 327, "outcome": "error", "keywords": ["test_scenario_metric_updates_propagation", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0002349410206079483, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "ValidationError"}], "longrepr": "@pytest.fixture\n    def sample_scenarios():\n        \"\"\"Create sample scenarios for integration testing.\"\"\"\n        # Create three scenarios with different characteristics\n        scenarios = []\n    \n        # Scenario 1: High accuracy, good convergence, moderate efficiency\n        scenario1 = Scenario(\n            id=\"high-accuracy-scenario\",\n            name=\"High Accuracy Climate Model\",\n            description=\"A climate model focused on accuracy\",\n            status=ScenarioStatus.ACTIVE,\n            priority_score=0.7,\n            tags=[\"climate\", \"high-resolution\", \"accuracy-focused\"],\n            resource_allocation={\"compute_nodes\": 100, \"storage\": 500, \"memory\": 200},\n        )\n    \n        # Add scientific metrics\n        scenario1.scientific_metrics = {\n            \"accuracy_global_temp\": ScientificMetric(\n                name=\"accuracy_global_temp\",\n                description=\"Accuracy of global temperature prediction\",\n                value=0.92,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=1.2,\n            ),\n            \"convergence_rate\": ScientificMetric(\n                name=\"convergence_rate\",\n                description=\"Convergence rate of simulation\",\n                value=0.85,\n                unit=\"iterations^-1\",\n                is_higher_better=True,\n                weight=1.0,\n            ),\n            \"computational_efficiency\": ScientificMetric(\n                name=\"computational_efficiency\",\n                description=\"Computational efficiency\",\n                value=0.65,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=0.8,\n            ),\n        }\n    \n        # Add simulations\n>       sim1 = Simulation(\n            id=\"sim-accuracy-1\",\n            name=\"Atmospheric Model\",\n            description=\"High-resolution atmospheric simulation\",\n            estimated_duration=timedelta(days=14),\n            priority=SimulationPriority.HIGH,\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_management_integration.py:89: ValidationError"}, "teardown": {"duration": 0.00014192494563758373, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_manager_adaptation", "lineno": 371, "outcome": "error", "keywords": ["test_priority_manager_adaptation", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00022143404930830002, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "ValidationError"}], "longrepr": "@pytest.fixture\n    def sample_scenarios():\n        \"\"\"Create sample scenarios for integration testing.\"\"\"\n        # Create three scenarios with different characteristics\n        scenarios = []\n    \n        # Scenario 1: High accuracy, good convergence, moderate efficiency\n        scenario1 = Scenario(\n            id=\"high-accuracy-scenario\",\n            name=\"High Accuracy Climate Model\",\n            description=\"A climate model focused on accuracy\",\n            status=ScenarioStatus.ACTIVE,\n            priority_score=0.7,\n            tags=[\"climate\", \"high-resolution\", \"accuracy-focused\"],\n            resource_allocation={\"compute_nodes\": 100, \"storage\": 500, \"memory\": 200},\n        )\n    \n        # Add scientific metrics\n        scenario1.scientific_metrics = {\n            \"accuracy_global_temp\": ScientificMetric(\n                name=\"accuracy_global_temp\",\n                description=\"Accuracy of global temperature prediction\",\n                value=0.92,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=1.2,\n            ),\n            \"convergence_rate\": ScientificMetric(\n                name=\"convergence_rate\",\n                description=\"Convergence rate of simulation\",\n                value=0.85,\n                unit=\"iterations^-1\",\n                is_higher_better=True,\n                weight=1.0,\n            ),\n            \"computational_efficiency\": ScientificMetric(\n                name=\"computational_efficiency\",\n                description=\"Computational efficiency\",\n                value=0.65,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=0.8,\n            ),\n        }\n    \n        # Add simulations\n>       sim1 = Simulation(\n            id=\"sim-accuracy-1\",\n            name=\"Atmospheric Model\",\n            description=\"High-resolution atmospheric simulation\",\n            estimated_duration=timedelta(days=14),\n            priority=SimulationPriority.HIGH,\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_management_integration.py:89: ValidationError"}, "teardown": {"duration": 0.00013762898743152618, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "lineno": 429, "outcome": "error", "keywords": ["test_manual_override_integration", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00021909503266215324, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_management_integration.py", "lineno": 89, "message": "ValidationError"}], "longrepr": "@pytest.fixture\n    def sample_scenarios():\n        \"\"\"Create sample scenarios for integration testing.\"\"\"\n        # Create three scenarios with different characteristics\n        scenarios = []\n    \n        # Scenario 1: High accuracy, good convergence, moderate efficiency\n        scenario1 = Scenario(\n            id=\"high-accuracy-scenario\",\n            name=\"High Accuracy Climate Model\",\n            description=\"A climate model focused on accuracy\",\n            status=ScenarioStatus.ACTIVE,\n            priority_score=0.7,\n            tags=[\"climate\", \"high-resolution\", \"accuracy-focused\"],\n            resource_allocation={\"compute_nodes\": 100, \"storage\": 500, \"memory\": 200},\n        )\n    \n        # Add scientific metrics\n        scenario1.scientific_metrics = {\n            \"accuracy_global_temp\": ScientificMetric(\n                name=\"accuracy_global_temp\",\n                description=\"Accuracy of global temperature prediction\",\n                value=0.92,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=1.2,\n            ),\n            \"convergence_rate\": ScientificMetric(\n                name=\"convergence_rate\",\n                description=\"Convergence rate of simulation\",\n                value=0.85,\n                unit=\"iterations^-1\",\n                is_higher_better=True,\n                weight=1.0,\n            ),\n            \"computational_efficiency\": ScientificMetric(\n                name=\"computational_efficiency\",\n                description=\"Computational efficiency\",\n                value=0.65,\n                unit=\"dimensionless\",\n                is_higher_better=True,\n                weight=0.8,\n            ),\n        }\n    \n        # Add simulations\n>       sim1 = Simulation(\n            id=\"sim-accuracy-1\",\n            name=\"Atmospheric Model\",\n            description=\"High-resolution atmospheric simulation\",\n            estimated_duration=timedelta(days=14),\n            priority=SimulationPriority.HIGH,\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-accuracy-1', ...nPriority.HIGH: 'high'>}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_management_integration.py:89: ValidationError"}, "teardown": {"duration": 0.0001383910421282053, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric::test_initialization", "lineno": 24, "outcome": "passed", "keywords": ["test_initialization", "TestScientificMetric", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001188019523397088, "outcome": "passed"}, "call": {"duration": 0.00012203596998006105, "outcome": "passed"}, "teardown": {"duration": 8.019607048481703e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScientificMetric::test_normalized_score", "lineno": 58, "outcome": "failed", "keywords": ["test_normalized_score", "TestScientificMetric", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.581997826695442e-05, "outcome": "passed"}, "call": {"duration": 0.0002310160780325532, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_model.py", "lineno": 133, "message": "AssertionError: assert 1.2000000000000002 == 1.2\n +  where 1.2000000000000002 = normalized_score()\n +    where normalized_score = ScientificMetric(name='accuracy', description=None, value=0.8, unit=None, target_value=None, weight=1.5, is_higher_better=True, confidence=1.0).normalized_score"}, "traceback": [{"path": "tests/scenario_management/test_scenario_model.py", "lineno": 133, "message": "AssertionError"}], "longrepr": "self = <tests.scenario_management.test_scenario_model.TestScientificMetric object at 0x7f459c1ca120>\n\n    def test_normalized_score(self):\n        \"\"\"Test calculation of normalized scores.\"\"\"\n        # Test higher is better without target\n        metric1 = ScientificMetric(\n            name=\"accuracy\",\n            value=0.85,\n            weight=1.0,\n            is_higher_better=True,\n        )\n    \n        assert metric1.normalized_score() == 0.85\n    \n        # Test lower is better without target\n        metric2 = ScientificMetric(\n            name=\"error\",\n            value=0.15,\n            weight=1.0,\n            is_higher_better=False,\n        )\n    \n        assert metric2.normalized_score() == 0.85  # 1 - 0.15\n    \n        # Test higher is better with target (at target)\n        metric3 = ScientificMetric(\n            name=\"convergence\",\n            value=0.9,\n            target_value=0.9,\n            weight=1.0,\n            is_higher_better=True,\n        )\n    \n        assert metric3.normalized_score() == 1.0  # At target\n    \n        # Test higher is better with target (below target)\n        metric4 = ScientificMetric(\n            name=\"convergence\",\n            value=0.45,\n            target_value=0.9,\n            weight=1.0,\n            is_higher_better=True,\n        )\n    \n        assert metric4.normalized_score() == 0.5  # 0.45 / 0.9\n    \n        # Test lower is better with target (at target)\n        metric5 = ScientificMetric(\n            name=\"error_rate\",\n            value=0.1,\n            target_value=0.1,\n            weight=1.0,\n            is_higher_better=False,\n        )\n    \n        assert metric5.normalized_score() == 1.0  # At target\n    \n        # Test lower is better with target (above target)\n        metric6 = ScientificMetric(\n            name=\"error_rate\",\n            value=0.2,\n            target_value=0.1,\n            weight=1.0,\n            is_higher_better=False,\n        )\n    \n        assert metric6.normalized_score() == 0.5  # 0.1 / 0.2\n    \n        # Test with weight\n        metric7 = ScientificMetric(\n            name=\"accuracy\",\n            value=0.8,\n            weight=1.5,\n            is_higher_better=True,\n        )\n    \n>       assert metric7.normalized_score() == 1.2  # 0.8 * 1.5\nE       AssertionError: assert 1.2000000000000002 == 1.2\nE        +  where 1.2000000000000002 = normalized_score()\nE        +    where normalized_score = ScientificMetric(name='accuracy', description=None, value=0.8, unit=None, target_value=None, weight=1.5, is_higher_better=True, confidence=1.0).normalized_score\n\ntests/scenario_management/test_scenario_model.py:133: AssertionError"}, "teardown": {"duration": 0.0001298669958487153, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective::test_initialization", "lineno": 138, "outcome": "passed", "keywords": ["test_initialization", "TestResearchObjective", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 9.985396172851324e-05, "outcome": "passed"}, "call": {"duration": 0.00011340901255607605, "outcome": "passed"}, "teardown": {"duration": 7.735996041446924e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResearchObjective::test_is_relevant_to_scenario", "lineno": 156, "outcome": "passed", "keywords": ["test_is_relevant_to_scenario", "TestResearchObjective", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.069502655416727e-05, "outcome": "passed"}, "call": {"duration": 0.00014125695452094078, "outcome": "passed"}, "teardown": {"duration": 7.642700802534819e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_initialization", "lineno": 189, "outcome": "passed", "keywords": ["test_initialization", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.286209776997566e-05, "outcome": "passed"}, "call": {"duration": 0.00010184594430029392, "outcome": "passed"}, "teardown": {"duration": 7.291801739484072e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_calculate_priority_score", "lineno": 210, "outcome": "passed", "keywords": ["test_calculate_priority_score", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 7.900700438767672e-05, "outcome": "passed"}, "call": {"duration": 0.00014214590191841125, "outcome": "passed"}, "teardown": {"duration": 7.780396845191717e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_update_priority", "lineno": 258, "outcome": "passed", "keywords": ["test_update_priority", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001138889929279685, "outcome": "passed"}, "call": {"duration": 0.00011861801613122225, "outcome": "passed"}, "teardown": {"duration": 7.482105866074562e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_get_simulation_status_counts", "lineno": 288, "outcome": "failed", "keywords": ["test_get_simulation_status_counts", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.55099642649293e-05, "outcome": "passed"}, "call": {"duration": 0.00012311700265854597, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_model.py", "lineno": 298, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-1', 'name': '...tus.RUNNING: 'running'>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_model.py", "lineno": 298, "message": "ValidationError"}], "longrepr": "self = <tests.scenario_management.test_scenario_model.TestScenario object at 0x7f459c1ca9f0>\n\n    def test_get_simulation_status_counts(self):\n        \"\"\"Test counting simulations by status.\"\"\"\n        scenario = Scenario(\n            id=\"scenario-123\",\n            name=\"Test Scenario\",\n            description=\"A test scenario\",\n        )\n    \n        # Add simulations with different statuses\n>       sim1 = Simulation(\n            id=\"sim-1\",\n            name=\"Sim 1\",\n            description=\"Running simulation\",\n            status=SimulationStatus.RUNNING,\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-1', 'name': '...tus.RUNNING: 'running'>}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_model.py:298: ValidationError"}, "teardown": {"duration": 0.00012762309052050114, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_get_derived_priority", "lineno": 329, "outcome": "passed", "keywords": ["test_get_derived_priority", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 9.48919914662838e-05, "outcome": "passed"}, "call": {"duration": 0.00011794304009526968, "outcome": "passed"}, "teardown": {"duration": 8.066208101809025e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenario::test_total_progress", "lineno": 353, "outcome": "failed", "keywords": ["test_total_progress", "TestScenario", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.48439522087574e-05, "outcome": "passed"}, "call": {"duration": 0.0001361349131911993, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/scenario_management/test_scenario_model.py", "lineno": 366, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nstages\n  Field required [type=missing, input_value={'id': 'sim-1', 'name': '... 'description': 'Sim 1'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/scenario_management/test_scenario_model.py", "lineno": 366, "message": "ValidationError"}], "longrepr": "self = <tests.scenario_management.test_scenario_model.TestScenario object at 0x7f459c1cacf0>\n\n    def test_total_progress(self):\n        \"\"\"Test calculating total progress across simulations.\"\"\"\n        scenario = Scenario(\n            id=\"scenario-123\",\n            name=\"Test Scenario\",\n            description=\"A test scenario\",\n        )\n    \n        # Test with no simulations\n        assert scenario.total_progress() == 0.0\n    \n        # Add simulations with different progress\n>       sim1 = Simulation(id=\"sim-1\", name=\"Sim 1\", description=\"Sim 1\")\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for Simulation\nE       stages\nE         Field required [type=missing, input_value={'id': 'sim-1', 'name': '... 'description': 'Sim 1'}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/scenario_management/test_scenario_model.py:366: ValidationError"}, "teardown": {"duration": 0.0001230350462719798, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult::test_initialization", "lineno": 384, "outcome": "passed", "keywords": ["test_initialization", "TestComparisonResult", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 9.296194184571505e-05, "outcome": "passed"}, "call": {"duration": 0.00011574407108128071, "outcome": "passed"}, "teardown": {"duration": 8.061307016760111e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestComparisonResult::test_clear_winner", "lineno": 403, "outcome": "passed", "keywords": ["test_clear_winner", "TestComparisonResult", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 9.894894901663065e-05, "outcome": "passed"}, "call": {"duration": 0.00011608703061938286, "outcome": "passed"}, "teardown": {"duration": 7.655401714146137e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_initialization", "lineno": 457, "outcome": "passed", "keywords": ["test_initialization", "TestScenarioEvaluationResult", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.734699804335833e-05, "outcome": "passed"}, "call": {"duration": 0.00010239798575639725, "outcome": "passed"}, "teardown": {"duration": 7.721106521785259e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestScenarioEvaluationResult::test_should_adjust_priority", "lineno": 478, "outcome": "passed", "keywords": ["test_should_adjust_priority", "TestScenarioEvaluationResult", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 7.690198253840208e-05, "outcome": "passed"}, "call": {"duration": 0.00011562800500541925, "outcome": "passed"}, "teardown": {"duration": 7.863400969654322e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_initialization", "lineno": 503, "outcome": "passed", "keywords": ["test_initialization", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 8.118804544210434e-05, "outcome": "passed"}, "call": {"duration": 0.00011013296898454428, "outcome": "passed"}, "teardown": {"duration": 7.406703662127256e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_get_absolute_allocation", "lineno": 521, "outcome": "passed", "keywords": ["test_get_absolute_allocation", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 7.693294901400805e-05, "outcome": "passed"}, "call": {"duration": 0.00010762899182736874, "outcome": "passed"}, "teardown": {"duration": 7.304397877305746e-05, "outcome": "passed"}}, {"nodeid": "tests/scenario_management/test_scenario_model.py::TestResourceAllocation::test_is_valid", "lineno": 541, "outcome": "passed", "keywords": ["test_is_valid", "TestResourceAllocation", "test_scenario_model.py", "scenario_management", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 7.679802365601063e-05, "outcome": "passed"}, "call": {"duration": 0.00010937592014670372, "outcome": "passed"}, "teardown": {"duration": 8.061400149017572e-05, "outcome": "passed"}}, {"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow::test_scenario_lifecycle", "lineno": 277, "outcome": "error", "keywords": ["test_scenario_lifecycle", "TestEndToEndWorkflow", "test_full_system_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0017081729602068663, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_full_system_integration.py", "lineno": 211, "message": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nstart_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nend_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nsimulation_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nnode_ids\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresources\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\npriority\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_full_system_integration.py", "lineno": 211, "message": "ValidationError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-justinchiu_cohere_com/pytest-172/test_scenario_lifecycle0')\n\n    @pytest.fixture\n    def integrated_system(tmp_path):\n        \"\"\"Create a fully integrated system with all components.\"\"\"\n        # Create job management components\n        job_queue = JobQueue()\n>       reservation_system = ResourceReservation()\nE       pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nE       start_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       end_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       simulation_id\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       node_ids\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       resources\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       priority\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_full_system_integration.py:211: ValidationError"}, "teardown": {"duration": 0.00015095691196620464, "outcome": "passed"}}, {"nodeid": "tests/test_full_system_integration.py::TestEndToEndWorkflow::test_multi_scenario_prioritization", "lineno": 391, "outcome": "error", "keywords": ["test_multi_scenario_prioritization", "TestEndToEndWorkflow", "test_full_system_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00040136894676834345, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_full_system_integration.py", "lineno": 211, "message": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nstart_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nend_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nsimulation_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nnode_ids\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresources\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\npriority\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_full_system_integration.py", "lineno": 211, "message": "ValidationError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-justinchiu_cohere_com/pytest-172/test_multi_scenario_prioritiza0')\n\n    @pytest.fixture\n    def integrated_system(tmp_path):\n        \"\"\"Create a fully integrated system with all components.\"\"\"\n        # Create job management components\n        job_queue = JobQueue()\n>       reservation_system = ResourceReservation()\nE       pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nE       start_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       end_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       simulation_id\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       node_ids\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       resources\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       priority\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_full_system_integration.py:211: ValidationError"}, "teardown": {"duration": 0.00014447607100009918, "outcome": "passed"}}, {"nodeid": "tests/test_full_system_integration.py::TestFailureResilienceWithForecasting::test_forecasting_affects_checkpoint_frequency", "lineno": 562, "outcome": "error", "keywords": ["test_forecasting_affects_checkpoint_frequency", "TestFailureResilienceWithForecasting", "test_full_system_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00039231101982295513, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_full_system_integration.py", "lineno": 211, "message": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nstart_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nend_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nsimulation_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nnode_ids\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresources\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\npriority\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_full_system_integration.py", "lineno": 211, "message": "ValidationError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-justinchiu_cohere_com/pytest-172/test_forecasting_affects_check0')\n\n    @pytest.fixture\n    def integrated_system(tmp_path):\n        \"\"\"Create a fully integrated system with all components.\"\"\"\n        # Create job management components\n        job_queue = JobQueue()\n>       reservation_system = ResourceReservation()\nE       pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nE       start_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       end_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       simulation_id\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       node_ids\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       resources\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       priority\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_full_system_integration.py:211: ValidationError"}, "teardown": {"duration": 0.00014306302182376385, "outcome": "passed"}}, {"nodeid": "tests/test_full_system_integration.py::TestResourceOptimizationWithPriorities::test_priority_affects_resource_allocation", "lineno": 627, "outcome": "error", "keywords": ["test_priority_affects_resource_allocation", "TestResourceOptimizationWithPriorities", "test_full_system_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00039481406565755606, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_full_system_integration.py", "lineno": 211, "message": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nstart_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nend_time\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nsimulation_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nnode_ids\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nresources\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\npriority\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_full_system_integration.py", "lineno": 211, "message": "ValidationError"}], "longrepr": "tmp_path = PosixPath('/tmp/pytest-of-justinchiu_cohere_com/pytest-172/test_priority_affects_resource0')\n\n    @pytest.fixture\n    def integrated_system(tmp_path):\n        \"\"\"Create a fully integrated system with all components.\"\"\"\n        # Create job management components\n        job_queue = JobQueue()\n>       reservation_system = ResourceReservation()\nE       pydantic_core._pydantic_core.ValidationError: 6 validation errors for ResourceReservation\nE       start_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       end_time\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       simulation_id\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       node_ids\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       resources\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\nE       priority\nE         Field required [type=missing, input_value={}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_full_system_integration.py:211: ValidationError"}, "teardown": {"duration": 0.00015044189058244228, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_compare_and_adjust_priorities", "lineno": 234, "outcome": "failed", "keywords": ["test_compare_and_adjust_priorities", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00018790294416248798, "outcome": "passed"}, "call": {"duration": 0.020658493041992188, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_compare_and_adjust_priorities>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f459be93420>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00013463094364851713, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_force_update_ignores_threshold", "lineno": 208, "outcome": "failed", "keywords": ["test_force_update_ignores_threshold", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011289294343441725, "outcome": "passed"}, "call": {"duration": 0.019221422029659152, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_force_update_ignores_threshold>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f459777dda0>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00013828789815306664, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_priority_changes", "lineno": 476, "outcome": "failed", "keywords": ["test_get_priority_changes", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011418794747442007, "outcome": "passed"}, "call": {"duration": 0.020260069984942675, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_get_priority_changes>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597564220>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00014225300401449203, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_priority_trend", "lineno": 506, "outcome": "failed", "keywords": ["test_get_priority_trend", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011593196541070938, "outcome": "passed"}, "call": {"duration": 0.020015183021314442, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_get_priority_trend>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597566520>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00012559699825942516, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_get_resource_allocation_history", "lineno": 535, "outcome": "failed", "keywords": ["test_get_resource_allocation_history", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001134329941123724, "outcome": "passed"}, "call": {"duration": 0.02002728800289333, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_get_resource_allocation_history>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597424c20>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00013136700727045536, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_initialization", "lineno": 109, "outcome": "failed", "keywords": ["test_initialization", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012375705409795046, "outcome": "passed"}, "call": {"duration": 0.020134122110903263, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_initialization>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597426f20>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00013469497207552195, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_manual_priority_override", "lineno": 395, "outcome": "failed", "keywords": ["test_manual_priority_override", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001131619792431593, "outcome": "passed"}, "call": {"duration": 0.08028416696470231, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_manual_priority_override>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597426340>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00015207298565655947, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_needs_evaluation", "lineno": 117, "outcome": "failed", "keywords": ["test_needs_evaluation", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011942803394049406, "outcome": "passed"}, "call": {"duration": 0.020167395938187838, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_needs_evaluation>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597567600>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00012878305278718472, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_reallocate_resources", "lineno": 291, "outcome": "failed", "keywords": ["test_reallocate_resources", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011288502719253302, "outcome": "passed"}, "call": {"duration": 0.019919640966691077, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_reallocate_resources>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4597565c60>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00013424793723970652, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_reallocation_strategy_threshold_based", "lineno": 347, "outcome": "failed", "keywords": ["test_reallocation_strategy_threshold_based", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012762995902448893, "outcome": "passed"}, "call": {"duration": 0.020218877936713398, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_reallocation_strategy_threshold_based>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f459777f4c0>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.0001334820408374071, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_recompute_all_priorities", "lineno": 425, "outcome": "failed", "keywords": ["test_recompute_all_priorities", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011565396562218666, "outcome": "passed"}, "call": {"duration": 0.01951400109101087, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_recompute_all_priorities>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f459777d9e0>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.0001307530328631401, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_small_priority_change_below_threshold", "lineno": 176, "outcome": "failed", "keywords": ["test_small_priority_change_below_threshold", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00011634803377091885, "outcome": "passed"}, "call": {"duration": 0.019337130011990666, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_small_priority_change_below_threshold>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f459be93240>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00014086696319282055, "outcome": "passed"}}, {"nodeid": "tests/test_priority_manager.py::TestPriorityManager::test_update_scenario_priority", "lineno": 138, "outcome": "failed", "keywords": ["test_update_scenario_priority", "TestPriorityManager", "test_priority_manager.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.0001167969312518835, "outcome": "passed"}, "call": {"duration": 0.019538301043212414, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/.venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError: \"Scenario\" object has no field \"update_priority\""}, "traceback": [{"path": "tests/test_priority_manager.py", "lineno": 48, "message": ""}, {"path": "tests/test_priority_manager.py", "lineno": 100, "message": "in _create_test_scenario"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 995, "message": "in __setattr__"}, {"path": ".venv/lib/python3.12/site-packages/pydantic/main.py", "lineno": 1042, "message": "ValueError"}], "longrepr": "self = <tests.test_priority_manager.TestPriorityManager testMethod=test_update_scenario_priority>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and mocks.\"\"\"\n        # Create evaluator and comparator mocks\n        self.evaluator_mock = mock.create_autospec(ScenarioEvaluator)\n        self.comparator_mock = mock.create_autospec(ScenarioComparator)\n    \n        # Configure example scenarios\n        self.scenarios = []\n        for i in range(5):\n>           scenario = self._create_test_scenario(i)\n\ntests/test_priority_manager.py:48: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/test_priority_manager.py:100: in _create_test_scenario\n    scenario.update_priority = patched_update_priority\n.venv/lib/python3.12/site-packages/pydantic/main.py:995: in __setattr__\n    elif (setattr_handler := self._setattr_handler(name, value)) is not None:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = Scenario(id='scenario-0', name='Test Scenario 0', description='A test scenario with index 0', simulations={}, status=<...compute_node': 10.0, 'memory': 100.0}, researcher_ids=[], related_scenarios=set(), research_objectives=[], metadata={})\nname = 'update_priority'\nvalue = <function TestPriorityManager._create_test_scenario.<locals>.patched_update_priority at 0x7f4599ad1440>\n\n    def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n        \"\"\"Get a handler for setting an attribute on the model instance.\n    \n        Returns:\n            A handler for setting an attribute on the model instance. Used for memoization of the handler.\n            Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n            Returns `None` when memoization is not safe, then the attribute is set directly.\n        \"\"\"\n        cls = self.__class__\n        if name in cls.__class_vars__:\n            raise AttributeError(\n                f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n            )\n        elif not _fields.is_valid_field_name(name):\n            if (attribute := cls.__private_attributes__.get(name)) is not None:\n                if hasattr(attribute, '__set__'):\n                    return lambda model, _name, val: attribute.__set__(model, val)\n                else:\n                    return _SIMPLE_SETATTR_HANDLERS['private']\n            else:\n                _object_setattr(self, name, value)\n                return None  # Can not return memoized handler with possibly freeform attr names\n    \n        attr = getattr(cls, name, None)\n        # NOTE: We currently special case properties and `cached_property`, but we might need\n        # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n        # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n        # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n        if isinstance(attr, cached_property):\n            return _SIMPLE_SETATTR_HANDLERS['cached_property']\n    \n        _check_frozen(cls, name, value)\n    \n        # We allow properties to be set only on non frozen models for now (to match dataclasses).\n        # This can be changed if it ever gets requested.\n        if isinstance(attr, property):\n            return lambda model, _name, val: attr.__set__(model, val)\n        elif cls.model_config.get('validate_assignment'):\n            return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n        elif name not in cls.__pydantic_fields__:\n            if cls.model_config.get('extra') != 'allow':\n                # TODO - matching error\n>               raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\nE               ValueError: \"Scenario\" object has no field \"update_priority\"\n\n.venv/lib/python3.12/site-packages/pydantic/main.py:1042: ValueError"}, "teardown": {"duration": 0.00016627705190330744, "outcome": "passed"}}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_complementary_scenario_detection", "lineno": 272, "outcome": "failed", "keywords": ["test_complementary_scenario_detection", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.000194865046069026, "outcome": "passed"}, "call": {"duration": 0.00014227500651031733, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_scenario_management_integration.py", "lineno": 28, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nsuccess_criteria\n  Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_scenario_management_integration.py", "lineno": 28, "message": "ValidationError"}], "longrepr": "self = <tests.test_scenario_management_integration.TestScenarioManagementIntegration testMethod=test_complementary_scenario_detection>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and integration components.\"\"\"\n        # Create research objectives\n        self.objectives = [\n>           ResearchObjective(\n                id=\"obj1\",\n                name=\"Energy Efficiency\",\n                description=\"Improve energy efficiency of the system\",\n                importance=0.8,\n            ),\n            ResearchObjective(\n                id=\"obj2\",\n                name=\"Accuracy Improvement\",\n                description=\"Improve prediction accuracy\",\n                importance=0.9,\n            ),\n            ResearchObjective(\n                id=\"obj3\",\n                name=\"Novel Applications\",\n                description=\"Discover novel applications of the algorithm\",\n                importance=0.7,\n            ),\n        ]\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nE       success_criteria\nE         Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_scenario_management_integration.py:28: ValidationError"}, "teardown": {"duration": 0.00012561806943267584, "outcome": "passed"}}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_end_to_end_priority_workflow", "lineno": 136, "outcome": "failed", "keywords": ["test_end_to_end_priority_workflow", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00014929997269064188, "outcome": "passed"}, "call": {"duration": 0.0001490960130468011, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_scenario_management_integration.py", "lineno": 28, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nsuccess_criteria\n  Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_scenario_management_integration.py", "lineno": 28, "message": "ValidationError"}], "longrepr": "self = <tests.test_scenario_management_integration.TestScenarioManagementIntegration testMethod=test_end_to_end_priority_workflow>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and integration components.\"\"\"\n        # Create research objectives\n        self.objectives = [\n>           ResearchObjective(\n                id=\"obj1\",\n                name=\"Energy Efficiency\",\n                description=\"Improve energy efficiency of the system\",\n                importance=0.8,\n            ),\n            ResearchObjective(\n                id=\"obj2\",\n                name=\"Accuracy Improvement\",\n                description=\"Improve prediction accuracy\",\n                importance=0.9,\n            ),\n            ResearchObjective(\n                id=\"obj3\",\n                name=\"Novel Applications\",\n                description=\"Discover novel applications of the algorithm\",\n                importance=0.7,\n            ),\n        ]\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nE       success_criteria\nE         Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_scenario_management_integration.py:28: ValidationError"}, "teardown": {"duration": 0.0001252690562978387, "outcome": "passed"}}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_manual_override_integration", "lineno": 234, "outcome": "failed", "keywords": ["test_manual_override_integration", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00012026203330606222, "outcome": "passed"}, "call": {"duration": 0.00018476799596101046, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_scenario_management_integration.py", "lineno": 28, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nsuccess_criteria\n  Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_scenario_management_integration.py", "lineno": 28, "message": "ValidationError"}], "longrepr": "self = <tests.test_scenario_management_integration.TestScenarioManagementIntegration testMethod=test_manual_override_integration>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and integration components.\"\"\"\n        # Create research objectives\n        self.objectives = [\n>           ResearchObjective(\n                id=\"obj1\",\n                name=\"Energy Efficiency\",\n                description=\"Improve energy efficiency of the system\",\n                importance=0.8,\n            ),\n            ResearchObjective(\n                id=\"obj2\",\n                name=\"Accuracy Improvement\",\n                description=\"Improve prediction accuracy\",\n                importance=0.9,\n            ),\n            ResearchObjective(\n                id=\"obj3\",\n                name=\"Novel Applications\",\n                description=\"Discover novel applications of the algorithm\",\n                importance=0.7,\n            ),\n        ]\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nE       success_criteria\nE         Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_scenario_management_integration.py:28: ValidationError"}, "teardown": {"duration": 0.00012739805970340967, "outcome": "passed"}}, {"nodeid": "tests/test_scenario_management_integration.py::TestScenarioManagementIntegration::test_priority_adaptation_to_metric_changes", "lineno": 196, "outcome": "failed", "keywords": ["test_priority_adaptation_to_metric_changes", "TestScenarioManagementIntegration", "test_scenario_management_integration.py", "tests", "concurrent_task_scheduler_scientific_computing", ""], "setup": {"duration": 0.00013468600809574127, "outcome": "passed"}, "call": {"duration": 0.00013513700105249882, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/tests/test_scenario_management_integration.py", "lineno": 28, "message": "pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nsuccess_criteria\n  Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"}, "traceback": [{"path": "tests/test_scenario_management_integration.py", "lineno": 28, "message": "ValidationError"}], "longrepr": "self = <tests.test_scenario_management_integration.TestScenarioManagementIntegration testMethod=test_priority_adaptation_to_metric_changes>\n\n    def setUp(self):\n        \"\"\"Set up test scenarios and integration components.\"\"\"\n        # Create research objectives\n        self.objectives = [\n>           ResearchObjective(\n                id=\"obj1\",\n                name=\"Energy Efficiency\",\n                description=\"Improve energy efficiency of the system\",\n                importance=0.8,\n            ),\n            ResearchObjective(\n                id=\"obj2\",\n                name=\"Accuracy Improvement\",\n                description=\"Improve prediction accuracy\",\n                importance=0.9,\n            ),\n            ResearchObjective(\n                id=\"obj3\",\n                name=\"Novel Applications\",\n                description=\"Discover novel applications of the algorithm\",\n                importance=0.7,\n            ),\n        ]\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for ResearchObjective\nE       success_criteria\nE         Field required [type=missing, input_value={'id': 'obj1', 'name': 'E...tem', 'importance': 0.8}, input_type=dict]\nE           For further information visit https://errors.pydantic.dev/2.11/v/missing\n\ntests/test_scenario_management_integration.py:28: ValidationError"}, "teardown": {"duration": 0.0001892769942060113, "outcome": "passed"}}], "warnings": [{"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}, {"message": "'H' is deprecated and will be removed in a future version, please use 'h' instead.", "category": "FutureWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/concurrent_task_scheduler/concurrent_task_scheduler_scientific_computing/concurrent_task_scheduler/resource_forecasting/data_collector.py", "lineno": 454}]}