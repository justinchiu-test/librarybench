"""Database index efficiency analysis."""

import os
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Union, Any
from dataclasses import dataclass
from datetime import datetime
import logging
from collections import defaultdict
import uuid
import re
import json
from enum import Enum

from pydantic import BaseModel, Field

from ..utils.types import (
    DatabaseEngine, 
    FileCategory, 
    ScanStatus, 
    AnalysisResult,
    DatabaseFile,
    OptimizationRecommendation,
    OptimizationPriority
)


logger = logging.getLogger(__name__)


class IndexInfo(BaseModel):
    """Information about a database index."""

    name: str
    table_name: str
    columns: List[str]
    size_bytes: int
    is_unique: bool = False
    is_primary: bool = False
    is_clustered: bool = False
    is_partial: bool = False
    usage_count: Optional[int] = None
    avg_query_time_ms: Optional[float] = None
    fragmentation_percent: Optional[float] = None
    database_name: Optional[str] = None
    schema_name: Optional[str] = None
    engine: DatabaseEngine
    metadata: Dict[str, Any] = Field(default_factory=dict)

    def get_column_signature(self) -> str:
        """Get a signature of index columns for comparison."""
        return ",".join(sorted(self.columns))
    
    def is_prefix_of(self, other: "IndexInfo") -> bool:
        """Check if this index is a prefix of another index."""
        if self.table_name != other.table_name:
            return False
            
        if len(self.columns) >= len(other.columns):
            return False
            
        # Check if all columns in this index are a prefix of the other index
        for i, col in enumerate(self.columns):
            if i >= len(other.columns) or col != other.columns[i]:
                return False
                
        return True


class IndexMetrics(BaseModel):
    """Metrics calculated for a database index."""

    index: IndexInfo
    space_to_performance_ratio: Optional[float] = None
    redundancy_score: float = 0.0
    usage_score: Optional[float] = None
    optimization_potential: float = 0.0
    overlapping_indexes: List[str] = Field(default_factory=list)
    duplicate_indexes: List[str] = Field(default_factory=list)
    unused_score: float = 0.0


class IndexAnalysisResult(AnalysisResult):
    """Results from index efficiency analysis."""

    total_indexes: int
    total_index_size_bytes: int
    indexes_by_table: Dict[str, int]
    redundant_indexes: List[IndexMetrics]
    unused_indexes: List[IndexMetrics]
    high_value_indexes: List[IndexMetrics]
    indexes_by_size: List[IndexMetrics]
    average_space_to_performance_ratio: Optional[float] = None
    total_potential_savings_bytes: int = 0


class IndexType(str, Enum):
    """Types of database indexes."""

    BTREE = "btree"
    HASH = "hash"
    GIST = "gist"
    GIN = "gin"
    SPATIAL = "spatial"
    FULLTEXT = "fulltext"
    BRIN = "brin"
    UNKNOWN = "unknown"


class QueryType(str, Enum):
    """Types of database queries for index usage."""

    SELECT = "select"
    JOIN = "join"
    ORDER_BY = "order_by"
    GROUP_BY = "group_by"
    RANGE_SCAN = "range_scan"
    POINT_LOOKUP = "point_lookup"
    UNKNOWN = "unknown"


class IndexValueCategory(str, Enum):
    """Categories for index value assessment."""

    HIGH_VALUE = "high_value"
    MEDIUM_VALUE = "medium_value"
    LOW_VALUE = "low_value"
    REDUNDANT = "redundant"
    UNUSED = "unused"


class IndexEfficiencyAnalyzer:
    """
    Analyzes database indexes for efficiency and optimization opportunities.
    
    This class examines database index files, analyzes their storage overhead
    versus query performance benefits, detects redundant and unused indexes,
    and provides optimization recommendations.
    """

    def __init__(
        self,
        min_usage_threshold: int = 10,
        redundancy_threshold: float = 0.8,
        space_performance_weight_ratio: float = 0.5,
        fragmentation_threshold: float = 30.0,
    ):
        """
        Initialize the index efficiency analyzer.

        Args:
            min_usage_threshold: Minimum usage count to consider an index used
            redundancy_threshold: Threshold to consider an index redundant (0.0-1.0)
            space_performance_weight_ratio: Weight to balance space vs performance (0.0-1.0)
            fragmentation_threshold: Fragmentation percent threshold to recommend rebuilding
        """
        self.min_usage_threshold = min_usage_threshold
        self.redundancy_threshold = redundancy_threshold
        self.space_performance_weight_ratio = space_performance_weight_ratio
        self.fragmentation_threshold = fragmentation_threshold

    def detect_redundant_indexes(self, indexes: List[IndexInfo]) -> Dict[str, List[str]]:
        """
        Detect redundant indexes (overlapping or duplicate).

        Args:
            indexes: List of index information

        Returns:
            Dictionary mapping index names to lists of redundant index names
        """
        redundant_map = defaultdict(list)
        
        # Group indexes by table name for easier comparison
        indexes_by_table = defaultdict(list)
        for idx in indexes:
            indexes_by_table[idx.table_name].append(idx)
        
        # For each table, find redundant indexes
        for table_name, table_indexes in indexes_by_table.items():
            table_indexes_dict = {idx.name: idx for idx in table_indexes}
            
            # Compare each pair of indexes
            for i, idx1 in enumerate(table_indexes):
                if idx1.is_primary:  # Skip primary keys as they're required
                    continue
                    
                for j, idx2 in enumerate(table_indexes):
                    if i == j or idx2.is_primary:
                        continue
                    
                    # Skip if index types are incompatible
                    if idx1.is_unique and not idx2.is_unique:
                        continue
                        
                    # Check for exact column match (duplicate)
                    if set(idx1.columns) == set(idx2.columns):
                        redundant_map[idx1.name].append(idx2.name)
                        
                    # Check for prefix indexes
                    elif idx1.is_prefix_of(idx2):
                        # A prefix index is redundant if the larger index is useful for the same queries
                        # We consider it redundant if the larger index is also frequently used
                        if idx2.usage_count and idx2.usage_count >= self.min_usage_threshold:
                            redundant_map[idx1.name].append(idx2.name)
        
        return redundant_map

    def calculate_space_performance_ratio(
        self, index: IndexInfo, table_size: Optional[int] = None
    ) -> float:
        """
        Calculate ratio of storage overhead to query performance benefit.

        Args:
            index: Index information
            table_size: Size of the table the index belongs to (optional)

        Returns:
            Ratio value (higher means more overhead for less benefit)
        """
        # If we have actual performance data
        if index.avg_query_time_ms is not None and index.usage_count is not None:
            # Estimate performance benefit: usage_count * (1000 / avg_query_time_ms)
            # Higher usage and lower query time means better performance
            performance_benefit = index.usage_count * (1000 / max(1.0, index.avg_query_time_ms))
        else:
            # Without performance data, estimate based on columns and uniqueness
            # More columns and unique constraints typically provide better performance
            performance_factor = len(index.columns) * (2 if index.is_unique else 1)
            performance_benefit = performance_factor * 100  # Scale factor
        
        # Size overhead calculation
        if table_size and table_size > 0:
            # Calculate as a percentage of table size
            size_overhead = (index.size_bytes / table_size) * 100
        else:
            # Without table size, use absolute size in MB as a proxy
            size_overhead = index.size_bytes / (1024 * 1024)
        
        # Prevent division by zero
        if performance_benefit <= 0:
            return float('inf')
            
        # Calculate ratio (higher means worse value)
        ratio = size_overhead / performance_benefit
        
        # Apply weighting factor to balance size vs performance importance
        weighted_ratio = ratio * self.space_performance_weight_ratio + (1 - self.space_performance_weight_ratio)
        
        return weighted_ratio

    def detect_unused_indexes(self, indexes: List[IndexInfo]) -> List[str]:
        """
        Detect unused or rarely used indexes.

        Args:
            indexes: List of index information

        Returns:
            List of unused index names
        """
        unused_indexes = []
        
        for idx in indexes:
            # Skip primary keys as they're required for data integrity
            if idx.is_primary:
                continue
                
            # Check usage count if available
            if idx.usage_count is not None:
                if idx.usage_count < self.min_usage_threshold:
                    unused_indexes.append(idx.name)
            
            # If no usage data, fall back to heuristics based on index type/purpose
            # This is less reliable but provides a best guess
            elif idx.metadata.get('purpose') == 'backup' or idx.metadata.get('is_obsolete'):
                unused_indexes.append(idx.name)
        
        return unused_indexes

    def calculate_index_metrics(
        self, 
        indexes: List[IndexInfo],
        tables_sizes: Dict[str, int] = None
    ) -> Dict[str, IndexMetrics]:
        """
        Calculate metrics for each index to determine optimization potential.

        Args:
            indexes: List of index information
            tables_sizes: Dictionary mapping table names to sizes in bytes

        Returns:
            Dictionary mapping index names to metrics
        """
        metrics = {}
        redundant_map = self.detect_redundant_indexes(indexes)
        unused_indexes = self.detect_unused_indexes(indexes)
        
        # Dictionary for fast lookup
        indexes_dict = {idx.name: idx for idx in indexes}
        
        # Calculate metrics for each index
        for idx in indexes:
            table_size = tables_sizes.get(idx.table_name) if tables_sizes else None
            
            # Calculate space to performance ratio
            space_perf_ratio = self.calculate_space_performance_ratio(idx, table_size)
            
            # Calculate redundancy score (0-1)
            # Higher score means more redundant
            redundant_with = redundant_map.get(idx.name, [])
            redundancy_score = len(redundant_with) / max(1, len(indexes_by_table[idx.table_name]) - 1)
            redundancy_score = min(1.0, redundancy_score)
            
            # Calculate usage score (0-1)
            # Higher score means more used
            usage_score = None
            if idx.usage_count is not None:
                usage_score = min(1.0, idx.usage_count / max(1, self.min_usage_threshold * 2))
                
            # Calculate unused score (0-1)
            # Higher score means less used
            unused_score = 1.0 - (usage_score or 0.0) if usage_score is not None else 0.5
            
            # If in unused list, set unused score high
            if idx.name in unused_indexes:
                unused_score = 0.9
                
            # Calculate overall optimization potential (0-1)
            # Higher means greater opportunity for optimization
            optimization_factors = [
                0.6 * redundancy_score,  # Higher weight for redundancy
                0.3 * unused_score,      # Medium weight for usage
                0.1 * min(1.0, space_perf_ratio / 10)  # Lower weight for space/perf ratio
            ]
            optimization_potential = sum(optimization_factors)
            
            # Separate lists for different types of redundancy
            duplicate_indexes = []
            overlapping_indexes = []
            
            for other_name in redundant_with:
                other = indexes_dict.get(other_name)
                if other and set(idx.columns) == set(other.columns):
                    duplicate_indexes.append(other_name)
                else:
                    overlapping_indexes.append(other_name)
            
            # Create metrics object
            metrics[idx.name] = IndexMetrics(
                index=idx,
                space_to_performance_ratio=space_perf_ratio,
                redundancy_score=redundancy_score,
                usage_score=usage_score,
                optimization_potential=optimization_potential,
                overlapping_indexes=overlapping_indexes,
                duplicate_indexes=duplicate_indexes,
                unused_score=unused_score
            )
        
        return metrics

    def categorize_indexes(self, metrics: Dict[str, IndexMetrics]) -> Dict[IndexValueCategory, List[str]]:
        """
        Categorize indexes based on their value and optimization potential.

        Args:
            metrics: Dictionary of index metrics

        Returns:
            Dictionary mapping categories to lists of index names
        """
        categories = {
            IndexValueCategory.HIGH_VALUE: [],
            IndexValueCategory.MEDIUM_VALUE: [],
            IndexValueCategory.LOW_VALUE: [],
            IndexValueCategory.REDUNDANT: [],
            IndexValueCategory.UNUSED: [],
        }
        
        for name, metric in metrics.items():
            # Primary keys are always high value
            if metric.index.is_primary:
                categories[IndexValueCategory.HIGH_VALUE].append(name)
                continue
                
            # Categorize based on metrics
            if metric.redundancy_score > self.redundancy_threshold:
                categories[IndexValueCategory.REDUNDANT].append(name)
            elif metric.unused_score > 0.8:
                categories[IndexValueCategory.UNUSED].append(name)
            elif metric.usage_score is not None and metric.usage_score > 0.7:
                categories[IndexValueCategory.HIGH_VALUE].append(name)
            elif metric.usage_score is not None and metric.usage_score > 0.3:
                categories[IndexValueCategory.MEDIUM_VALUE].append(name)
            else:
                categories[IndexValueCategory.LOW_VALUE].append(name)
        
        return categories

    def generate_recommendations(
        self, metrics: Dict[str, IndexMetrics], categories: Dict[IndexValueCategory, List[str]]
    ) -> List[OptimizationRecommendation]:
        """
        Generate optimization recommendations based on index analysis.

        Args:
            metrics: Dictionary of index metrics
            categories: Dictionary mapping categories to lists of index names

        Returns:
            List of optimization recommendations
        """
        recommendations = []
        
        # Group redundant indexes by table
        redundant_by_table = defaultdict(list)
        for name in categories[IndexValueCategory.REDUNDANT]:
            metric = metrics[name]
            redundant_by_table[metric.index.table_name].append((name, metric))
        
        # Generate recommendations for redundant indexes
        for table, redundant_indexes in redundant_by_table.items():
            if not redundant_indexes:
                continue
                
            # Sort by optimization potential (highest first)
            redundant_indexes.sort(key=lambda x: x[1].optimization_potential, reverse=True)
            
            # Group by duplicate sets
            duplicates_groups = defaultdict(list)
            for name, metric in redundant_indexes:
                if metric.duplicate_indexes:
                    # Create a signature for the group based on column list
                    cols_sig = metric.index.get_column_signature()
                    duplicates_groups[cols_sig].append((name, metric))
            
            # Recommendations for duplicate indexes
            for cols_sig, dups in duplicates_groups.items():
                if len(dups) < 2:
                    continue
                    
                # Select indexes to keep and remove
                # Sort by usage and uniqueness (keep the most used and unique ones)
                sorted_dups = sorted(
                    dups,
                    key=lambda x: (
                        x[1].index.is_primary,  # Keep primary keys
                        x[1].index.is_unique,   # Prefer unique indexes
                        x[1].index.is_clustered,  # Prefer clustered indexes
                        x[1].usage_score or 0   # Prefer more used indexes
                    ),
                    reverse=True
                )
                
                to_keep = sorted_dups[0][0]
                to_remove = [name for name, _ in sorted_dups[1:]]
                
                if not to_remove:
                    continue
                    
                # Calculate space savings
                space_savings = sum(metrics[name].index.size_bytes for name in to_remove)
                
                recommendations.append(
                    OptimizationRecommendation(
                        id=f"IDX-DUP-{uuid.uuid4().hex[:6]}",
                        title=f"Remove duplicate indexes on {table}",
                        description=(
                            f"Remove {len(to_remove)} duplicate indexes on table {table}. "
                            f"Keep index '{to_keep}' and remove: {', '.join(to_remove)}. "
                            f"These indexes have identical column coverage and are redundant."
                        ),
                        priority=OptimizationPriority.HIGH,
                        estimated_space_savings_bytes=space_savings,
                        estimated_performance_impact_percent=0.0,  # No performance impact as we keep one index
                        implementation_complexity="low",
                        affected_files=[metrics[name].index.name for name in to_remove],
                    )
                )
            
            # Recommendations for overlapping indexes
            overlapping_groups = []
            for name, metric in redundant_indexes:
                if metric.overlapping_indexes and name not in {i for g in duplicates_groups.values() for i, _ in g}:
                    overlapping_groups.append((name, metric))
                    
            if overlapping_groups:
                # Sort by optimization potential
                overlapping_groups.sort(key=lambda x: x[1].optimization_potential, reverse=True)
                
                # Take top 3 for recommendation
                top_overlapping = overlapping_groups[:3]
                
                space_savings = sum(metrics[name].index.size_bytes for name, _ in top_overlapping)
                
                recommendations.append(
                    OptimizationRecommendation(
                        id=f"IDX-OVER-{uuid.uuid4().hex[:6]}",
                        title=f"Review overlapping indexes on {table}",
                        description=(
                            f"Review {len(top_overlapping)} overlapping indexes on table {table}. "
                            f"Indexes {[name for name, _ in top_overlapping]} have overlapping columns. "
                            f"Consider consolidating these into fewer, more efficient indexes."
                        ),
                        priority=OptimizationPriority.MEDIUM,
                        estimated_space_savings_bytes=space_savings // 2,  # Assume 50% savings
                        estimated_performance_impact_percent=5.0,  # Slight performance boost from better indexes
                        implementation_complexity="medium",
                        affected_files=[name for name, _ in top_overlapping],
                    )
                )
        
        # Generate recommendations for unused indexes
        unused_names = categories[IndexValueCategory.UNUSED]
        if unused_names:
            # Group by table
            unused_by_table = defaultdict(list)
            for name in unused_names:
                metric = metrics[name]
                unused_by_table[metric.index.table_name].append((name, metric))
                
            for table, unused_indexes in unused_by_table.items():
                if len(unused_indexes) > 3:
                    # If many unused indexes, batch recommendations
                    space_savings = sum(metrics[name].index.size_bytes for name, _ in unused_indexes)
                    unused_names_list = [name for name, _ in unused_indexes]
                    
                    recommendations.append(
                        OptimizationRecommendation(
                            id=f"IDX-UNUSED-{uuid.uuid4().hex[:6]}",
                            title=f"Remove unused indexes on {table}",
                            description=(
                                f"Remove {len(unused_indexes)} unused indexes on table {table}: {', '.join(unused_names_list[:5])} "
                                f"{f'and {len(unused_names_list) - 5} more' if len(unused_names_list) > 5 else ''}. "
                                f"These indexes have not been used in queries and are consuming space unnecessarily."
                            ),
                            priority=OptimizationPriority.HIGH,
                            estimated_space_savings_bytes=space_savings,
                            estimated_performance_impact_percent=-2.0,  # Slight negative impact on some queries
                            implementation_complexity="low",
                            affected_files=unused_names_list,
                        )
                    )
                else:
                    # Individual recommendations for smaller sets
                    for name, metric in unused_indexes:
                        recommendations.append(
                            OptimizationRecommendation(
                                id=f"IDX-UNUSED-{uuid.uuid4().hex[:6]}",
                                title=f"Remove unused index {name} on {table}",
                                description=(
                                    f"Remove unused index {name} on table {table}. "
                                    f"This index has not been used in queries and is consuming "
                                    f"{metric.index.size_bytes / (1024*1024):.2f} MB of space unnecessarily."
                                ),
                                priority=OptimizationPriority.MEDIUM,
                                estimated_space_savings_bytes=metric.index.size_bytes,
                                estimated_performance_impact_percent=-1.0,  # Minor impact
                                implementation_complexity="low",
                                affected_files=[name],
                            )
                        )
        
        # Recommendations for fragmented indexes
        fragmented_indexes = []
        for name, metric in metrics.items():
            if (metric.index.fragmentation_percent is not None and 
                metric.index.fragmentation_percent > self.fragmentation_threshold):
                fragmented_indexes.append((name, metric))
                
        if fragmented_indexes:
            # Sort by fragmentation level (highest first)
            fragmented_indexes.sort(key=lambda x: x[1].index.fragmentation_percent or 0, reverse=True)
            
            # Take top 5 most fragmented
            top_fragmented = fragmented_indexes[:5]
            
            recommendations.append(
                OptimizationRecommendation(
                    id=f"IDX-FRAG-{uuid.uuid4().hex[:6]}",
                    title="Rebuild highly fragmented indexes",
                    description=(
                        f"Rebuild {len(top_fragmented)} highly fragmented indexes: "
                        f"{', '.join([name for name, metric in top_fragmented])}. "
                        f"These indexes have fragmentation levels above {self.fragmentation_threshold}%, "
                        f"which can degrade query performance and waste storage space."
                    ),
                    priority=OptimizationPriority.MEDIUM,
                    estimated_space_savings_bytes=sum([
                        int(metric.index.size_bytes * (metric.index.fragmentation_percent or 0) / 100 * 0.3)
                        for _, metric in top_fragmented
                    ]),
                    estimated_performance_impact_percent=10.0,  # Significant performance boost
                    implementation_complexity="medium",
                    affected_files=[name for name, _ in top_fragmented],
                )
            )
        
        return recommendations

    def analyze_indexes(
        self,
        indexes: List[IndexInfo],
        tables_sizes: Optional[Dict[str, int]] = None,
    ) -> IndexAnalysisResult:
        """
        Analyze database indexes for efficiency and optimization opportunities.

        Args:
            indexes: List of index information
            tables_sizes: Dictionary mapping table names to sizes in bytes

        Returns:
            Index analysis result
        """
        start_time = datetime.now()
        tables_sizes = tables_sizes or {}
        
        try:
            # Calculate metrics for each index
            index_metrics = self.calculate_index_metrics(indexes, tables_sizes)
            
            # Categorize indexes
            categories = self.categorize_indexes(index_metrics)
            
            # Generate recommendations
            recommendations = self.generate_recommendations(index_metrics, categories)
            
            # Calculate total index size
            total_index_size = sum(idx.size_bytes for idx in indexes)
            
            # Calculate average space to performance ratio
            valid_ratios = [m.space_to_performance_ratio for m in index_metrics.values() 
                           if m.space_to_performance_ratio is not None and m.space_to_performance_ratio != float('inf')]
            avg_space_perf_ratio = sum(valid_ratios) / len(valid_ratios) if valid_ratios else None
            
            # Count indexes by table
            indexes_by_table = defaultdict(int)
            for idx in indexes:
                indexes_by_table[idx.table_name] += 1
            
            # Calculate potential savings
            total_savings = sum(r.estimated_space_savings_bytes or 0 for r in recommendations)
            
            # Create sorted lists of indexes by category
            redundant_indexes = [index_metrics[name] for name in categories[IndexValueCategory.REDUNDANT]]
            unused_indexes = [index_metrics[name] for name in categories[IndexValueCategory.UNUSED]]
            high_value_indexes = [index_metrics[name] for name in categories[IndexValueCategory.HIGH_VALUE]]
            
            # Sort indexes by size
            indexes_by_size = sorted(
                list(index_metrics.values()),
                key=lambda m: m.index.size_bytes,
                reverse=True
            )
            
            # Create result
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            return IndexAnalysisResult(
                analysis_duration_seconds=duration,
                scan_status=ScanStatus.COMPLETED,
                total_indexes=len(indexes),
                total_index_size_bytes=total_index_size,
                indexes_by_table=dict(indexes_by_table),
                redundant_indexes=redundant_indexes,
                unused_indexes=unused_indexes,
                high_value_indexes=high_value_indexes,
                indexes_by_size=indexes_by_size,
                average_space_to_performance_ratio=avg_space_perf_ratio,
                total_potential_savings_bytes=total_savings,
                recommendations=recommendations,
            )
            
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.error(f"Error in index analysis: {e}")
            
            return IndexAnalysisResult(
                analysis_duration_seconds=duration,
                scan_status=ScanStatus.FAILED,
                error_message=str(e),
                total_indexes=0,
                total_index_size_bytes=0,
                indexes_by_table={},
                redundant_indexes=[],
                unused_indexes=[],
                high_value_indexes=[],
                indexes_by_size=[],
            )