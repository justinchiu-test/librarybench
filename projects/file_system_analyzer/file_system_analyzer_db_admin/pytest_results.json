{"created": 1747193952.9308643, "duration": 0.47705721855163574, "exitcode": 1, "root": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin", "environment": {}, "summary": {"passed": 34, "failed": 12, "total": 46, "collected": 46}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests", "type": "Dir"}]}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py", "outcome": "passed", "result": [{"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyzer_initialization", "type": "Function", "lineno": 13}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_calculate_compression_metrics", "type": "Function", "lineno": 22}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_compare_algorithms", "type": "Function", "lineno": 53}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyze_retention_efficiency", "type": "Function", "lineno": 86}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_generate_recommendations", "type": "Function", "lineno": 116}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyze_backup_compression", "type": "Function", "lineno": 173}]}, {"nodeid": "tests/backup_compression", "outcome": "passed", "result": [{"nodeid": "tests/backup_compression/test_compression_analyzer.py", "type": "Module"}]}, {"nodeid": "tests/db_file_recognition/test_detector.py", "outcome": "passed", "result": [{"nodeid": "tests/db_file_recognition/test_detector.py::test_detector_initialization", "type": "Function", "lineno": 11}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_detect_engine_from_path", "type": "Function", "lineno": 19}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_detect_category_from_path", "type": "Function", "lineno": 39}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_analyze_file", "type": "Function", "lineno": 61}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_scan_directory", "type": "Function", "lineno": 87}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_invalid_directory", "type": "Function", "lineno": 125}]}, {"nodeid": "tests/db_file_recognition", "outcome": "passed", "result": [{"nodeid": "tests/db_file_recognition/test_detector.py", "type": "Module"}]}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py", "outcome": "passed", "result": [{"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_analyzer_initialization", "type": "Function", "lineno": 12}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_detect_redundant_indexes", "type": "Function", "lineno": 20}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_calculate_space_performance_ratio", "type": "Function", "lineno": 40}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_detect_unused_indexes", "type": "Function", "lineno": 62}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_calculate_index_metrics", "type": "Function", "lineno": 80}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_categorize_indexes", "type": "Function", "lineno": 107}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_generate_recommendations", "type": "Function", "lineno": 130}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_analyze_indexes", "type": "Function", "lineno": 160}]}, {"nodeid": "tests/index_efficiency", "outcome": "passed", "result": [{"nodeid": "tests/index_efficiency/test_index_analyzer.py", "type": "Module"}]}, {"nodeid": "tests/interfaces/test_api.py", "outcome": "passed", "result": [{"nodeid": "tests/interfaces/test_api.py::test_api_initialization", "type": "Function", "lineno": 15}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_database_files", "type": "Function", "lineno": 31}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_transaction_logs", "type": "Function", "lineno": 76}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_index_efficiency", "type": "Function", "lineno": 105}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_tablespace_fragmentation", "type": "Function", "lineno": 137}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_backup_compression", "type": "Function", "lineno": 167}, {"nodeid": "tests/interfaces/test_api.py::test_comprehensive_analysis", "type": "Function", "lineno": 193}]}, {"nodeid": "tests/interfaces", "outcome": "passed", "result": [{"nodeid": "tests/interfaces/test_api.py", "type": "Module"}]}, {"nodeid": "tests/mock_data/mongodb/backups", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mongodb/config", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mongodb/data", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mongodb/journal", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mongodb/log", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mongodb", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/mongodb/backups", "type": "Dir"}, {"nodeid": "tests/mock_data/mongodb/config", "type": "Dir"}, {"nodeid": "tests/mock_data/mongodb/data", "type": "Dir"}, {"nodeid": "tests/mock_data/mongodb/journal", "type": "Dir"}, {"nodeid": "tests/mock_data/mongodb/log", "type": "Dir"}]}, {"nodeid": "tests/mock_data/mssql", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mysql/backups", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mysql/config", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mysql/data", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mysql/logs", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/mysql", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/mysql/backups", "type": "Dir"}, {"nodeid": "tests/mock_data/mysql/config", "type": "Dir"}, {"nodeid": "tests/mock_data/mysql/data", "type": "Dir"}, {"nodeid": "tests/mock_data/mysql/logs", "type": "Dir"}]}, {"nodeid": "tests/mock_data/oracle", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql/backups", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql/config", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql/data/base/16384", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql/data/base", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/postgresql/data/base/16384", "type": "Dir"}]}, {"nodeid": "tests/mock_data/postgresql/data", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/postgresql/data/base", "type": "Dir"}]}, {"nodeid": "tests/mock_data/postgresql/pg_log", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql/pg_xlog", "outcome": "passed", "result": []}, {"nodeid": "tests/mock_data/postgresql", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/postgresql/backups", "type": "Dir"}, {"nodeid": "tests/mock_data/postgresql/config", "type": "Dir"}, {"nodeid": "tests/mock_data/postgresql/data", "type": "Dir"}, {"nodeid": "tests/mock_data/postgresql/pg_log", "type": "Dir"}, {"nodeid": "tests/mock_data/postgresql/pg_xlog", "type": "Dir"}]}, {"nodeid": "tests/mock_data", "outcome": "passed", "result": [{"nodeid": "tests/mock_data/mongodb", "type": "Dir"}, {"nodeid": "tests/mock_data/mssql", "type": "Dir"}, {"nodeid": "tests/mock_data/mysql", "type": "Dir"}, {"nodeid": "tests/mock_data/oracle", "type": "Dir"}, {"nodeid": "tests/mock_data/postgresql", "type": "Dir"}]}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py", "outcome": "passed", "result": [{"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyzer_initialization", "type": "Function", "lineno": 15}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_detect_fragmentation_severity", "type": "Function", "lineno": 24}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyze_free_space_distribution", "type": "Function", "lineno": 40}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_estimate_reorganization_benefit", "type": "Function", "lineno": 64}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_calculate_optimal_fill_factor", "type": "Function", "lineno": 97}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_generate_visualization_data", "type": "Function", "lineno": 123}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_generate_recommendations", "type": "Function", "lineno": 149}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyze_tablespace_fragmentation", "type": "Function", "lineno": 220}]}, {"nodeid": "tests/tablespace_fragmentation", "outcome": "passed", "result": [{"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py", "type": "Module"}]}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py", "outcome": "passed", "result": [{"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyzer_initialization", "type": "Function", "lineno": 14}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyze_log_files", "type": "Function", "lineno": 22}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_detect_growth_pattern", "type": "Function", "lineno": 41}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_correlate_with_operations", "type": "Function", "lineno": 92}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_generate_retention_recommendations", "type": "Function", "lineno": 143}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyze_logs", "type": "Function", "lineno": 170}]}, {"nodeid": "tests/transaction_log_analysis", "outcome": "passed", "result": [{"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py", "type": "Module"}]}, {"nodeid": "tests/utils/test_file_utils.py", "outcome": "passed", "result": [{"nodeid": "tests/utils/test_file_utils.py::test_get_file_stats", "type": "Function", "lineno": 16}, {"nodeid": "tests/utils/test_file_utils.py::test_find_files", "type": "Function", "lineno": 43}, {"nodeid": "tests/utils/test_file_utils.py::test_calculate_dir_size", "type": "Function", "lineno": 70}, {"nodeid": "tests/utils/test_file_utils.py::test_get_disk_usage", "type": "Function", "lineno": 87}, {"nodeid": "tests/utils/test_file_utils.py::test_estimate_file_growth_rate", "type": "Function", "lineno": 104}]}, {"nodeid": "tests/utils", "outcome": "passed", "result": [{"nodeid": "tests/utils/test_file_utils.py", "type": "Module"}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/backup_compression", "type": "Dir"}, {"nodeid": "tests/db_file_recognition", "type": "Dir"}, {"nodeid": "tests/index_efficiency", "type": "Dir"}, {"nodeid": "tests/interfaces", "type": "Dir"}, {"nodeid": "tests/mock_data", "type": "Dir"}, {"nodeid": "tests/tablespace_fragmentation", "type": "Dir"}, {"nodeid": "tests/transaction_log_analysis", "type": "Dir"}, {"nodeid": "tests/utils", "type": "Dir"}]}], "tests": [{"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyzer_initialization", "lineno": 13, "outcome": "passed", "keywords": ["test_analyzer_initialization", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0004691079957410693, "outcome": "passed"}, "call": {"duration": 0.00031438993755728006, "outcome": "passed"}, "teardown": {"duration": 0.0002792080631479621, "outcome": "passed"}}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_calculate_compression_metrics", "lineno": 22, "outcome": "passed", "keywords": ["test_calculate_compression_metrics", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0004005120135843754, "outcome": "passed"}, "call": {"duration": 0.0003686740528792143, "outcome": "passed"}, "teardown": {"duration": 0.00021136505529284477, "outcome": "passed"}}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_compare_algorithms", "lineno": 53, "outcome": "passed", "keywords": ["test_compare_algorithms", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00026331294793635607, "outcome": "passed"}, "call": {"duration": 0.0005319779738783836, "outcome": "passed"}, "teardown": {"duration": 0.00018824497237801552, "outcome": "passed"}}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyze_retention_efficiency", "lineno": 86, "outcome": "failed", "keywords": ["test_analyze_retention_efficiency", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00026506802532821894, "outcome": "passed"}, "call": {"duration": 0.0010032349964603782, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/backup_compression/test_compression_analyzer.py", "lineno": 102, "message": "AssertionError: assert 'retention_periods' in {'differential_backup_metrics': {'avg_compression_ratio': 0, 'avg_duration_seconds': 0, 'avg_size_bytes': 0, 'count': ...size_bytes': 771751936.0, 'count': 4, ...}, 'full_vs_differential_ratio': None, 'full_vs_incremental_ratio': 23.0, ...}"}, "traceback": [{"path": "tests/backup_compression/test_compression_analyzer.py", "lineno": 102, "message": "AssertionError"}], "longrepr": "mock_backup_files = [{'backup_date': datetime.datetime(2025, 4, 14, 3, 39, 12, 548480), 'backup_duration_seconds': 3600, 'backup_strategy'...'backup_strategy': <BackupStrategy.FULL: 'full'>, 'compression_algorithm': <CompressionAlgorithm.BZIP2: 'bzip2'>, ...}]\n\n    def test_analyze_retention_efficiency(mock_backup_files):\n        \"\"\"Test analysis of backup retention efficiency.\"\"\"\n        analyzer = BackupCompressionAnalyzer()\n    \n        from file_system_analyzer_db_admin.backup_compression.compression_analyzer import BackupInfo\n    \n        # Create backup objects from mock data\n        backup_objects = []\n        for backup_data in mock_backup_files:\n            backup_objects.append(BackupInfo(**backup_data))\n    \n        # Analyze retention\n        retention_analysis = analyzer.analyze_retention_efficiency(backup_objects)\n    \n        # Check retention analysis\n>       assert \"retention_periods\" in retention_analysis\nE       AssertionError: assert 'retention_periods' in {'differential_backup_metrics': {'avg_compression_ratio': 0, 'avg_duration_seconds': 0, 'avg_size_bytes': 0, 'count': ...size_bytes': 771751936.0, 'count': 4, ...}, 'full_vs_differential_ratio': None, 'full_vs_incremental_ratio': 23.0, ...}\n\ntests/backup_compression/test_compression_analyzer.py:102: AssertionError"}, "teardown": {"duration": 0.00023142306599766016, "outcome": "passed"}}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_generate_recommendations", "lineno": 116, "outcome": "failed", "keywords": ["test_generate_recommendations", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00027615693397819996, "outcome": "passed"}, "call": {"duration": 0.0007993750041350722, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/backup_compression/test_compression_analyzer.py", "lineno": 139, "message": "assert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/backup_compression/test_compression_analyzer.py", "lineno": 139, "message": "AssertionError"}], "longrepr": "mock_backup_files = [{'backup_date': datetime.datetime(2025, 4, 14, 3, 39, 12, 570693), 'backup_duration_seconds': 3600, 'backup_strategy'...'backup_strategy': <BackupStrategy.FULL: 'full'>, 'compression_algorithm': <CompressionAlgorithm.BZIP2: 'bzip2'>, ...}]\n\n    def test_generate_recommendations(mock_backup_files):\n        \"\"\"Test generation of backup optimization recommendations.\"\"\"\n        analyzer = BackupCompressionAnalyzer()\n    \n        from file_system_analyzer_db_admin.backup_compression.compression_analyzer import BackupInfo\n    \n        # Create backup objects from mock data\n        backup_objects = []\n        for backup_data in mock_backup_files:\n            backup_objects.append(BackupInfo(**backup_data))\n    \n        # Compare algorithms\n        comparisons = {\n            \"mysql\": analyzer.compare_algorithms(backup_objects, filter_engine=\"mysql\"),\n            \"postgresql\": analyzer.compare_algorithms(backup_objects, filter_engine=\"postgresql\"),\n            \"mongodb\": analyzer.compare_algorithms(backup_objects, filter_engine=\"mongodb\"),\n        }\n    \n        # Generate recommendations\n        recommendations = analyzer.generate_recommendations(backup_objects, comparisons)\n    \n        # Check recommendations\n>       assert len(recommendations) > 0\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/backup_compression/test_compression_analyzer.py:139: AssertionError"}, "teardown": {"duration": 0.00022904900833964348, "outcome": "passed"}}, {"nodeid": "tests/backup_compression/test_compression_analyzer.py::test_analyze_backup_compression", "lineno": 173, "outcome": "failed", "keywords": ["test_analyze_backup_compression", "test_compression_analyzer.py", "backup_compression", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0002721439814195037, "outcome": "passed"}, "call": {"duration": 0.0019903091015294194, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/backup_compression/test_compression_analyzer.py", "lineno": 189, "message": "AssertionError: assert <ScanStatus.FAILED: 'failed'> == <ScanStatus.C...: 'completed'>\n  \n  - completed\n  + failed"}, "traceback": [{"path": "tests/backup_compression/test_compression_analyzer.py", "lineno": 189, "message": "AssertionError"}], "log": [{"name": "file_system_analyzer_db_admin.backup_compression.compression_analyzer", "msg": "Error in backup compression analysis: 6 validation errors for BackupCompressionAnalysisResult\nbackups_by_algorithm.`CompressionAlgorithm.GZIP`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.GZIP', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.NONE`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.NONE', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.SNAPPY`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.SNAPPY', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.BZIP2`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.BZIP2', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_strategy.`BackupStrategy.FULL`.[key]\n  Input should be 'full', 'incremental', 'differential', 'transaction_log' or 'mixed' [type=enum, input_value='BackupStrategy.FULL', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_strategy.`BackupStrategy.INCREMENTAL`.[key]\n  Input should be 'full', 'incremental', 'differential', 'transaction_log' or 'mixed' [type=enum, input_value='BackupStrategy.INCREMENTAL', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/backup_compression/compression_analyzer.py", "filename": "compression_analyzer.py", "module": "compression_analyzer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 718, "funcName": "analyze_backup_compression", "created": 1747193952.579702, "msecs": 579.0, "relativeCreated": 662.5027656555176, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}], "longrepr": "mock_backup_files = [{'backup_date': datetime.datetime(2025, 4, 14, 3, 39, 12, 578441), 'backup_duration_seconds': 3600, 'backup_strategy'...'backup_strategy': <BackupStrategy.FULL: 'full'>, 'compression_algorithm': <CompressionAlgorithm.BZIP2: 'bzip2'>, ...}]\n\n    def test_analyze_backup_compression(mock_backup_files):\n        \"\"\"Test full backup compression analysis workflow.\"\"\"\n        analyzer = BackupCompressionAnalyzer()\n    \n        from file_system_analyzer_db_admin.backup_compression.compression_analyzer import BackupInfo\n    \n        # Create backup objects from mock data\n        backup_objects = []\n        for backup_data in mock_backup_files:\n            backup_objects.append(BackupInfo(**backup_data))\n    \n        # Run analysis\n        analysis_result = analyzer.analyze_backup_compression(backup_objects)\n    \n        # Check result\n>       assert analysis_result.scan_status == ScanStatus.COMPLETED\nE       AssertionError: assert <ScanStatus.FAILED: 'failed'> == <ScanStatus.C...: 'completed'>\nE         \nE         - completed\nE         + failed\n\ntests/backup_compression/test_compression_analyzer.py:189: AssertionError"}, "teardown": {"duration": 0.00023074401542544365, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_detector_initialization", "lineno": 11, "outcome": "passed", "keywords": ["test_detector_initialization", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00022025697398930788, "outcome": "passed"}, "call": {"duration": 0.00021656707394868135, "outcome": "passed"}, "teardown": {"duration": 0.0001616950612515211, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_detect_engine_from_path", "lineno": 19, "outcome": "passed", "keywords": ["test_detect_engine_from_path", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0001802908955141902, "outcome": "passed"}, "call": {"duration": 0.0005841871025040746, "outcome": "passed"}, "teardown": {"duration": 0.0001597400987520814, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_detect_category_from_path", "lineno": 39, "outcome": "failed", "keywords": ["test_detect_category_from_path", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00017221900634467602, "outcome": "passed"}, "call": {"duration": 0.0007547719869762659, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/db_file_recognition/test_detector.py", "lineno": 52, "message": "AssertionError: assert <FileCategory.DATA: 'data'> == <FileCategory.LOG: 'log'>\n  \n  - log\n  + data"}, "traceback": [{"path": "tests/db_file_recognition/test_detector.py", "lineno": 52, "message": "AssertionError"}], "longrepr": "def test_detect_category_from_path():\n        \"\"\"Test category detection from file path.\"\"\"\n        detector = DatabaseFileDetector()\n    \n        # MySQL files\n        assert detector.detect_category_from_path(Path(\"/var/lib/mysql/data/users.ibd\"), DatabaseEngine.MYSQL) == FileCategory.DATA\n        assert detector.detect_category_from_path(Path(\"/var/lib/mysql/mysql-bin.000001\"), DatabaseEngine.MYSQL) == FileCategory.LOG\n        assert detector.detect_category_from_path(Path(\"/var/lib/mysql/backup.sql\"), DatabaseEngine.MYSQL) == FileCategory.BACKUP\n        assert detector.detect_category_from_path(Path(\"/var/lib/mysql/my.cnf\"), DatabaseEngine.MYSQL) == FileCategory.CONFIG\n    \n        # PostgreSQL files\n        assert detector.detect_category_from_path(Path(\"/var/lib/postgresql/data/base/16384/16385\"), DatabaseEngine.POSTGRESQL) == FileCategory.DATA\n>       assert detector.detect_category_from_path(Path(\"/var/lib/postgresql/pg_wal/000000010000000000000001\"), DatabaseEngine.POSTGRESQL) == FileCategory.LOG\nE       AssertionError: assert <FileCategory.DATA: 'data'> == <FileCategory.LOG: 'log'>\nE         \nE         - log\nE         + data\n\ntests/db_file_recognition/test_detector.py:52: AssertionError"}, "teardown": {"duration": 0.00021038600243628025, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_analyze_file", "lineno": 61, "outcome": "passed", "keywords": ["test_analyze_file", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0002771309809759259, "outcome": "passed"}, "call": {"duration": 0.0010764061007648706, "outcome": "passed"}, "teardown": {"duration": 0.00018686894327402115, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_scan_directory", "lineno": 87, "outcome": "passed", "keywords": ["test_scan_directory", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0002667499938979745, "outcome": "passed"}, "call": {"duration": 0.023067336063832045, "outcome": "passed"}, "teardown": {"duration": 0.00019720301497727633, "outcome": "passed"}}, {"nodeid": "tests/db_file_recognition/test_detector.py::test_invalid_directory", "lineno": 125, "outcome": "passed", "keywords": ["test_invalid_directory", "test_detector.py", "db_file_recognition", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00018557009752839804, "outcome": "passed"}, "call": {"duration": 0.0002934979274868965, "outcome": "passed"}, "teardown": {"duration": 0.00015855301171541214, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_analyzer_initialization", "lineno": 12, "outcome": "passed", "keywords": ["test_analyzer_initialization", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00019834493286907673, "outcome": "passed"}, "call": {"duration": 0.0002177039859816432, "outcome": "passed"}, "teardown": {"duration": 0.0001656099921092391, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_detect_redundant_indexes", "lineno": 20, "outcome": "passed", "keywords": ["test_detect_redundant_indexes", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00028956704773008823, "outcome": "passed"}, "call": {"duration": 0.00027563702315092087, "outcome": "passed"}, "teardown": {"duration": 0.00018250895664095879, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_calculate_space_performance_ratio", "lineno": 40, "outcome": "passed", "keywords": ["test_calculate_space_performance_ratio", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00028895295690745115, "outcome": "passed"}, "call": {"duration": 0.002715575974434614, "outcome": "passed"}, "teardown": {"duration": 0.0002080181147903204, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_detect_unused_indexes", "lineno": 62, "outcome": "passed", "keywords": ["test_detect_unused_indexes", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00032410898711532354, "outcome": "passed"}, "call": {"duration": 0.000237233005464077, "outcome": "passed"}, "teardown": {"duration": 0.00017952790949493647, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_calculate_index_metrics", "lineno": 80, "outcome": "failed", "keywords": ["test_calculate_index_metrics", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00027932191733270884, "outcome": "passed"}, "call": {"duration": 0.00032216403633356094, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError: name 'indexes_by_table' is not defined"}, "traceback": [{"path": "tests/index_efficiency/test_index_analyzer.py", "lineno": 86, "message": ""}, {"path": "file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError"}], "longrepr": "mock_indexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\n\n    def test_calculate_index_metrics(mock_indexes):\n        \"\"\"Test calculation of index metrics.\"\"\"\n        analyzer = IndexEfficiencyAnalyzer()\n        table_sizes = {\"users\": 1073741824, \"products\": 536870912}  # 1GB, 512MB\n    \n>       metrics = analyzer.calculate_index_metrics(mock_indexes, table_sizes)\n\ntests/index_efficiency/test_index_analyzer.py:86: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <file_system_analyzer_db_admin.index_efficiency.index_analyzer.IndexEfficiencyAnalyzer object at 0x7fda0b5fe600>\nindexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\ntables_sizes = {'products': 536870912, 'users': 1073741824}\n\n    def calculate_index_metrics(\n        self,\n        indexes: List[IndexInfo],\n        tables_sizes: Dict[str, int] = None\n    ) -> Dict[str, IndexMetrics]:\n        \"\"\"\n        Calculate metrics for each index to determine optimization potential.\n    \n        Args:\n            indexes: List of index information\n            tables_sizes: Dictionary mapping table names to sizes in bytes\n    \n        Returns:\n            Dictionary mapping index names to metrics\n        \"\"\"\n        metrics = {}\n        redundant_map = self.detect_redundant_indexes(indexes)\n        unused_indexes = self.detect_unused_indexes(indexes)\n    \n        # Dictionary for fast lookup\n        indexes_dict = {idx.name: idx for idx in indexes}\n    \n        # Calculate metrics for each index\n        for idx in indexes:\n            table_size = tables_sizes.get(idx.table_name) if tables_sizes else None\n    \n            # Calculate space to performance ratio\n            space_perf_ratio = self.calculate_space_performance_ratio(idx, table_size)\n    \n            # Calculate redundancy score (0-1)\n            # Higher score means more redundant\n            redundant_with = redundant_map.get(idx.name, [])\n>           redundancy_score = len(redundant_with) / max(1, len(indexes_by_table[idx.table_name]) - 1)\nE           NameError: name 'indexes_by_table' is not defined\n\nfile_system_analyzer_db_admin/index_efficiency/index_analyzer.py:314: NameError"}, "teardown": {"duration": 0.0002428439911454916, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_categorize_indexes", "lineno": 107, "outcome": "failed", "keywords": ["test_categorize_indexes", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0003075948916375637, "outcome": "passed"}, "call": {"duration": 0.00028636003844439983, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError: name 'indexes_by_table' is not defined"}, "traceback": [{"path": "tests/index_efficiency/test_index_analyzer.py", "lineno": 113, "message": ""}, {"path": "file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError"}], "longrepr": "mock_indexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\n\n    def test_categorize_indexes(mock_indexes):\n        \"\"\"Test categorization of indexes by value.\"\"\"\n        analyzer = IndexEfficiencyAnalyzer()\n        table_sizes = {\"users\": 1073741824, \"products\": 536870912}  # 1GB, 512MB\n    \n>       metrics = analyzer.calculate_index_metrics(mock_indexes, table_sizes)\n\ntests/index_efficiency/test_index_analyzer.py:113: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <file_system_analyzer_db_admin.index_efficiency.index_analyzer.IndexEfficiencyAnalyzer object at 0x7fda0b5fdd00>\nindexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\ntables_sizes = {'products': 536870912, 'users': 1073741824}\n\n    def calculate_index_metrics(\n        self,\n        indexes: List[IndexInfo],\n        tables_sizes: Dict[str, int] = None\n    ) -> Dict[str, IndexMetrics]:\n        \"\"\"\n        Calculate metrics for each index to determine optimization potential.\n    \n        Args:\n            indexes: List of index information\n            tables_sizes: Dictionary mapping table names to sizes in bytes\n    \n        Returns:\n            Dictionary mapping index names to metrics\n        \"\"\"\n        metrics = {}\n        redundant_map = self.detect_redundant_indexes(indexes)\n        unused_indexes = self.detect_unused_indexes(indexes)\n    \n        # Dictionary for fast lookup\n        indexes_dict = {idx.name: idx for idx in indexes}\n    \n        # Calculate metrics for each index\n        for idx in indexes:\n            table_size = tables_sizes.get(idx.table_name) if tables_sizes else None\n    \n            # Calculate space to performance ratio\n            space_perf_ratio = self.calculate_space_performance_ratio(idx, table_size)\n    \n            # Calculate redundancy score (0-1)\n            # Higher score means more redundant\n            redundant_with = redundant_map.get(idx.name, [])\n>           redundancy_score = len(redundant_with) / max(1, len(indexes_by_table[idx.table_name]) - 1)\nE           NameError: name 'indexes_by_table' is not defined\n\nfile_system_analyzer_db_admin/index_efficiency/index_analyzer.py:314: NameError"}, "teardown": {"duration": 0.00024973799008876085, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_generate_recommendations", "lineno": 130, "outcome": "failed", "keywords": ["test_generate_recommendations", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00029445800464600325, "outcome": "passed"}, "call": {"duration": 0.0002803630195558071, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError: name 'indexes_by_table' is not defined"}, "traceback": [{"path": "tests/index_efficiency/test_index_analyzer.py", "lineno": 136, "message": ""}, {"path": "file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "lineno": 314, "message": "NameError"}], "longrepr": "mock_indexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\n\n    def test_generate_recommendations(mock_indexes):\n        \"\"\"Test generation of optimization recommendations.\"\"\"\n        analyzer = IndexEfficiencyAnalyzer()\n        table_sizes = {\"users\": 1073741824, \"products\": 536870912}  # 1GB, 512MB\n    \n>       metrics = analyzer.calculate_index_metrics(mock_indexes, table_sizes)\n\ntests/index_efficiency/test_index_analyzer.py:136: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <file_system_analyzer_db_admin.index_efficiency.index_analyzer.IndexEfficiencyAnalyzer object at 0x7fda0b5fd430>\nindexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\ntables_sizes = {'products': 536870912, 'users': 1073741824}\n\n    def calculate_index_metrics(\n        self,\n        indexes: List[IndexInfo],\n        tables_sizes: Dict[str, int] = None\n    ) -> Dict[str, IndexMetrics]:\n        \"\"\"\n        Calculate metrics for each index to determine optimization potential.\n    \n        Args:\n            indexes: List of index information\n            tables_sizes: Dictionary mapping table names to sizes in bytes\n    \n        Returns:\n            Dictionary mapping index names to metrics\n        \"\"\"\n        metrics = {}\n        redundant_map = self.detect_redundant_indexes(indexes)\n        unused_indexes = self.detect_unused_indexes(indexes)\n    \n        # Dictionary for fast lookup\n        indexes_dict = {idx.name: idx for idx in indexes}\n    \n        # Calculate metrics for each index\n        for idx in indexes:\n            table_size = tables_sizes.get(idx.table_name) if tables_sizes else None\n    \n            # Calculate space to performance ratio\n            space_perf_ratio = self.calculate_space_performance_ratio(idx, table_size)\n    \n            # Calculate redundancy score (0-1)\n            # Higher score means more redundant\n            redundant_with = redundant_map.get(idx.name, [])\n>           redundancy_score = len(redundant_with) / max(1, len(indexes_by_table[idx.table_name]) - 1)\nE           NameError: name 'indexes_by_table' is not defined\n\nfile_system_analyzer_db_admin/index_efficiency/index_analyzer.py:314: NameError"}, "teardown": {"duration": 0.00023337604943662882, "outcome": "passed"}}, {"nodeid": "tests/index_efficiency/test_index_analyzer.py::test_analyze_indexes", "lineno": 160, "outcome": "failed", "keywords": ["test_analyze_indexes", "test_index_analyzer.py", "index_efficiency", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00029120000544935465, "outcome": "passed"}, "call": {"duration": 0.000771110993809998, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/index_efficiency/test_index_analyzer.py", "lineno": 169, "message": "AssertionError: assert <ScanStatus.FAILED: 'failed'> == <ScanStatus.C...: 'completed'>\n  \n  - completed\n  + failed"}, "traceback": [{"path": "tests/index_efficiency/test_index_analyzer.py", "lineno": 169, "message": "AssertionError"}], "log": [{"name": "file_system_analyzer_db_admin.index_efficiency.index_analyzer", "msg": "Error in index analysis: name 'indexes_by_table' is not defined", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "filename": "index_analyzer.py", "module": "index_analyzer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 682, "funcName": "analyze_indexes", "created": 1747193952.6924381, "msecs": 692.0, "relativeCreated": 775.2389907836914, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}], "longrepr": "mock_indexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\n\n    def test_analyze_indexes(mock_indexes):\n        \"\"\"Test full index analysis workflow.\"\"\"\n        analyzer = IndexEfficiencyAnalyzer()\n        table_sizes = {\"users\": 1073741824, \"products\": 536870912}  # 1GB, 512MB\n    \n        analysis_result = analyzer.analyze_indexes(mock_indexes, table_sizes)\n    \n        # Check result\n>       assert analysis_result.scan_status == ScanStatus.COMPLETED\nE       AssertionError: assert <ScanStatus.FAILED: 'failed'> == <ScanStatus.C...: 'completed'>\nE         \nE         - completed\nE         + failed\n\ntests/index_efficiency/test_index_analyzer.py:169: AssertionError"}, "teardown": {"duration": 0.00022732000797986984, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_api_initialization", "lineno": 15, "outcome": "passed", "keywords": ["test_api_initialization", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0005646439967676997, "outcome": "passed"}, "call": {"duration": 0.00032618094701319933, "outcome": "passed"}, "teardown": {"duration": 0.00043970008846372366, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_database_files", "lineno": 31, "outcome": "passed", "keywords": ["test_analyze_database_files", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00041884405072778463, "outcome": "passed"}, "call": {"duration": 0.009417287074029446, "outcome": "passed"}, "teardown": {"duration": 0.0003067130455747247, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_transaction_logs", "lineno": 76, "outcome": "passed", "keywords": ["test_analyze_transaction_logs", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0004718269919976592, "outcome": "passed"}, "call": {"duration": 0.0029359880136325955, "outcome": "passed", "log": [{"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/mysql/mysql-bin.000001: [Errno 2] No such file or directory: '/var/lib/mysql/mysql-bin.000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7130384, "msecs": 713.0, "relativeCreated": 795.8393096923828, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/postgresql/data/pg_wal/000000010000000A00000001: [Errno 2] No such file or directory: '/var/lib/postgresql/data/pg_wal/000000010000000A00000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7139874, "msecs": 713.0, "relativeCreated": 796.788215637207, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting disk usage for /var/lib/mysql: [Errno 2] No such file or directory: '/var/lib/mysql'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 216, "funcName": "get_disk_usage", "created": 1747193952.7141583, "msecs": 714.0, "relativeCreated": 796.9591617584229, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}]}, "teardown": {"duration": 0.00029386207461357117, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_index_efficiency", "lineno": 105, "outcome": "failed", "keywords": ["test_analyze_index_efficiency", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0003993960563093424, "outcome": "passed"}, "call": {"duration": 0.0012216530740261078, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 127, "message": "AssertionError: assert 'failed' == 'completed'\n  \n  - completed\n  + failed"}, "traceback": [{"path": "tests/interfaces/test_api.py", "lineno": 127, "message": "AssertionError"}], "log": [{"name": "file_system_analyzer_db_admin.index_efficiency.index_analyzer", "msg": "Error in index analysis: name 'indexes_by_table' is not defined", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/index_efficiency/index_analyzer.py", "filename": "index_analyzer.py", "module": "index_analyzer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 682, "funcName": "analyze_indexes", "created": 1747193952.7175457, "msecs": 717.0, "relativeCreated": 800.3466129302979, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}], "longrepr": "mock_indexes = [IndexInfo(name='users_pk', table_name='users', columns=['id'], size_bytes=16777216, is_unique=True, is_primary=True, ...=9.0, database_name='myapp', schema_name='public', engine=<DatabaseEngine.POSTGRESQL: 'postgresql'>, metadata={}), ...]\ntemp_output_dir = '/tmp/tmpzbi08r68'\n\n    def test_analyze_index_efficiency(mock_indexes, temp_output_dir):\n        \"\"\"Test index efficiency analysis through API.\"\"\"\n        api = StorageOptimizerAPI(output_dir=temp_output_dir)\n    \n        # Convert indexes to dictionaries\n        indexes = [idx.dict() for idx in mock_indexes]\n        assert len(indexes) > 0\n    \n        # Sample table sizes\n        table_sizes = {\"users\": 1073741824, \"products\": 536870912}  # 1GB, 512MB\n    \n        # Analyze indexes\n        index_result = api.analyze_index_efficiency(\n            indexes=indexes,\n            tables_sizes=table_sizes,\n            export_format=\"json\",\n            export_filename=\"index_analysis.json\",\n        )\n    \n        # Check results\n        assert index_result is not None\n>       assert index_result.get(\"scan_status\") == \"completed\"\nE       AssertionError: assert 'failed' == 'completed'\nE         \nE         - completed\nE         + failed\n\ntests/interfaces/test_api.py:127: AssertionError"}, "teardown": {"duration": 0.00030259392224252224, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_tablespace_fragmentation", "lineno": 137, "outcome": "passed", "keywords": ["test_analyze_tablespace_fragmentation", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0004571779863908887, "outcome": "passed"}, "call": {"duration": 0.004409044981002808, "outcome": "passed"}, "teardown": {"duration": 0.00029975303914397955, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_analyze_backup_compression", "lineno": 167, "outcome": "failed", "keywords": ["test_analyze_backup_compression", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.000378024997189641, "outcome": "passed"}, "call": {"duration": 0.001801869017072022, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 181, "message": "AssertionError: assert 'failed' == 'completed'\n  \n  - completed\n  + failed"}, "traceback": [{"path": "tests/interfaces/test_api.py", "lineno": 181, "message": "AssertionError"}], "log": [{"name": "file_system_analyzer_db_admin.backup_compression.compression_analyzer", "msg": "Error in backup compression analysis: 6 validation errors for BackupCompressionAnalysisResult\nbackups_by_algorithm.`CompressionAlgorithm.GZIP`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.GZIP', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.NONE`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.NONE', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.SNAPPY`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.SNAPPY', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_algorithm.`CompressionAlgorithm.BZIP2`.[key]\n  Input should be 'gzip', 'bzip2', 'lz4', 'zstd', 'xz', 'snappy', 'native', 'custom' or 'none' [type=enum, input_value='CompressionAlgorithm.BZIP2', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_strategy.`BackupStrategy.FULL`.[key]\n  Input should be 'full', 'incremental', 'differential', 'transaction_log' or 'mixed' [type=enum, input_value='BackupStrategy.FULL', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum\nbackups_by_strategy.`BackupStrategy.INCREMENTAL`.[key]\n  Input should be 'full', 'incremental', 'differential', 'transaction_log' or 'mixed' [type=enum, input_value='BackupStrategy.INCREMENTAL', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/enum", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/backup_compression/compression_analyzer.py", "filename": "compression_analyzer.py", "module": "compression_analyzer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 718, "funcName": "analyze_backup_compression", "created": 1747193952.7329135, "msecs": 732.0, "relativeCreated": 815.7143592834473, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}], "longrepr": "mock_backup_files = [{'backup_date': datetime.datetime(2025, 4, 14, 3, 39, 12, 731576), 'backup_duration_seconds': 3600, 'backup_strategy'...'backup_strategy': <BackupStrategy.FULL: 'full'>, 'compression_algorithm': <CompressionAlgorithm.BZIP2: 'bzip2'>, ...}]\ntemp_output_dir = '/tmp/tmp5g8jywp5'\n\n    def test_analyze_backup_compression(mock_backup_files, temp_output_dir):\n        \"\"\"Test backup compression analysis through API.\"\"\"\n        api = StorageOptimizerAPI(output_dir=temp_output_dir)\n    \n        # Analyze backups\n        backup_result = api.analyze_backup_compression(\n            backups=mock_backup_files,\n            export_format=\"json\",\n            export_filename=\"backup_analysis.json\",\n        )\n    \n        # Check results\n        assert backup_result is not None\n>       assert backup_result.get(\"scan_status\") == \"completed\"\nE       AssertionError: assert 'failed' == 'completed'\nE         \nE         - completed\nE         + failed\n\ntests/interfaces/test_api.py:181: AssertionError"}, "teardown": {"duration": 0.0003057480789721012, "outcome": "passed"}}, {"nodeid": "tests/interfaces/test_api.py::test_comprehensive_analysis", "lineno": 193, "outcome": "passed", "keywords": ["test_comprehensive_analysis", "test_api.py", "interfaces", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0003958779852837324, "outcome": "passed"}, "call": {"duration": 0.016778037999756634, "outcome": "passed", "log": [{"name": "file_system_analyzer_db_admin.transaction_log_analysis.log_analyzer", "msg": "Error in log analysis: 8 validation errors for LogAnalysisResult\nlog_files.2.sequence_number.str\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\nlog_files.2.sequence_number.int\n  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_type\nlog_files.2.sequence_number.float\n  Input should be a valid number [type=float_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_type\nlog_files.2.sequence_number.datetime\n  Input should be a valid datetime [type=datetime_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/datetime_type\nlog_files.4.sequence_number.str\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type\nlog_files.4.sequence_number.int\n  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/int_type\nlog_files.4.sequence_number.float\n  Input should be a valid number [type=float_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_type\nlog_files.4.sequence_number.datetime\n  Input should be a valid datetime [type=datetime_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/datetime_type", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/transaction_log_analysis/log_analyzer.py", "filename": "log_analyzer.py", "module": "log_analyzer", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 660, "funcName": "analyze_logs", "created": 1747193952.7528136, "msecs": 752.0, "relativeCreated": 835.6144428253174, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.interfaces.api", "msg": "Error converting backup data: 3 validation errors for BackupInfo\ncompression_algorithm\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_date\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_strategy\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "filename": "api.py", "module": "api", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 385, "funcName": "analyze_backup_compression", "created": 1747193952.7530262, "msecs": 753.0, "relativeCreated": 835.827112197876, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.interfaces.api", "msg": "Error converting backup data: 3 validation errors for BackupInfo\ncompression_algorithm\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_date\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_strategy\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "filename": "api.py", "module": "api", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 385, "funcName": "analyze_backup_compression", "created": 1747193952.7531168, "msecs": 753.0, "relativeCreated": 835.9177112579346, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.interfaces.api", "msg": "Error converting backup data: 3 validation errors for BackupInfo\ncompression_algorithm\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_date\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nbackup_strategy\n  Field required [type=missing, input_value={'path': '/home/justinchi...: False, 'metadata': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "filename": "api.py", "module": "api", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 385, "funcName": "analyze_backup_compression", "created": 1747193952.7531905, "msecs": 753.0, "relativeCreated": 835.991382598877, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}]}, "teardown": {"duration": 0.00030098704155534506, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyzer_initialization", "lineno": 15, "outcome": "passed", "keywords": ["test_analyzer_initialization", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00020720798056572676, "outcome": "passed"}, "call": {"duration": 0.00021349696908146143, "outcome": "passed"}, "teardown": {"duration": 0.00015328300651162863, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_detect_fragmentation_severity", "lineno": 24, "outcome": "passed", "keywords": ["test_detect_fragmentation_severity", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00017317000310868025, "outcome": "passed"}, "call": {"duration": 0.00023684592451900244, "outcome": "passed"}, "teardown": {"duration": 0.00015239196363836527, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyze_free_space_distribution", "lineno": 40, "outcome": "failed", "keywords": ["test_analyze_free_space_distribution", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00016529997810721397, "outcome": "passed"}, "call": {"duration": 0.0007295720279216766, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/tablespace_fragmentation/test_fragmentation_analyzer.py", "lineno": 59, "message": "AssertionError: assert <SpaceDistrib...: 'clustered'> == <SpaceDistrib...D: 'depleted'>\n  \n  - depleted\n  + clustered"}, "traceback": [{"path": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py", "lineno": 59, "message": "AssertionError"}], "longrepr": "def test_analyze_free_space_distribution():\n        \"\"\"Test analysis of free space distribution patterns.\"\"\"\n        analyzer = TablespaceFragmentationAnalyzer()\n    \n        # Uniform distribution (one large chunk)\n        uniform_chunks = [1073741824]  # One 1GB chunk\n        assert analyzer.analyze_free_space_distribution(uniform_chunks, 1073741824) == SpaceDistributionType.UNIFORM\n    \n        # Clustered distribution (a few large chunks)\n        clustered_chunks = [805306368, 268435456]  # 768MB and 256MB chunks\n        assert analyzer.analyze_free_space_distribution(clustered_chunks, 1073741824) == SpaceDistributionType.CLUSTERED\n    \n        # Scattered distribution (many small chunks)\n        scattered_chunks = [1048576] * 100  # 100 chunks of 1MB each\n        assert analyzer.analyze_free_space_distribution(scattered_chunks, 104857600) == SpaceDistributionType.SCATTERED\n    \n        # Depleted (almost no free space)\n        depleted_chunks = [102400, 51200]  # Two tiny chunks (100KB, 50KB)\n>       assert analyzer.analyze_free_space_distribution(depleted_chunks, 153600) == SpaceDistributionType.DEPLETED\nE       AssertionError: assert <SpaceDistrib...: 'clustered'> == <SpaceDistrib...D: 'depleted'>\nE         \nE         - depleted\nE         + clustered\n\ntests/tablespace_fragmentation/test_fragmentation_analyzer.py:59: AssertionError"}, "teardown": {"duration": 0.00021081394515931606, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_estimate_reorganization_benefit", "lineno": 64, "outcome": "passed", "keywords": ["test_estimate_reorganization_benefit", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00018175400327891111, "outcome": "passed"}, "call": {"duration": 0.00022199004888534546, "outcome": "passed"}, "teardown": {"duration": 0.00015268893912434578, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_calculate_optimal_fill_factor", "lineno": 97, "outcome": "passed", "keywords": ["test_calculate_optimal_fill_factor", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00016943097580224276, "outcome": "passed"}, "call": {"duration": 0.00022382906172424555, "outcome": "passed"}, "teardown": {"duration": 0.0001485480461269617, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_generate_visualization_data", "lineno": 123, "outcome": "passed", "keywords": ["test_generate_visualization_data", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00033402303233742714, "outcome": "passed"}, "call": {"duration": 0.00034038908779621124, "outcome": "passed"}, "teardown": {"duration": 0.00021083501633256674, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_generate_recommendations", "lineno": 149, "outcome": "passed", "keywords": ["test_generate_recommendations", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00031986297108232975, "outcome": "passed"}, "call": {"duration": 0.0005750689888373017, "outcome": "passed"}, "teardown": {"duration": 0.0001970849698409438, "outcome": "passed"}}, {"nodeid": "tests/tablespace_fragmentation/test_fragmentation_analyzer.py::test_analyze_tablespace_fragmentation", "lineno": 220, "outcome": "passed", "keywords": ["test_analyze_tablespace_fragmentation", "test_fragmentation_analyzer.py", "tablespace_fragmentation", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00032775697764009237, "outcome": "passed"}, "call": {"duration": 0.0007123990217223763, "outcome": "passed"}, "teardown": {"duration": 0.00020506908185780048, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyzer_initialization", "lineno": 14, "outcome": "passed", "keywords": ["test_analyzer_initialization", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0002072569914162159, "outcome": "passed"}, "call": {"duration": 0.00021534902043640614, "outcome": "passed"}, "teardown": {"duration": 0.00014906201977282763, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyze_log_files", "lineno": 22, "outcome": "passed", "keywords": ["test_analyze_log_files", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00029287603683769703, "outcome": "passed"}, "call": {"duration": 0.0005071769701316953, "outcome": "passed", "log": [{"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/mysql/mysql-bin.000001: [Errno 2] No such file or directory: '/var/lib/mysql/mysql-bin.000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7782414, "msecs": 778.0, "relativeCreated": 861.0422611236572, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/postgresql/data/pg_wal/000000010000000A00000001: [Errno 2] No such file or directory: '/var/lib/postgresql/data/pg_wal/000000010000000A00000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7783675, "msecs": 778.0, "relativeCreated": 861.168384552002, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}]}, "teardown": {"duration": 0.00018425402231514454, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_detect_growth_pattern", "lineno": 41, "outcome": "passed", "keywords": ["test_detect_growth_pattern", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00018336600624024868, "outcome": "passed"}, "call": {"duration": 0.003651668084785342, "outcome": "passed"}, "teardown": {"duration": 0.00019699998665601015, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_correlate_with_operations", "lineno": 92, "outcome": "failed", "keywords": ["test_correlate_with_operations", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00019959593191742897, "outcome": "passed"}, "call": {"duration": 0.0011097859824076295, "outcome": "failed", "crash": {"path": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/transaction_log_analysis/test_log_analyzer.py", "lineno": 133, "message": "AssertionError: assert <LogGrowthCorrelation.NONE: 'none'> in [<LogGrowthCorrelation.LOW: 'low'>, <LogGrowthCorrelation.MEDIUM: 'medium'>]"}, "traceback": [{"path": "tests/transaction_log_analysis/test_log_analyzer.py", "lineno": 133, "message": "AssertionError"}], "longrepr": "def test_correlate_with_operations():\n        \"\"\"Test correlation of log growth with operations.\"\"\"\n        analyzer = TransactionLogAnalyzer()\n    \n        # Sample log growth rates\n        log_growth_rates = [10, 20, 30, 40, 50]\n    \n        # Sample operation frequencies that correlate well\n        high_correlation_ops = {\n            \"INSERT\": [100, 200, 300, 400, 500],  # Perfect correlation\n            \"UPDATE\": [90, 180, 270, 360, 450],   # Strong correlation\n        }\n    \n        # Sample operation frequencies with weak correlation\n        weak_correlation_ops = {\n            \"DELETE\": [10, 20, 10, 20, 10],       # Weak correlation\n            \"BACKUP\": [500, 100, 300, 200, 400],  # Negative correlation\n        }\n    \n        # Test correlations\n        high_corr_result = analyzer.correlate_with_operations(\n            DatabaseEngine.MYSQL,\n            log_growth_rates,\n            high_correlation_ops\n        )\n    \n        weak_corr_result = analyzer.correlate_with_operations(\n            DatabaseEngine.MYSQL,\n            log_growth_rates,\n            weak_correlation_ops\n        )\n    \n        # Check results\n        assert \"INSERT\" in high_corr_result\n        assert high_corr_result[\"INSERT\"] == LogGrowthCorrelation.HIGH\n    \n        assert \"UPDATE\" in high_corr_result\n        assert high_corr_result[\"UPDATE\"] == LogGrowthCorrelation.HIGH\n    \n        assert \"DELETE\" in weak_corr_result\n>       assert weak_corr_result[\"DELETE\"] in [LogGrowthCorrelation.LOW, LogGrowthCorrelation.MEDIUM]\nE       AssertionError: assert <LogGrowthCorrelation.NONE: 'none'> in [<LogGrowthCorrelation.LOW: 'low'>, <LogGrowthCorrelation.MEDIUM: 'medium'>]\n\ntests/transaction_log_analysis/test_log_analyzer.py:133: AssertionError"}, "teardown": {"duration": 0.00020995992235839367, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_generate_retention_recommendations", "lineno": 143, "outcome": "passed", "keywords": ["test_generate_retention_recommendations", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0001868229592218995, "outcome": "passed"}, "call": {"duration": 0.0003355719381943345, "outcome": "passed"}, "teardown": {"duration": 0.0001557020004838705, "outcome": "passed"}}, {"nodeid": "tests/transaction_log_analysis/test_log_analyzer.py::test_analyze_logs", "lineno": 170, "outcome": "passed", "keywords": ["test_analyze_logs", "test_log_analyzer.py", "transaction_log_analysis", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00029225810430943966, "outcome": "passed"}, "call": {"duration": 0.0017035140190273523, "outcome": "passed", "log": [{"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/mysql/mysql-bin.000001: [Errno 2] No such file or directory: '/var/lib/mysql/mysql-bin.000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7947059, "msecs": 794.0, "relativeCreated": 877.5067329406738, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}, {"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /var/lib/postgresql/data/pg_wal/000000010000000A00000001: [Errno 2] No such file or directory: '/var/lib/postgresql/data/pg_wal/000000010000000A00000001'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.7948303, "msecs": 794.0, "relativeCreated": 877.6311874389648, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}]}, "teardown": {"duration": 0.00021402304992079735, "outcome": "passed"}}, {"nodeid": "tests/utils/test_file_utils.py::test_get_file_stats", "lineno": 16, "outcome": "passed", "keywords": ["test_get_file_stats", "test_file_utils.py", "utils", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.0002957279793918133, "outcome": "passed"}, "call": {"duration": 0.0005211730021983385, "outcome": "passed", "log": [{"name": "file_system_analyzer_db_admin.utils.file_utils", "msg": "Error getting stats for /home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/mock_data/nonexistent.txt: [Errno 2] No such file or directory: '/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/mock_data/nonexistent.txt'", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/utils/file_utils.py", "filename": "file_utils.py", "module": "file_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 55, "funcName": "get_file_stats", "created": 1747193952.798005, "msecs": 798.0, "relativeCreated": 880.8059692382812, "thread": 140577220241216, "threadName": "MainThread", "processName": "MainProcess", "process": 525966, "taskName": null}]}, "teardown": {"duration": 0.000183248077519238, "outcome": "passed"}}, {"nodeid": "tests/utils/test_file_utils.py::test_find_files", "lineno": 43, "outcome": "passed", "keywords": ["test_find_files", "test_file_utils.py", "utils", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00025793700478971004, "outcome": "passed"}, "call": {"duration": 0.0028731119818985462, "outcome": "passed"}, "teardown": {"duration": 0.00019196898210793734, "outcome": "passed"}}, {"nodeid": "tests/utils/test_file_utils.py::test_calculate_dir_size", "lineno": 70, "outcome": "passed", "keywords": ["test_calculate_dir_size", "test_file_utils.py", "utils", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00026530097238719463, "outcome": "passed"}, "call": {"duration": 0.0015487050404772162, "outcome": "passed"}, "teardown": {"duration": 0.00018884101882576942, "outcome": "passed"}}, {"nodeid": "tests/utils/test_file_utils.py::test_get_disk_usage", "lineno": 87, "outcome": "passed", "keywords": ["test_get_disk_usage", "test_file_utils.py", "utils", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00027370196767151356, "outcome": "passed"}, "call": {"duration": 0.00028471206314861774, "outcome": "passed"}, "teardown": {"duration": 0.0001786800567060709, "outcome": "passed"}}, {"nodeid": "tests/utils/test_file_utils.py::test_estimate_file_growth_rate", "lineno": 104, "outcome": "passed", "keywords": ["test_estimate_file_growth_rate", "test_file_utils.py", "utils", "tests", "file_system_analyzer_db_admin", ""], "setup": {"duration": 0.00016495503950864077, "outcome": "passed"}, "call": {"duration": 0.0002630719682201743, "outcome": "passed"}, "teardown": {"duration": 0.0001579240197315812, "outcome": "passed"}}], "warnings": [{"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "config", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "config", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "config", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/index_efficiency/test_index_analyzer.py", "lineno": 54}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/export.py", "lineno": 47}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 159}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 159}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 82}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 82}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/export.py", "lineno": 47}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 225}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 111}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/export.py", "lineno": 47}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 288}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 143}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 143}, {"message": "The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/tests/interfaces/test_api.py", "lineno": 143}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/export.py", "lineno": 47}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 351}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/export.py", "lineno": 47}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 411}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 159}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 225}, {"message": "The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/interfaces/api.py", "lineno": 411}, {"message": "divide by zero encountered in scalar divide", "category": "RuntimeWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/transaction_log_analysis/log_analyzer.py", "lineno": 259}, {"message": "divide by zero encountered in scalar divide", "category": "RuntimeWarning", "when": "runtest", "filename": "/home/justinchiu_cohere_com/librarybench/projects/file_system_analyzer/file_system_analyzer_db_admin/file_system_analyzer_db_admin/transaction_log_analysis/log_analyzer.py", "lineno": 272}]}