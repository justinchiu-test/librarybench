{"created": 1746024036.484342, "duration": 0.15285706520080566, "exitcode": 1, "root": "/Users/celine/Research/librarybench", "environment": {}, "summary": {"failed": 5, "total": 5, "collected": 5}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative", "type": "Dir"}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/librarybench", "outcome": "passed", "result": []}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/pipeline", "outcome": "passed", "result": []}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_dynamic_tasks.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_dynamic_tasks.py::test_dynamic_task_creation_and_execution", "type": "Function", "lineno": 7}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_error_handling_and_alerting.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_error_handling_and_alerting.py::test_unexpected_exception_triggers_alert", "type": "Function", "lineno": 7}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_output.txt", "outcome": "passed", "result": []}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_pipeline.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_pipeline.py::test_basic_pipeline_execution_and_data_passing", "type": "Function", "lineno": 7}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_retry_backoff.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_retry_backoff.py::test_retry_with_exponential_backoff", "type": "Function", "lineno": 8}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_timeout.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_timeout.py::test_task_timeout_and_retry", "type": "Function", "lineno": 8}]}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/librarybench", "type": "Package"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/pipeline", "type": "Package"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_dynamic_tasks.py", "type": "Module"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_error_handling_and_alerting.py", "type": "Module"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_output.txt", "type": "DoctestTextfile"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_pipeline.py", "type": "Module"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_retry_backoff.py", "type": "Module"}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_timeout.py", "type": "Module"}]}], "tests": [{"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_dynamic_tasks.py::test_dynamic_task_creation_and_execution", "lineno": 7, "outcome": "failed", "keywords": ["test_dynamic_task_creation_and_execution", "test_dynamic_tasks.py", "workflow_orchestration_DataScientist_o4-mini_iterative", "librarybench", ""], "setup": {"duration": 0.00011891699978150427, "outcome": "passed"}, "call": {"duration": 0.0005708329990739003, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_DataScientist_o4-mini_iterative/test_dynamic_tasks.py", "lineno": 40, "message": "IndexError: list index out of range"}, "traceback": [{"path": "test_dynamic_tasks.py", "lineno": 40, "message": "IndexError"}], "longrepr": "def test_dynamic_task_creation_and_execution():\n        ctx = ExecutionContext()\n        meta = MetadataStorage()\n        notifier = DummyNotifier()\n    \n        # Preload a list of items in context\n        ctx.set('items', [1, 2, 3])\n    \n        # Creator task: generates processing tasks for each item\n        def creator(context):\n            items = context.get('items')\n            new_tasks = []\n            for item in items:\n                def make_func(i):\n                    def proc(ctx):\n                        return {f'item_{i}': i * 2}\n                    return proc\n                t = Task(\n                    name=f'proc_{item}',\n                    func=make_func(item),\n                    outputs=[f'item_{item}']\n                )\n                new_tasks.append(t)\n            return new_tasks\n    \n        creator_task = Task(name='creator', func=creator, inputs=['items'])\n        pipeline = Pipeline([creator_task], ctx, meta, notifier)\n        pipeline.run()\n    \n        # After run, dynamic tasks should have executed\n        for i in [1,2,3]:\n            assert ctx.get(f'item_{i}') == i * 2\n>           rec = meta.get_all(f'proc_{i}')[0]\nE           IndexError: list index out of range\n\ntest_dynamic_tasks.py:40: IndexError"}, "teardown": {"duration": 0.000155083995196037, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_error_handling_and_alerting.py::test_unexpected_exception_triggers_alert", "lineno": 7, "outcome": "failed", "keywords": ["test_unexpected_exception_triggers_alert", "test_error_handling_and_alerting.py", "workflow_orchestration_DataScientist_o4-mini_iterative", "librarybench", ""], "setup": {"duration": 8.800000068731606e-05, "outcome": "passed"}, "call": {"duration": 0.0006865000032121316, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_DataScientist_o4-mini_iterative/test_error_handling_and_alerting.py", "lineno": 28, "message": "AssertionError: assert 'error' == 'failure'\n  \n  - failure\n  + error"}, "traceback": [{"path": "test_error_handling_and_alerting.py", "lineno": 28, "message": "AssertionError"}], "longrepr": "def test_unexpected_exception_triggers_alert():\n        ctx = ExecutionContext()\n        meta = MetadataStorage()\n        notifier = DummyNotifier()\n    \n        def bad_task(context):\n            # raises unexpected error\n            raise KeyError(\"Oops!\")\n    \n        t = Task(\n            name='bad',\n            func=bad_task,\n            max_retries=0,\n            retry_delay_seconds=0,\n            backoff=False\n        )\n        pipeline = Pipeline([t], ctx, meta, notifier)\n        pipeline.run()\n    \n        # Task failed\n>       assert t.state == 'failure'\nE       AssertionError: assert 'error' == 'failure'\nE         \nE         - failure\nE         + error\n\ntest_error_handling_and_alerting.py:28: AssertionError"}, "teardown": {"duration": 7.566700514871627e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_pipeline.py::test_basic_pipeline_execution_and_data_passing", "lineno": 7, "outcome": "failed", "keywords": ["test_basic_pipeline_execution_and_data_passing", "test_pipeline.py", "workflow_orchestration_DataScientist_o4-mini_iterative", "librarybench", ""], "setup": {"duration": 5.662499461323023e-05, "outcome": "passed"}, "call": {"duration": 0.00026954099303111434, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_DataScientist_o4-mini_iterative/test_pipeline.py", "lineno": 37, "message": "assert 0 == 1\n +  where 0 = len([])"}, "traceback": [{"path": "test_pipeline.py", "lineno": 37, "message": "AssertionError"}], "longrepr": "def test_basic_pipeline_execution_and_data_passing():\n        ctx = ExecutionContext()\n        meta = MetadataStorage()\n        notifier = DummyNotifier()\n    \n        def task1(context):\n            # produce 'a' = 1\n            return {'a': 1}\n    \n        def task2(context):\n            a = context.get('a')\n            return {'b': a + 1}\n    \n        t1 = Task(name='t1', func=task1, outputs=['a'])\n        t2 = Task(name='t2', func=task2, inputs=['a'], outputs=['b'])\n        pipeline = Pipeline([t1, t2], ctx, meta, notifier)\n        pipeline.run()\n    \n        # Check context values\n        assert ctx.get('a') == 1\n        assert ctx.get('b') == 2\n    \n        # Check states\n        assert t1.state == 'success'\n        assert t2.state == 'success'\n    \n        # Metadata recorded\n        rec1 = meta.get_all('t1')\n        rec2 = meta.get_all('t2')\n>       assert len(rec1) == 1\nE       assert 0 == 1\nE        +  where 0 = len([])\n\ntest_pipeline.py:37: AssertionError"}, "teardown": {"duration": 5.7375000324100256e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_retry_backoff.py::test_retry_with_exponential_backoff", "lineno": 8, "outcome": "failed", "keywords": ["test_retry_with_exponential_backoff", "test_retry_backoff.py", "workflow_orchestration_DataScientist_o4-mini_iterative", "librarybench", ""], "setup": {"duration": 0.00011379100033082068, "outcome": "passed"}, "call": {"duration": 0.0002390000008745119, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_DataScientist_o4-mini_iterative/test_retry_backoff.py", "lineno": 45, "message": "IndexError: list index out of range"}, "traceback": [{"path": "test_retry_backoff.py", "lineno": 45, "message": "IndexError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x102604c30>\n\n    def test_retry_with_exponential_backoff(monkeypatch):\n        ctx = ExecutionContext()\n        meta = MetadataStorage()\n        notifier = DummyNotifier()\n    \n        # Create a flaky task that fails twice then succeeds\n        attempts = {'count': 0}\n        delays = []\n    \n        def flaky(context):\n            attempts['count'] += 1\n            if attempts['count'] < 3:\n                raise ValueError(\"Intermittent failure\")\n            return {'x': 42}\n    \n        # Monkeypatch time.sleep to record delays without real waiting\n        def fake_sleep(d):\n            delays.append(d)\n        monkeypatch.setattr(time, 'sleep', fake_sleep)\n    \n        t = Task(\n            name='flaky',\n            func=flaky,\n            outputs=['x'],\n            max_retries=4,\n            retry_delay_seconds=1,\n            backoff=True,\n            timeout=None\n        )\n        pipeline = Pipeline([t], ctx, meta, notifier)\n        pipeline.run()\n    \n        # Should have succeeded\n        assert t.state == 'success'\n        assert ctx.get('x') == 42\n        # It should have taken 3 attempts\n>       rec = meta.get_all('flaky')[0]\nE       IndexError: list index out of range\n\ntest_retry_backoff.py:45: IndexError"}, "teardown": {"duration": 8.629199874121696e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_DataScientist_o4-mini_iterative/test_timeout.py::test_task_timeout_and_retry", "lineno": 8, "outcome": "failed", "keywords": ["test_task_timeout_and_retry", "test_timeout.py", "workflow_orchestration_DataScientist_o4-mini_iterative", "librarybench", ""], "setup": {"duration": 9.129098907578737e-05, "outcome": "passed"}, "call": {"duration": 0.11369616600859445, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_DataScientist_o4-mini_iterative/test_timeout.py", "lineno": 42, "message": "AssertionError: assert 'error' == 'failure'\n  \n  - failure\n  + error"}, "traceback": [{"path": "test_timeout.py", "lineno": 42, "message": "AssertionError"}], "longrepr": "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x102606780>\n\n    def test_task_timeout_and_retry(monkeypatch):\n        ctx = ExecutionContext()\n        meta = MetadataStorage()\n        notifier = DummyNotifier()\n    \n        # A task that sleeps longer than its timeout\n        def slow_task(context):\n            time.sleep(0.1)\n            return {'y': 'done'}\n    \n        # Monkeypatch time.sleep so that slow_task actually runs real sleep,\n        # but pipeline retry waits are no-ops\n        original_sleep = time.sleep\n        def fake_sleep(d):\n            # only fake small waits\n            if d < 0.05:\n                return\n            original_sleep(d)\n        monkeypatch.setattr(time, 'sleep', fake_sleep)\n    \n        t = Task(\n            name='slow',\n            func=slow_task,\n            outputs=['y'],\n            max_retries=1,\n            retry_delay_seconds=0.01,\n            backoff=False,\n            timeout=0.05\n        )\n        pipeline = Pipeline([t], ctx, meta, notifier)\n        pipeline.run()\n    \n        # Task should have failed\n>       assert t.state == 'failure'\nE       AssertionError: assert 'error' == 'failure'\nE         \nE         - failure\nE         + error\n\ntest_timeout.py:42: AssertionError"}, "teardown": {"duration": 0.00011995800014119595, "outcome": "passed"}}]}