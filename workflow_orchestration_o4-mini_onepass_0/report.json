{"created": 1745475696.725813, "duration": 0.19269990921020508, "exitcode": 1, "root": "/Users/celine/Research/librarybench", "environment": {}, "summary": {"passed": 8, "failed": 7, "total": 15, "collected": 15}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py", "type": "Module"}]}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_creation", "type": "Function", "lineno": 12}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_creation", "type": "Function", "lineno": 26}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_dag_validation", "type": "Function", "lineno": 44}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_execution", "type": "Function", "lineno": 76}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_execution", "type": "Function", "lineno": 95}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_retry", "type": "Function", "lineno": 118}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_failure_propagation", "type": "Function", "lineno": 150}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_parallel_execution", "type": "Function", "lineno": 182}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_partial_execution", "type": "Function", "lineno": 221}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_timeout", "type": "Function", "lineno": 244}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_retry_with_backoff", "type": "Function", "lineno": 263}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_context_passing", "type": "Function", "lineno": 296}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_logging", "type": "Function", "lineno": 336}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_scheduling", "type": "Function", "lineno": 380}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_execution_history", "type": "Function", "lineno": 419}]}], "tests": [{"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_creation", "lineno": 12, "outcome": "passed", "keywords": ["test_task_creation", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.0001652080100029707, "outcome": "passed"}, "call": {"duration": 0.0001383339986205101, "outcome": "passed"}, "teardown": {"duration": 9.708298603072762e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_creation", "lineno": 26, "outcome": "passed", "keywords": ["test_workflow_creation", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.00010416703298687935, "outcome": "passed"}, "call": {"duration": 0.00011566700413823128, "outcome": "passed"}, "teardown": {"duration": 9.070802479982376e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_dag_validation", "lineno": 44, "outcome": "passed", "keywords": ["test_dag_validation", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.754100417718291e-05, "outcome": "passed"}, "call": {"duration": 0.00029720802558586, "outcome": "passed"}, "teardown": {"duration": 8.670898387208581e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_execution", "lineno": 76, "outcome": "failed", "keywords": ["test_task_execution", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 9.458401473239064e-05, "outcome": "passed"}, "call": {"duration": 0.0002690419787541032, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/tests.py", "lineno": 83, "message": "AssertionError: assert <TaskState.FAILURE: 'failure'> == <TaskState.SUCCESS: 'success'>\n +  where <TaskState.FAILURE: 'failure'> = <workflow.Task object at 0x106cf86b0>.state\n +  and   <TaskState.SUCCESS: 'success'> = TaskState.SUCCESS"}, "traceback": [{"path": "tests.py", "lineno": 83, "message": "AssertionError"}], "longrepr": "def test_task_execution():\n        \"\"\"Test execution of a single task.\"\"\"\n        # Successful task\n        task = Task(name=\"success_task\", func=successful_task)\n        task.execute()\n    \n>       assert task.state == TaskState.SUCCESS\nE       AssertionError: assert <TaskState.FAILURE: 'failure'> == <TaskState.SUCCESS: 'success'>\nE        +  where <TaskState.FAILURE: 'failure'> = <workflow.Task object at 0x106cf86b0>.state\nE        +  and   <TaskState.SUCCESS: 'success'> = TaskState.SUCCESS\n\ntests.py:83: AssertionError"}, "teardown": {"duration": 0.00015454198000952601, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_execution", "lineno": 95, "outcome": "failed", "keywords": ["test_workflow_execution", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.0001027500256896019, "outcome": "passed"}, "call": {"duration": 0.000453209038823843, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/workflow.py", "lineno": 251, "message": "workflow.TaskFailedError: test_workflow_execution.<locals>.<lambda>() takes 0 positional arguments but 1 was given"}, "traceback": [{"path": "tests.py", "lineno": 111, "message": ""}, {"path": "workflow.py", "lineno": 251, "message": "TaskFailedError"}], "longrepr": "def test_workflow_execution():\n        \"\"\"Test execution of a workflow with multiple tasks.\"\"\"\n        workflow = Workflow()\n    \n        # Create tasks with dependencies\n        task1 = Task(name=\"task1\", func=lambda: \"result1\")\n        task2 = Task(\n            name=\"task2\", func=lambda: f\"result2 using {workflow.get_task_result('task1')}\"\n        )\n        task2.dependencies = [\"task1\"]\n    \n        workflow.add_task(task1)\n        workflow.add_task(task2)\n    \n        # Run the workflow\n>       results = workflow.run()\n\ntests.py:111: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x1065d08a0>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_workflow_execution.<locals>.<lambda>() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError"}, "teardown": {"duration": 0.0001258750562556088, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_retry", "lineno": 118, "outcome": "passed", "keywords": ["test_task_retry", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 9.624997619539499e-05, "outcome": "passed"}, "call": {"duration": 0.00030737504130229354, "outcome": "passed"}, "teardown": {"duration": 8.345802780240774e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_failure_propagation", "lineno": 150, "outcome": "passed", "keywords": ["test_failure_propagation", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.512503700330853e-05, "outcome": "passed"}, "call": {"duration": 0.0003019580035470426, "outcome": "passed"}, "teardown": {"duration": 8.55830148793757e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_parallel_execution", "lineno": 182, "outcome": "failed", "keywords": ["test_parallel_execution", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.733395952731371e-05, "outcome": "passed"}, "call": {"duration": 0.00028070894768461585, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/workflow.py", "lineno": 251, "message": "workflow.TaskFailedError: test_parallel_execution.<locals>.slow_task1() takes 0 positional arguments but 1 was given"}, "traceback": [{"path": "tests.py", "lineno": 209, "message": ""}, {"path": "workflow.py", "lineno": 251, "message": "TaskFailedError"}], "longrepr": "def test_parallel_execution():\n        \"\"\"Test parallel execution of independent tasks.\"\"\"\n    \n        def slow_task1():\n            time.sleep(0.2)\n            return \"slow1\"\n    \n        def slow_task2():\n            time.sleep(0.2)\n            return \"slow2\"\n    \n        workflow = Workflow()\n    \n        task1 = Task(name=\"slow_task1\", func=slow_task1)\n        task2 = Task(name=\"slow_task2\", func=slow_task2)\n        task3 = Task(\n            name=\"dependent_task\",\n            func=lambda: \"dependent\",\n            dependencies=[\"slow_task1\", \"slow_task2\"],\n        )\n    \n        workflow.add_task(task1)\n        workflow.add_task(task2)\n        workflow.add_task(task3)\n    \n        start_time = time.time()\n>       workflow.run()\n\ntests.py:209: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x1066d15b0>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_parallel_execution.<locals>.slow_task1() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError"}, "teardown": {"duration": 0.00013816601131111383, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_partial_execution", "lineno": 221, "outcome": "failed", "keywords": ["test_workflow_partial_execution", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.00010200001997873187, "outcome": "passed"}, "call": {"duration": 0.00032295798882842064, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/workflow.py", "lineno": 251, "message": "workflow.TaskFailedError: test_workflow_partial_execution.<locals>.<lambda>() takes 0 positional arguments but 1 was given"}, "traceback": [{"path": "tests.py", "lineno": 237, "message": ""}, {"path": "workflow.py", "lineno": 251, "message": "TaskFailedError"}], "longrepr": "def test_workflow_partial_execution():\n        \"\"\"Test workflow execution with partially completed tasks.\"\"\"\n        workflow = Workflow()\n    \n        task1 = Task(name=\"task1\", func=lambda: \"result1\")\n        task2 = Task(name=\"task2\", func=lambda: \"result2\", dependencies=[\"task1\"])\n    \n        workflow.add_task(task1)\n        workflow.add_task(task2)\n    \n        # Manually mark the first task as complete\n        workflow.tasks[\"task1\"].state = TaskState.SUCCESS\n        workflow.tasks[\"task1\"].result = \"result1\"\n    \n        # Run the workflow - it should skip task1 and only run task2\n>       results = workflow.run()\n\ntests.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x106cf9260>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_workflow_partial_execution.<locals>.<lambda>() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError"}, "teardown": {"duration": 0.00015058397548273206, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_timeout", "lineno": 244, "outcome": "failed", "keywords": ["test_task_timeout", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.00011079199612140656, "outcome": "passed"}, "call": {"duration": 0.0005607089842669666, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/tests.py", "lineno": 257, "message": "AssertionError: Regex pattern did not match.\n Regex: 'timed out'\n Input: 'test_task_timeout.<locals>.slow_task() takes 0 positional arguments but 1 was given'"}, "traceback": [{"path": "tests.py", "lineno": 257, "message": "AssertionError"}], "longrepr": "def test_task_timeout():\n        \"\"\"Test task timeout functionality.\"\"\"\n    \n        def slow_task():\n            time.sleep(0.5)\n            return \"slow result\"\n    \n        task = Task(name=\"timeout_task\", func=slow_task, timeout=0.1)\n    \n        workflow = Workflow()\n        workflow.add_task(task)\n    \n        with pytest.raises(TaskFailedError, match=\"timed out\"):\n>           workflow.run()\n\ntests.py:258: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x106cfa8b0>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_task_timeout.<locals>.slow_task() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError\n\nDuring handling of the above exception, another exception occurred:\n\n    def test_task_timeout():\n        \"\"\"Test task timeout functionality.\"\"\"\n    \n        def slow_task():\n            time.sleep(0.5)\n            return \"slow result\"\n    \n        task = Task(name=\"timeout_task\", func=slow_task, timeout=0.1)\n    \n        workflow = Workflow()\n        workflow.add_task(task)\n    \n>       with pytest.raises(TaskFailedError, match=\"timed out\"):\nE       AssertionError: Regex pattern did not match.\nE        Regex: 'timed out'\nE        Input: 'test_task_timeout.<locals>.slow_task() takes 0 positional arguments but 1 was given'\n\ntests.py:257: AssertionError"}, "teardown": {"duration": 0.00019745901226997375, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_retry_with_backoff", "lineno": 263, "outcome": "passed", "keywords": ["test_task_retry_with_backoff", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.00010975002078339458, "outcome": "passed"}, "call": {"duration": 0.0003005000180564821, "outcome": "passed"}, "teardown": {"duration": 8.566700853407383e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_context_passing", "lineno": 296, "outcome": "passed", "keywords": ["test_task_context_passing", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.875003550201654e-05, "outcome": "passed"}, "call": {"duration": 0.00042329198913648725, "outcome": "passed"}, "teardown": {"duration": 8.670799434185028e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_task_logging", "lineno": 336, "outcome": "failed", "keywords": ["test_task_logging", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.949998300522566e-05, "outcome": "passed"}, "call": {"duration": 0.0005166670307517052, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/workflow.py", "lineno": 251, "message": "workflow.TaskFailedError: test_task_logging.<locals>.logging_task() takes 0 positional arguments but 1 was given"}, "traceback": [{"path": "tests.py", "lineno": 364, "message": ""}, {"path": "workflow.py", "lineno": 251, "message": "TaskFailedError"}], "longrepr": "def test_task_logging():\n        \"\"\"Test task execution logging.\"\"\"\n        # Configure a logger for testing\n        logger = logging.getLogger(\"test_logger\")\n        logger.setLevel(logging.INFO)\n    \n        # Use a StringIO as a log handler to capture logs\n        from io import StringIO\n        log_capture = StringIO()\n        handler = logging.StreamHandler(log_capture)\n        formatter = logging.Formatter('%(levelname)s: %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    \n        # Define a task that logs messages\n        def logging_task():\n            logger.info(\"Task started\")\n            logger.warning(\"This is a warning\")\n            logger.error(\"This is an error\")\n            return \"success with logs\"\n    \n        # Create and execute the task\n        task = Task(name=\"logging_task\", func=logging_task, logger=logger)\n    \n        # Create a workflow and run it\n        workflow = Workflow()\n        workflow.add_task(task)\n>       workflow.run()\n\ntests.py:364: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x106bd9d50>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_task_logging.<locals>.logging_task() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError"}, "teardown": {"duration": 0.00017925002612173557, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_scheduling", "lineno": 380, "outcome": "passed", "keywords": ["test_workflow_scheduling", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 0.00010475004091858864, "outcome": "passed"}, "call": {"duration": 0.00016354199033230543, "outcome": "passed"}, "teardown": {"duration": 8.120795246213675e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass/tests.py::test_workflow_execution_history", "lineno": 419, "outcome": "failed", "keywords": ["test_workflow_execution_history", "tests.py", "workflow_orchestration_o4-mini_onepass", "librarybench", ""], "setup": {"duration": 8.61250446178019e-05, "outcome": "passed"}, "call": {"duration": 0.0002965419553220272, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass/workflow.py", "lineno": 251, "message": "workflow.TaskFailedError: test_workflow_execution_history.<locals>.<lambda>() takes 0 positional arguments but 1 was given"}, "traceback": [{"path": "tests.py", "lineno": 432, "message": ""}, {"path": "workflow.py", "lineno": 251, "message": "TaskFailedError"}], "longrepr": "def test_workflow_execution_history():\n        \"\"\"Test that workflow execution history is properly tracked.\"\"\"\n        workflow = Workflow(name=\"history_workflow\")\n    \n        # Create tasks\n        task1 = Task(name=\"task1\", func=lambda: \"result1\")\n        task2 = Task(name=\"task2\", func=lambda: \"result2\", dependencies=[\"task1\"])\n        workflow.add_task(task1)\n        workflow.add_task(task2)\n    \n        # Run the workflow multiple times\n        for _ in range(3):\n>           workflow.run()\n\ntests.py:432: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x106c5d130>\n\n    def run(self):\n        self.validate()\n        run_rec = ExecutionRunRecord(self.name)\n        run_rec.start_time = datetime.datetime.now()\n        context = {}\n        results = {}\n    \n        # Partial success tasks\n        for name, task in self.tasks.items():\n            if task.state == TaskState.SUCCESS:\n                results[name] = task.result\n    \n        pending = set(self.tasks.keys())\n        running_futs = {}\n        executor = ThreadPoolExecutor(max_workers=len(self.tasks) or 1)\n        failed_task = None\n    \n        try:\n            while pending or running_futs:\n                # Schedule new tasks\n                if failed_task is None:\n                    for t_name in list(pending):\n                        task = self.tasks[t_name]\n                        if task.state == TaskState.SUCCESS:\n                            pending.remove(t_name)\n                            continue\n                        # dependencies satisfied?\n                        if all(dep in results for dep in task.dependencies):\n                            fut = executor.submit(task.execute, context)\n                            running_futs[fut] = t_name\n                            pending.remove(t_name)\n                if not running_futs:\n                    break\n                done, _ = wait(running_futs.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    t_name = running_futs.pop(fut)\n                    task = self.tasks[t_name]\n                    rec = task.execution_records[-1]\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(\n                        rec.start_time, rec.end_time, rec.status\n                    )\n                    if task.state == TaskState.SUCCESS:\n                        results[t_name] = task.result\n                        # update context if dict\n                        if isinstance(task.result, dict):\n                            context = task.result\n                    elif task.state == TaskState.RETRYING:\n                        # re-queue\n                        pending.add(t_name)\n                    else:  # FAILURE\n                        failed_task = task\n                if failed_task:\n                    break\n        finally:\n            executor.shutdown(wait=False)\n    \n        # Handle failures and propagation\n        if failed_task:\n            # mark downstream tasks as failed\n            for t_name in list(pending):\n                task = self.tasks[t_name]\n                if any(self.tasks[d].state == TaskState.FAILURE for d in task.dependencies):\n                    task.state = TaskState.FAILURE\n                    task.error = TaskFailedError(f\"Task {t_name} failed due to dependency failure\")\n                    run_rec.task_executions[t_name] = ExecutionTaskRecord(None, None, \"failure\")\n            run_rec.end_time = datetime.datetime.now()\n            run_rec.status = \"failure\"\n            self.execution_history.append(run_rec)\n>           raise TaskFailedError(str(failed_task.error))\nE           workflow.TaskFailedError: test_workflow_execution_history.<locals>.<lambda>() takes 0 positional arguments but 1 was given\n\nworkflow.py:251: TaskFailedError"}, "teardown": {"duration": 0.0002138749696314335, "outcome": "passed"}}]}