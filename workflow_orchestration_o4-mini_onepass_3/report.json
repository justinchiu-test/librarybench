{"created": 1745547946.0107021, "duration": 0.6285891532897949, "exitcode": 1, "root": "/Users/celine/Research/librarybench", "environment": {}, "summary": {"passed": 10, "failed": 5, "total": 15, "collected": 15}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py", "type": "Module"}]}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py", "outcome": "passed", "result": [{"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_creation", "type": "Function", "lineno": 12}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_creation", "type": "Function", "lineno": 26}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_dag_validation", "type": "Function", "lineno": 44}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_execution", "type": "Function", "lineno": 76}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_execution", "type": "Function", "lineno": 95}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_retry", "type": "Function", "lineno": 118}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_failure_propagation", "type": "Function", "lineno": 150}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_parallel_execution", "type": "Function", "lineno": 182}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_partial_execution", "type": "Function", "lineno": 221}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_timeout", "type": "Function", "lineno": 244}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_retry_with_backoff", "type": "Function", "lineno": 263}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_context_passing", "type": "Function", "lineno": 296}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_logging", "type": "Function", "lineno": 336}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_scheduling", "type": "Function", "lineno": 380}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_execution_history", "type": "Function", "lineno": 419}]}], "tests": [{"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_creation", "lineno": 12, "outcome": "passed", "keywords": ["test_task_creation", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00019166700076311827, "outcome": "passed"}, "call": {"duration": 0.0001650839694775641, "outcome": "passed"}, "teardown": {"duration": 0.00011616700794547796, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_creation", "lineno": 26, "outcome": "passed", "keywords": ["test_workflow_creation", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00011141702998429537, "outcome": "passed"}, "call": {"duration": 0.0001174590433947742, "outcome": "passed"}, "teardown": {"duration": 9.954202687367797e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_dag_validation", "lineno": 44, "outcome": "passed", "keywords": ["test_dag_validation", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 9.429198689758778e-05, "outcome": "passed"}, "call": {"duration": 0.00034362502628937364, "outcome": "passed"}, "teardown": {"duration": 8.958298712968826e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_execution", "lineno": 76, "outcome": "failed", "keywords": ["test_task_execution", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 9.64159844443202e-05, "outcome": "passed"}, "call": {"duration": 0.00018550001550465822, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "lineno": 74, "message": "Exception: Task failed"}, "traceback": [{"path": "tests.py", "lineno": 89, "message": ""}, {"path": "workflow.py", "lineno": 113, "message": "in execute"}, {"path": "workflow.py", "lineno": 91, "message": "in execute"}, {"path": "tests.py", "lineno": 74, "message": "Exception"}], "longrepr": "def test_task_execution():\n        \"\"\"Test execution of a single task.\"\"\"\n        # Successful task\n        task = Task(name=\"success_task\", func=successful_task)\n        task.execute()\n    \n        assert task.state == TaskState.SUCCESS\n        assert task.result == \"success\"\n        assert task.error is None\n    \n        # Failing task\n        task = Task(name=\"failing_task\", func=failing_task)\n>       task.execute()\n\ntests.py:89: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nworkflow.py:113: in execute\n    raise e\nworkflow.py:91: in execute\n    res = self.func()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    def failing_task():\n>       raise Exception(\"Task failed\")\nE       Exception: Task failed\n\ntests.py:74: Exception"}, "teardown": {"duration": 0.00017541699344292283, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_execution", "lineno": 95, "outcome": "passed", "keywords": ["test_workflow_execution", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00011787505354732275, "outcome": "passed"}, "call": {"duration": 0.002597208018414676, "outcome": "passed"}, "teardown": {"duration": 0.00011595804244279861, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_retry", "lineno": 118, "outcome": "passed", "keywords": ["test_task_retry", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00010775000555440784, "outcome": "passed"}, "call": {"duration": 0.0004819169989787042, "outcome": "passed"}, "teardown": {"duration": 0.0001004580408334732, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_failure_propagation", "lineno": 150, "outcome": "failed", "keywords": ["test_failure_propagation", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 9.574997238814831e-05, "outcome": "passed"}, "call": {"duration": 0.0004669580375775695, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "lineno": 176, "message": "AssertionError: assert <TaskState.PENDING: 'pending'> == <TaskState.FAILURE: 'failure'>\n +  where <TaskState.PENDING: 'pending'> = <workflow.Task object at 0x1067ed710>.state\n +  and   <TaskState.FAILURE: 'failure'> = TaskState.FAILURE"}, "traceback": [{"path": "tests.py", "lineno": 176, "message": "AssertionError"}], "longrepr": "def test_failure_propagation():\n        \"\"\"Test failure propagation to dependent tasks.\"\"\"\n        workflow = Workflow()\n    \n        # Create a failing task that fails immediately for the test\n        def test_failing_task():\n            raise Exception(\"Task will always fail\")\n    \n        task1 = Task(name=\"test_failing_task\", func=test_failing_task, max_retries=0)\n    \n        # Create a dependent task\n        task2 = Task(\n            name=\"dependent_task\", func=successful_task, dependencies=[\"test_failing_task\"]\n        )\n    \n        workflow.add_task(task1)\n        workflow.add_task(task2)\n    \n        # Run the workflow\n        with pytest.raises(TaskFailedError):\n            workflow.run()\n    \n        # Check that the task is in failure state\n        assert workflow.tasks[\"test_failing_task\"].state == TaskState.FAILURE\n        # Dependent task should be marked as failed due to dependency failure\n>       assert workflow.tasks[\"dependent_task\"].state == TaskState.FAILURE\nE       AssertionError: assert <TaskState.PENDING: 'pending'> == <TaskState.FAILURE: 'failure'>\nE        +  where <TaskState.PENDING: 'pending'> = <workflow.Task object at 0x1067ed710>.state\nE        +  and   <TaskState.FAILURE: 'failure'> = TaskState.FAILURE\n\ntests.py:176: AssertionError"}, "teardown": {"duration": 0.00011791597353294492, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_parallel_execution", "lineno": 182, "outcome": "passed", "keywords": ["test_parallel_execution", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 9.26670036278665e-05, "outcome": "passed"}, "call": {"duration": 0.20638187503209338, "outcome": "passed"}, "teardown": {"duration": 0.00041620800038799644, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_partial_execution", "lineno": 221, "outcome": "failed", "keywords": ["test_workflow_partial_execution", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.0003714999766089022, "outcome": "passed"}, "call": {"duration": 0.0010167500004172325, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "lineno": 241, "message": "KeyError: 'task1'"}, "traceback": [{"path": "tests.py", "lineno": 241, "message": "KeyError"}], "longrepr": "def test_workflow_partial_execution():\n        \"\"\"Test workflow execution with partially completed tasks.\"\"\"\n        workflow = Workflow()\n    \n        task1 = Task(name=\"task1\", func=lambda: \"result1\")\n        task2 = Task(name=\"task2\", func=lambda: \"result2\", dependencies=[\"task1\"])\n    \n        workflow.add_task(task1)\n        workflow.add_task(task2)\n    \n        # Manually mark the first task as complete\n        workflow.tasks[\"task1\"].state = TaskState.SUCCESS\n        workflow.tasks[\"task1\"].result = \"result1\"\n    \n        # Run the workflow - it should skip task1 and only run task2\n        results = workflow.run()\n    \n        assert workflow.tasks[\"task1\"].state == TaskState.SUCCESS\n        assert workflow.tasks[\"task2\"].state == TaskState.SUCCESS\n>       assert results[\"task1\"] == \"result1\"\nE       KeyError: 'task1'\n\ntests.py:241: KeyError"}, "teardown": {"duration": 0.0005722499918192625, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_timeout", "lineno": 244, "outcome": "failed", "keywords": ["test_task_timeout", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.0002948750043287873, "outcome": "passed"}, "call": {"duration": 0.10384066600818187, "outcome": "failed", "crash": {"path": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "lineno": 257, "message": "AssertionError: Regex pattern did not match.\n Regex: 'timed out'\n Input: 'Workflow failed'"}, "traceback": [{"path": "tests.py", "lineno": 257, "message": "AssertionError"}], "longrepr": "def test_task_timeout():\n        \"\"\"Test task timeout functionality.\"\"\"\n    \n        def slow_task():\n            time.sleep(0.5)\n            return \"slow result\"\n    \n        task = Task(name=\"timeout_task\", func=slow_task, timeout=0.1)\n    \n        workflow = Workflow()\n        workflow.add_task(task)\n    \n        with pytest.raises(TaskFailedError, match=\"timed out\"):\n>           workflow.run()\n\ntests.py:258: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <workflow.Workflow object at 0x106826360>\n\n    def run(self):\n        \"\"\"\n        Execute the workflow, running tasks respecting dependencies,\n        handling retries, timeouts, failure propagation, parallel execution.\n        Returns a dict of task_name -> result.\n        Raises TaskFailedError on any failure.\n        \"\"\"\n        # Validate DAG\n        self.validate()\n    \n        wf_start = datetime.datetime.now()\n        wf_status = \"success\"\n        results = {}\n        # build fresh context for this run\n        context = {}\n    \n        # Keep looping until no PENDING/RETRYING tasks remain\n        executor = concurrent.futures.ThreadPoolExecutor(max_workers=len(self.tasks))\n        try:\n            while True:\n                # Propagate dependency failures\n                for t in self.tasks.values():\n                    if t.state in (TaskState.PENDING, TaskState.RETRYING):\n                        for dep in t.dependencies:\n                            dep_task = self.tasks[dep]\n                            if dep_task.state == TaskState.FAILURE:\n                                t.state = TaskState.FAILURE\n                                t.error = TaskFailedError(f\"Dependency {dep} failed\")\n                # Find ready tasks\n                ready = []\n                for t in self.tasks.values():\n                    if t.state in (TaskState.PENDING, TaskState.RETRYING):\n                        if all(self.tasks[dep].state == TaskState.SUCCESS for dep in t.dependencies):\n                            ready.append(t)\n                if not ready:\n                    break  # nothing to run\n                # Schedule batch\n                futures = {}\n                for t in ready:\n                    # Prepare context slice\n                    ctx = context.copy()\n                    fut = executor.submit(t.execute, ctx)\n                    futures[fut] = t\n                # Collect results\n                error_occurred = False\n                for fut, t in futures.items():\n                    try:\n                        # handle timeout\n                        if t.timeout is not None:\n                            fut.result(timeout=t.timeout)\n                        else:\n                            fut.result()\n                    except concurrent.futures.TimeoutError:\n                        # Task timed out\n                        t.state = TaskState.FAILURE\n                        t.error = TaskFailedError(\"Task timed out\")\n                        error_occurred = True\n                        fut.cancel()\n                    except Exception:\n                        # Underlying task.execute sets state appropriately\n                        error_occurred = True\n                    else:\n                        # on success, merge result\n                        if isinstance(t.result, dict):\n                            context.update(t.result)\n                        results[t.name] = t.result\n                if error_occurred:\n                    wf_status = \"failure\"\n                    break\n            # After batches, check if any failure in tasks triggers workflow failure\n            if any(t.state == TaskState.FAILURE for t in self.tasks.values()):\n                wf_status = \"failure\"\n        finally:\n            executor.shutdown(wait=False)\n            wf_end = datetime.datetime.now()\n            # Build workflow execution record\n            task_execs = {}\n            for name, t in self.tasks.items():\n                # pick latest execution record for this run\n                if t.execution_records:\n                    rec = t.execution_records[-1]\n                    status = t.state.value\n                    task_execs[name] = TaskExecutionInfo(rec.start_time, rec.end_time, status)\n                else:\n                    # never executed (e.g., skipped), mark as success with zero times\n                    now = wf_end\n                    task_execs[name] = TaskExecutionInfo(now, now, t.state.value)\n            self.execution_history.append(WorkflowExecution(wf_start, wf_end, wf_status, task_execs))\n    \n        if wf_status == \"failure\":\n>           raise TaskFailedError(\"Workflow failed\")\nE           workflow.TaskFailedError: Workflow failed\n\nworkflow.py:253: TaskFailedError\n\nDuring handling of the above exception, another exception occurred:\n\n    def test_task_timeout():\n        \"\"\"Test task timeout functionality.\"\"\"\n    \n        def slow_task():\n            time.sleep(0.5)\n            return \"slow result\"\n    \n        task = Task(name=\"timeout_task\", func=slow_task, timeout=0.1)\n    \n        workflow = Workflow()\n        workflow.add_task(task)\n    \n>       with pytest.raises(TaskFailedError, match=\"timed out\"):\nE       AssertionError: Regex pattern did not match.\nE        Regex: 'timed out'\nE        Input: 'Workflow failed'\n\ntests.py:257: AssertionError"}, "teardown": {"duration": 0.000516875006724149, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_retry_with_backoff", "lineno": 263, "outcome": "failed", "keywords": ["test_task_retry_with_backoff", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00043045799247920513, "outcome": "passed"}, "call": {"duration": 0.0012462919694371521, "outcome": "failed", "crash": {"path": "/Users/celine/miniconda3/envs/library/lib/python3.13/unittest/mock.py", "lineno": 1226, "message": "Exception: Intentional failure"}, "traceback": [{"path": "tests.py", "lineno": 281, "message": ""}, {"path": "workflow.py", "lineno": 108, "message": "in execute"}, {"path": "workflow.py", "lineno": 91, "message": "in execute"}, {"path": "../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py", "lineno": 1167, "message": "in __call__"}, {"path": "../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py", "lineno": 1171, "message": "in _mock_call"}, {"path": "../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py", "lineno": 1226, "message": "Exception"}], "longrepr": "def test_task_retry_with_backoff():\n        \"\"\"Test task retry with exponential backoff.\"\"\"\n        # Mock a function that fails every time\n        mock_func = MagicMock(side_effect=Exception(\"Intentional failure\"))\n    \n        # Create a task with retry policy\n        task = Task(\n            name=\"retry_task\",\n            func=mock_func,\n            max_retries=3,\n            retry_delay=0.1\n        )\n    \n        # Record the start time\n        start_time = time.time()\n    \n        # Execute the task (it will fail after all retries)\n>       task.execute()\n\ntests.py:281: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nworkflow.py:108: in execute\n    raise e\nworkflow.py:91: in execute\n    res = self.func()\n../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py:1167: in __call__\n    return self._mock_call(*args, **kwargs)\n../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py:1171: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <MagicMock id='4405090624'>, args = (), kwargs = {}\neffect = Exception('Intentional failure')\n\n    def _execute_mock_call(self, /, *args, **kwargs):\n        # separate from _increment_mock_call so that awaited functions are\n        # executed separately from their call, also AsyncMock overrides this method\n    \n        effect = self.side_effect\n        if effect is not None:\n            if _is_exception(effect):\n>               raise effect\nE               Exception: Intentional failure\n\n../../../miniconda3/envs/library/lib/python3.13/unittest/mock.py:1226: Exception"}, "teardown": {"duration": 0.00016858300659805536, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_context_passing", "lineno": 296, "outcome": "passed", "keywords": ["test_task_context_passing", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.00010420900071039796, "outcome": "passed"}, "call": {"duration": 0.0004045420209877193, "outcome": "passed"}, "teardown": {"duration": 8.774996967986226e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_task_logging", "lineno": 336, "outcome": "passed", "keywords": ["test_task_logging", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 8.541601710021496e-05, "outcome": "passed"}, "call": {"duration": 0.00044379098108038306, "outcome": "passed", "log": [{"name": "test_logger", "msg": "Task started", "args": null, "levelname": "INFO", "levelno": 20, "pathname": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "filename": "tests.py", "module": "tests", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 353, "funcName": "logging_task", "created": 1745547946.0084748, "msecs": 8.0, "relativeCreated": 809.131, "thread": 6115487744, "threadName": "ThreadPoolExecutor-6_0", "processName": "MainProcess", "process": 77119, "taskName": null}, {"name": "test_logger", "msg": "This is a warning", "args": null, "levelname": "WARNING", "levelno": 30, "pathname": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "filename": "tests.py", "module": "tests", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 354, "funcName": "logging_task", "created": 1745547946.008566, "msecs": 8.0, "relativeCreated": 809.222, "thread": 6115487744, "threadName": "ThreadPoolExecutor-6_0", "processName": "MainProcess", "process": 77119, "taskName": null}, {"name": "test_logger", "msg": "This is an error", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": "/Users/celine/Research/librarybench/workflow_orchestration_o4-mini_onepass_3/tests.py", "filename": "tests.py", "module": "tests", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 355, "funcName": "logging_task", "created": 1745547946.008606, "msecs": 8.0, "relativeCreated": 809.262, "thread": 6115487744, "threadName": "ThreadPoolExecutor-6_0", "processName": "MainProcess", "process": 77119, "taskName": null}]}, "teardown": {"duration": 8.679198799654841e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_scheduling", "lineno": 380, "outcome": "passed", "keywords": ["test_workflow_scheduling", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 8.595897816121578e-05, "outcome": "passed"}, "call": {"duration": 0.00012979196617379785, "outcome": "passed"}, "teardown": {"duration": 8.341600187122822e-05, "outcome": "passed"}}, {"nodeid": "workflow_orchestration_o4-mini_onepass_3/tests.py::test_workflow_execution_history", "lineno": 419, "outcome": "passed", "keywords": ["test_workflow_execution_history", "tests.py", "workflow_orchestration_o4-mini_onepass_3", "librarybench", ""], "setup": {"duration": 0.0001377919688820839, "outcome": "passed"}, "call": {"duration": 0.0006214999593794346, "outcome": "passed"}, "teardown": {"duration": 9.366596350446343e-05, "outcome": "passed"}}]}